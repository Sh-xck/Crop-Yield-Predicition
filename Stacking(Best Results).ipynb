{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c27ddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "crp5=pd.read_csv('Ragi.csv')\n",
    "gj1=crp5.drop(['Crop','Year','Season','State','TotalRainfall'],axis=1)\n",
    "import numpy as np\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "x = crp5[features]\n",
    "y = crp5['Yield']\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sx=StandardScaler().fit(x)\n",
    "y=np.array(y)\n",
    "x=sx.transform(x)\n",
    "X_train=x[:250]\n",
    "X_test=x[250:260]\n",
    "y_train=y[:250]\n",
    "y_test=y[250:260]\n",
    "X_tr=x[260:]\n",
    "y_tr=y[260:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe62af98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af20dd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 9ms/step - loss: 0.9553 - val_loss: 0.6226\n",
      "Epoch 2/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.4993 - val_loss: 1.4153\n",
      "Epoch 3/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.4078 - val_loss: 0.3497\n",
      "Epoch 4/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.5155 - val_loss: 1.0796\n",
      "Epoch 5/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.4577 - val_loss: 1.5956\n",
      "Epoch 6/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3463 - val_loss: 0.3937\n",
      "Epoch 7/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3449 - val_loss: 1.2243\n",
      "Epoch 8/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.4024 - val_loss: 1.1802\n",
      "Epoch 9/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3150 - val_loss: 0.7020\n",
      "Epoch 10/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.2846 - val_loss: 0.3381\n",
      "Epoch 11/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.2746 - val_loss: 0.5384\n",
      "Epoch 12/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3187 - val_loss: 0.6486\n",
      "Epoch 13/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.2796 - val_loss: 0.4704\n",
      "Epoch 14/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1969 - val_loss: 0.4923\n",
      "Epoch 15/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1784 - val_loss: 0.3943\n",
      "Epoch 16/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.2890 - val_loss: 0.5892\n",
      "Epoch 17/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1783 - val_loss: 0.6460\n",
      "Epoch 18/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1634 - val_loss: 1.4452\n",
      "Epoch 19/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2365 - val_loss: 0.6631\n",
      "Epoch 20/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1514 - val_loss: 0.8402\n",
      "Epoch 21/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2482 - val_loss: 0.4825\n",
      "Epoch 22/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1906 - val_loss: 0.6506\n",
      "Epoch 23/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1293 - val_loss: 0.4598\n",
      "Epoch 24/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1457 - val_loss: 0.6217\n",
      "Epoch 25/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1482 - val_loss: 0.5247\n",
      "Epoch 26/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1697 - val_loss: 0.5619\n",
      "Epoch 27/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1782 - val_loss: 0.5128\n",
      "Epoch 28/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.2233 - val_loss: 0.9629\n",
      "Epoch 29/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1652 - val_loss: 0.6730\n",
      "Epoch 30/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1170 - val_loss: 0.8726\n",
      "Epoch 31/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.1192 - val_loss: 0.4621\n",
      "Epoch 32/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2300 - val_loss: 1.1239\n",
      "Epoch 33/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1968 - val_loss: 0.5432\n",
      "Epoch 34/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1559 - val_loss: 0.6162\n",
      "Epoch 35/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1303 - val_loss: 0.6970\n",
      "Epoch 36/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1268 - val_loss: 0.4595\n",
      "Epoch 37/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1023 - val_loss: 0.4877\n",
      "Epoch 38/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1052 - val_loss: 0.7188\n",
      "Epoch 39/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1936 - val_loss: 0.9344\n",
      "Epoch 40/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1537 - val_loss: 0.3781\n",
      "Epoch 41/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1076 - val_loss: 0.4727\n",
      "Epoch 42/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1005 - val_loss: 0.5737\n",
      "Epoch 43/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1012 - val_loss: 0.4305\n",
      "Epoch 44/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1134 - val_loss: 0.3504\n",
      "Epoch 45/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1139 - val_loss: 0.2870\n",
      "Epoch 46/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1647 - val_loss: 0.5610\n",
      "Epoch 47/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1209 - val_loss: 0.4944\n",
      "Epoch 48/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1026 - val_loss: 0.4098\n",
      "Epoch 49/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1292 - val_loss: 0.1925\n",
      "Epoch 50/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1349 - val_loss: 0.5298\n",
      "Epoch 51/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1125 - val_loss: 0.3818\n",
      "Epoch 52/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0995 - val_loss: 0.4105\n",
      "Epoch 53/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0968 - val_loss: 0.4850\n",
      "Epoch 54/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1022 - val_loss: 0.2843\n",
      "Epoch 55/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1039 - val_loss: 0.4742\n",
      "Epoch 56/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0923 - val_loss: 0.5106\n",
      "Epoch 57/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0970 - val_loss: 0.3586\n",
      "Epoch 58/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0851 - val_loss: 0.4123\n",
      "Epoch 59/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0878 - val_loss: 0.4330\n",
      "Epoch 60/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0917 - val_loss: 0.1953\n",
      "Epoch 61/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0914 - val_loss: 0.2954\n",
      "Epoch 62/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1485 - val_loss: 0.5354\n",
      "Epoch 63/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1375 - val_loss: 0.6522\n",
      "Epoch 64/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1015 - val_loss: 0.5814\n",
      "Epoch 65/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0943 - val_loss: 0.4536\n",
      "Epoch 66/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1008 - val_loss: 0.5890\n",
      "Epoch 67/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1077 - val_loss: 0.3060\n",
      "Epoch 68/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1207 - val_loss: 0.4770\n",
      "Epoch 69/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1397 - val_loss: 0.2888\n",
      "Epoch 70/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1076 - val_loss: 0.2653\n",
      "Epoch 71/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1064 - val_loss: 0.2675\n",
      "Epoch 72/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1160 - val_loss: 0.3184\n",
      "Epoch 73/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0910 - val_loss: 0.5776\n",
      "Epoch 74/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0981 - val_loss: 0.4170\n",
      "Epoch 75/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0879 - val_loss: 0.4709\n",
      "Epoch 76/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0798 - val_loss: 0.5795\n",
      "Epoch 77/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0815 - val_loss: 0.5130\n",
      "Epoch 78/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0814 - val_loss: 0.4519\n",
      "Epoch 79/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0760 - val_loss: 0.6507\n",
      "Epoch 80/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0819 - val_loss: 0.4543\n",
      "Epoch 81/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0769 - val_loss: 0.4166\n",
      "Epoch 82/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0777 - val_loss: 0.6362\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0771 - val_loss: 0.5759\n",
      "Epoch 84/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0811 - val_loss: 0.5024\n",
      "Epoch 85/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1060 - val_loss: 0.6123\n",
      "Epoch 86/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1152 - val_loss: 0.4336\n",
      "Epoch 87/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0917 - val_loss: 0.5475\n",
      "Epoch 88/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0862 - val_loss: 0.4319\n",
      "Epoch 89/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1362 - val_loss: 0.5286\n",
      "Epoch 90/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1112 - val_loss: 0.6587\n",
      "Epoch 91/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1330 - val_loss: 0.9759\n",
      "Epoch 92/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1166 - val_loss: 0.2785\n",
      "Epoch 93/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0947 - val_loss: 0.6315\n",
      "Epoch 94/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1141 - val_loss: 0.3022\n",
      "Epoch 95/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1356 - val_loss: 0.3170\n",
      "Epoch 96/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1095 - val_loss: 0.5144\n",
      "Epoch 97/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1134 - val_loss: 0.4450\n",
      "Epoch 98/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0876 - val_loss: 0.5154\n",
      "Epoch 99/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0761 - val_loss: 0.4759\n",
      "Epoch 100/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0930 - val_loss: 0.5092\n",
      "Epoch 101/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0954 - val_loss: 0.4741\n",
      "Epoch 102/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0833 - val_loss: 0.5685\n",
      "Epoch 103/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0783 - val_loss: 0.6439\n",
      "Epoch 104/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0818 - val_loss: 0.6264\n",
      "Epoch 105/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0792 - val_loss: 0.6838\n",
      "Epoch 106/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0839 - val_loss: 0.5911\n",
      "Epoch 107/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0836 - val_loss: 0.6195\n",
      "Epoch 108/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0736 - val_loss: 0.6114\n",
      "Epoch 109/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0835 - val_loss: 0.6830\n",
      "Epoch 110/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0819 - val_loss: 0.6229\n",
      "Epoch 111/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0887 - val_loss: 0.6963\n",
      "Epoch 112/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0854 - val_loss: 0.6254\n",
      "Epoch 113/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0799 - val_loss: 0.6547\n",
      "Epoch 114/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0838 - val_loss: 0.6425\n",
      "Epoch 115/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0813 - val_loss: 0.6019\n",
      "Epoch 116/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0921 - val_loss: 0.6514\n",
      "Epoch 117/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0856 - val_loss: 0.4935\n",
      "Epoch 118/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1051 - val_loss: 0.4223\n",
      "Epoch 119/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1225 - val_loss: 0.7001\n",
      "Epoch 120/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1049 - val_loss: 0.5965\n",
      "Epoch 121/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0809 - val_loss: 0.6290\n",
      "Epoch 122/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0832 - val_loss: 0.5224\n",
      "Epoch 123/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0766 - val_loss: 0.5664\n",
      "Epoch 124/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0796 - val_loss: 0.4508\n",
      "Epoch 125/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0732 - val_loss: 0.5185\n",
      "Epoch 126/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0803 - val_loss: 0.4206\n",
      "Epoch 127/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.1034 - val_loss: 0.6221\n",
      "Epoch 128/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0772 - val_loss: 0.6274\n",
      "Epoch 129/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0882 - val_loss: 0.5689\n",
      "Epoch 130/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0748 - val_loss: 0.6114\n",
      "Epoch 131/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0722 - val_loss: 0.5030\n",
      "Epoch 132/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0742 - val_loss: 0.5379\n",
      "Epoch 133/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0824 - val_loss: 0.5618\n",
      "Epoch 134/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0767 - val_loss: 0.6111\n",
      "Epoch 135/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0849 - val_loss: 0.5986\n",
      "Epoch 136/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0743 - val_loss: 0.5445\n",
      "Epoch 137/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0790 - val_loss: 0.4653\n",
      "Epoch 138/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0895 - val_loss: 0.5585\n",
      "Epoch 139/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0790 - val_loss: 0.5459\n",
      "Epoch 140/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0779 - val_loss: 0.5354\n",
      "Epoch 141/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0748 - val_loss: 0.5362\n",
      "Epoch 142/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0997 - val_loss: 0.6329\n",
      "Epoch 143/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0799 - val_loss: 0.4410\n",
      "Epoch 144/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0825 - val_loss: 0.2629\n",
      "Epoch 145/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1211 - val_loss: 0.4959\n",
      "Epoch 146/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1239 - val_loss: 0.3200\n",
      "Epoch 147/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1157 - val_loss: 0.6432\n",
      "Epoch 148/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0885 - val_loss: 0.5364\n",
      "Epoch 149/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0899 - val_loss: 0.6808\n",
      "Epoch 150/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1220 - val_loss: 0.5267\n",
      "Epoch 151/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0896 - val_loss: 0.6417\n",
      "Epoch 152/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1090 - val_loss: 0.5734\n",
      "Epoch 153/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1000 - val_loss: 1.1723\n",
      "Epoch 154/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2981 - val_loss: 0.5808\n",
      "Epoch 155/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2462 - val_loss: 0.3152\n",
      "Epoch 156/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1639 - val_loss: 0.5216\n",
      "Epoch 157/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1149 - val_loss: 0.3558\n",
      "Epoch 158/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1984 - val_loss: 0.5669\n",
      "Epoch 159/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1124 - val_loss: 0.4783\n",
      "Epoch 160/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0935 - val_loss: 0.4790\n",
      "Epoch 161/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1057 - val_loss: 0.2599\n",
      "Epoch 162/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1383 - val_loss: 0.3470\n",
      "Epoch 163/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1465 - val_loss: 0.3222\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1117 - val_loss: 0.5169\n",
      "Epoch 165/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0886 - val_loss: 0.5608\n",
      "Epoch 166/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0876 - val_loss: 0.6804\n",
      "Epoch 167/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0863 - val_loss: 0.5824\n",
      "Epoch 168/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0822 - val_loss: 0.6377\n",
      "Epoch 169/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0738 - val_loss: 0.6483\n",
      "Epoch 170/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0763 - val_loss: 0.6863\n",
      "Epoch 171/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0714 - val_loss: 0.7605\n",
      "Epoch 172/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0781 - val_loss: 0.6197\n",
      "Epoch 173/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0731 - val_loss: 0.4883\n",
      "Epoch 174/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0798 - val_loss: 0.7438\n",
      "Epoch 175/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0720 - val_loss: 0.6353\n",
      "Epoch 176/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0661 - val_loss: 0.5486\n",
      "Epoch 177/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0677 - val_loss: 0.6768\n",
      "Epoch 178/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0740 - val_loss: 0.7974\n",
      "Epoch 179/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1206 - val_loss: 0.6161\n",
      "Epoch 180/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0869 - val_loss: 0.4199\n",
      "Epoch 181/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0742 - val_loss: 0.4350\n",
      "Epoch 182/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0748 - val_loss: 0.2588\n",
      "Epoch 183/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0985 - val_loss: 0.7941\n",
      "Epoch 184/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1116 - val_loss: 0.6915\n",
      "Epoch 185/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0772 - val_loss: 0.6857\n",
      "Epoch 186/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0744 - val_loss: 0.6906\n",
      "Epoch 187/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0675 - val_loss: 0.7596\n",
      "Epoch 188/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0706 - val_loss: 0.6283\n",
      "Epoch 189/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0756 - val_loss: 0.8792\n",
      "Epoch 190/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0697 - val_loss: 0.6142\n",
      "Epoch 191/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0706 - val_loss: 0.7762\n",
      "Epoch 192/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0683 - val_loss: 0.6622\n",
      "Epoch 193/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0719 - val_loss: 0.8221\n",
      "Epoch 194/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0662 - val_loss: 0.6556\n",
      "Epoch 195/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0688 - val_loss: 0.7795\n",
      "Epoch 196/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0675 - val_loss: 0.7551\n",
      "Epoch 197/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0677 - val_loss: 0.6981\n",
      "Epoch 198/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0702 - val_loss: 0.7604\n",
      "Epoch 199/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0692 - val_loss: 0.6863\n",
      "Epoch 200/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0688 - val_loss: 0.8665\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "Epoch 1/200\n",
      "32/32 [==============================] - 1s 11ms/step - loss: 1.1868 - val_loss: 1.0501\n",
      "Epoch 2/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6370 - val_loss: 0.6938\n",
      "Epoch 3/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.5096 - val_loss: 1.2132\n",
      "Epoch 4/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.4225 - val_loss: 1.0914\n",
      "Epoch 5/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.3910 - val_loss: 1.1999\n",
      "Epoch 6/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.3874 - val_loss: 1.6341\n",
      "Epoch 7/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.3696 - val_loss: 1.4072\n",
      "Epoch 8/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.3589 - val_loss: 1.6675\n",
      "Epoch 9/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.3946 - val_loss: 0.7488\n",
      "Epoch 10/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.3349 - val_loss: 1.0733\n",
      "Epoch 11/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.3557 - val_loss: 1.3812\n",
      "Epoch 12/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.3113 - val_loss: 1.3929\n",
      "Epoch 13/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.3184 - val_loss: 1.3374\n",
      "Epoch 14/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2758 - val_loss: 0.3655\n",
      "Epoch 15/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2990 - val_loss: 1.1140\n",
      "Epoch 16/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.2749 - val_loss: 1.2551\n",
      "Epoch 17/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2601 - val_loss: 1.4402\n",
      "Epoch 18/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.2544 - val_loss: 0.6930\n",
      "Epoch 19/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1901 - val_loss: 0.5673\n",
      "Epoch 20/200\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2613 - val_loss: 0.4066\n",
      "Epoch 21/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.2373 - val_loss: 0.7409\n",
      "Epoch 22/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2387 - val_loss: 0.5075\n",
      "Epoch 23/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.1755 - val_loss: 0.6433\n",
      "Epoch 24/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.3236 - val_loss: 0.6183\n",
      "Epoch 25/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1787 - val_loss: 0.3248\n",
      "Epoch 26/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1765 - val_loss: 0.7043\n",
      "Epoch 27/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1386 - val_loss: 0.3960\n",
      "Epoch 28/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1552 - val_loss: 0.4468\n",
      "Epoch 29/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1428 - val_loss: 0.3506\n",
      "Epoch 30/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1227 - val_loss: 0.5028\n",
      "Epoch 31/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1615 - val_loss: 0.5510\n",
      "Epoch 32/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1438 - val_loss: 0.6833\n",
      "Epoch 33/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1218 - val_loss: 0.5014\n",
      "Epoch 34/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1419 - val_loss: 0.2983\n",
      "Epoch 35/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1500 - val_loss: 0.7442\n",
      "Epoch 36/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1340 - val_loss: 0.3611\n",
      "Epoch 37/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.1372 - val_loss: 0.4227\n",
      "Epoch 38/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1097 - val_loss: 0.4418\n",
      "Epoch 39/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.1291 - val_loss: 0.3929\n",
      "Epoch 40/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1435 - val_loss: 0.8189\n",
      "Epoch 41/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1595 - val_loss: 0.5455\n",
      "Epoch 42/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.1166 - val_loss: 0.4104\n",
      "Epoch 43/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1386 - val_loss: 0.5259\n",
      "Epoch 44/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1172 - val_loss: 0.4859\n",
      "Epoch 45/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 6ms/step - loss: 0.1207 - val_loss: 0.4483\n",
      "Epoch 46/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1019 - val_loss: 0.3410\n",
      "Epoch 47/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0911 - val_loss: 0.3956\n",
      "Epoch 48/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1181 - val_loss: 0.4533\n",
      "Epoch 49/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.2003 - val_loss: 0.4781\n",
      "Epoch 50/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1285 - val_loss: 0.4429\n",
      "Epoch 51/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1318 - val_loss: 0.2639\n",
      "Epoch 52/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1611 - val_loss: 0.3368\n",
      "Epoch 53/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1089 - val_loss: 0.2403\n",
      "Epoch 54/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1354 - val_loss: 0.4726\n",
      "Epoch 55/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1134 - val_loss: 0.4976\n",
      "Epoch 56/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0988 - val_loss: 0.4588\n",
      "Epoch 57/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1061 - val_loss: 0.4476\n",
      "Epoch 58/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1052 - val_loss: 0.4739\n",
      "Epoch 59/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1170 - val_loss: 0.4705\n",
      "Epoch 60/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0923 - val_loss: 0.5984\n",
      "Epoch 61/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.3347 - val_loss: 0.6299\n",
      "Epoch 62/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2218 - val_loss: 0.3653\n",
      "Epoch 63/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.2163 - val_loss: 0.4385\n",
      "Epoch 64/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1353 - val_loss: 0.5005\n",
      "Epoch 65/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1498 - val_loss: 0.1653\n",
      "Epoch 66/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1361 - val_loss: 0.2492\n",
      "Epoch 67/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0995 - val_loss: 0.2997\n",
      "Epoch 68/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0927 - val_loss: 0.3208\n",
      "Epoch 69/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0944 - val_loss: 0.3363\n",
      "Epoch 70/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0966 - val_loss: 0.3810\n",
      "Epoch 71/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0949 - val_loss: 0.2794\n",
      "Epoch 72/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0890 - val_loss: 0.2371\n",
      "Epoch 73/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1015 - val_loss: 0.2252\n",
      "Epoch 74/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1106 - val_loss: 0.4190\n",
      "Epoch 75/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0995 - val_loss: 0.4219\n",
      "Epoch 76/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0960 - val_loss: 0.3207\n",
      "Epoch 77/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0881 - val_loss: 0.2668\n",
      "Epoch 78/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1218 - val_loss: 0.6128\n",
      "Epoch 79/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1019 - val_loss: 0.5825\n",
      "Epoch 80/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1025 - val_loss: 0.2422\n",
      "Epoch 81/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.1092 - val_loss: 0.2866\n",
      "Epoch 82/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1290 - val_loss: 0.1804\n",
      "Epoch 83/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1094 - val_loss: 0.5861\n",
      "Epoch 84/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1694 - val_loss: 0.1810\n",
      "Epoch 85/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1807 - val_loss: 0.5860\n",
      "Epoch 86/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1452 - val_loss: 0.4318\n",
      "Epoch 87/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0994 - val_loss: 0.5772\n",
      "Epoch 88/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1023 - val_loss: 0.3671\n",
      "Epoch 89/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0850 - val_loss: 0.2099\n",
      "Epoch 90/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0996 - val_loss: 0.2816\n",
      "Epoch 91/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0980 - val_loss: 0.2540\n",
      "Epoch 92/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0960 - val_loss: 0.3697\n",
      "Epoch 93/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0838 - val_loss: 0.3391\n",
      "Epoch 94/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0752 - val_loss: 0.4152\n",
      "Epoch 95/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0852 - val_loss: 0.4947\n",
      "Epoch 96/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0790 - val_loss: 0.3560\n",
      "Epoch 97/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0805 - val_loss: 0.3536\n",
      "Epoch 98/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0835 - val_loss: 0.4384\n",
      "Epoch 99/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0954 - val_loss: 0.3711\n",
      "Epoch 100/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1658 - val_loss: 0.3955\n",
      "Epoch 101/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1124 - val_loss: 0.3909\n",
      "Epoch 102/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0906 - val_loss: 0.2679\n",
      "Epoch 103/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0932 - val_loss: 0.4626\n",
      "Epoch 104/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0858 - val_loss: 0.2418\n",
      "Epoch 105/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0685 - val_loss: 0.3059\n",
      "Epoch 106/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0696 - val_loss: 0.2930\n",
      "Epoch 107/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0726 - val_loss: 0.3943\n",
      "Epoch 108/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0725 - val_loss: 0.4967\n",
      "Epoch 109/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0705 - val_loss: 0.4024\n",
      "Epoch 110/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0816 - val_loss: 0.2724\n",
      "Epoch 111/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0830 - val_loss: 0.4656\n",
      "Epoch 112/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0832 - val_loss: 0.2589\n",
      "Epoch 113/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1051 - val_loss: 0.6897\n",
      "Epoch 114/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0927 - val_loss: 0.3714\n",
      "Epoch 115/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1065 - val_loss: 0.5081\n",
      "Epoch 116/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0912 - val_loss: 0.4045\n",
      "Epoch 117/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0798 - val_loss: 0.5681\n",
      "Epoch 118/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0844 - val_loss: 0.5798\n",
      "Epoch 119/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0735 - val_loss: 0.5137\n",
      "Epoch 120/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0738 - val_loss: 0.6950\n",
      "Epoch 121/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0716 - val_loss: 0.6363\n",
      "Epoch 122/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0773 - val_loss: 0.6177\n",
      "Epoch 123/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0872 - val_loss: 0.6396\n",
      "Epoch 124/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0806 - val_loss: 0.4857\n",
      "Epoch 125/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0866 - val_loss: 0.5412\n",
      "Epoch 126/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0889 - val_loss: 0.3332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0951 - val_loss: 0.5476\n",
      "Epoch 128/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0924 - val_loss: 0.4702\n",
      "Epoch 129/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1760 - val_loss: 0.6242\n",
      "Epoch 130/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0942 - val_loss: 0.3559\n",
      "Epoch 131/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1010 - val_loss: 0.2737\n",
      "Epoch 132/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1231 - val_loss: 0.4957\n",
      "Epoch 133/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0799 - val_loss: 0.2062\n",
      "Epoch 134/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0772 - val_loss: 0.2761\n",
      "Epoch 135/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0667 - val_loss: 0.3904\n",
      "Epoch 136/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0701 - val_loss: 0.3825\n",
      "Epoch 137/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0678 - val_loss: 0.4566\n",
      "Epoch 138/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0724 - val_loss: 0.3155\n",
      "Epoch 139/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0718 - val_loss: 0.4488\n",
      "Epoch 140/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0633 - val_loss: 0.4250\n",
      "Epoch 141/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1042 - val_loss: 0.3873\n",
      "Epoch 142/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.1048 - val_loss: 0.6018\n",
      "Epoch 143/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0866 - val_loss: 0.1742\n",
      "Epoch 144/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0988 - val_loss: 0.4800\n",
      "Epoch 145/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0790 - val_loss: 0.5572\n",
      "Epoch 146/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0662 - val_loss: 0.4380\n",
      "Epoch 147/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0690 - val_loss: 0.3088\n",
      "Epoch 148/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0756 - val_loss: 0.5577\n",
      "Epoch 149/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0807 - val_loss: 0.5768\n",
      "Epoch 150/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0653 - val_loss: 0.5432\n",
      "Epoch 151/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0638 - val_loss: 0.5687\n",
      "Epoch 152/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0654 - val_loss: 0.5174\n",
      "Epoch 153/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0818 - val_loss: 0.5753\n",
      "Epoch 154/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0687 - val_loss: 0.7120\n",
      "Epoch 155/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0647 - val_loss: 0.6139\n",
      "Epoch 156/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0666 - val_loss: 0.5149\n",
      "Epoch 157/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0706 - val_loss: 0.4804\n",
      "Epoch 158/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0758 - val_loss: 0.5585\n",
      "Epoch 159/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0860 - val_loss: 0.4525\n",
      "Epoch 160/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0906 - val_loss: 0.4430\n",
      "Epoch 161/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0715 - val_loss: 0.5412\n",
      "Epoch 162/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0672 - val_loss: 0.5335\n",
      "Epoch 163/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0981 - val_loss: 0.6519\n",
      "Epoch 164/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0745 - val_loss: 0.4697\n",
      "Epoch 165/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0782 - val_loss: 0.5926\n",
      "Epoch 166/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0675 - val_loss: 0.5634\n",
      "Epoch 167/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0803 - val_loss: 0.6489\n",
      "Epoch 168/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0897 - val_loss: 0.6025\n",
      "Epoch 169/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0992 - val_loss: 0.3490\n",
      "Epoch 170/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0916 - val_loss: 0.6481\n",
      "Epoch 171/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0798 - val_loss: 0.5878\n",
      "Epoch 172/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0652 - val_loss: 0.5970\n",
      "Epoch 173/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0605 - val_loss: 0.6226\n",
      "Epoch 174/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0651 - val_loss: 0.5750\n",
      "Epoch 175/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0936 - val_loss: 0.6099\n",
      "Epoch 176/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0821 - val_loss: 0.7634\n",
      "Epoch 177/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0832 - val_loss: 0.4934\n",
      "Epoch 178/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0721 - val_loss: 0.6415\n",
      "Epoch 179/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1304 - val_loss: 0.6308\n",
      "Epoch 180/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1548 - val_loss: 0.6476\n",
      "Epoch 181/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1215 - val_loss: 0.4410\n",
      "Epoch 182/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.3249 - val_loss: 0.9279\n",
      "Epoch 183/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1826 - val_loss: 0.4441\n",
      "Epoch 184/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1193 - val_loss: 0.4838\n",
      "Epoch 185/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1036 - val_loss: 0.4470\n",
      "Epoch 186/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1071 - val_loss: 0.2299\n",
      "Epoch 187/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1650 - val_loss: 0.4297\n",
      "Epoch 188/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0896 - val_loss: 0.3520\n",
      "Epoch 189/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0820 - val_loss: 0.4507\n",
      "Epoch 190/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0847 - val_loss: 0.7625\n",
      "Epoch 191/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0947 - val_loss: 0.5694\n",
      "Epoch 192/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0742 - val_loss: 0.6576\n",
      "Epoch 193/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0771 - val_loss: 0.6125\n",
      "Epoch 194/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0852 - val_loss: 0.5388\n",
      "Epoch 195/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0821 - val_loss: 0.6094\n",
      "Epoch 196/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0820 - val_loss: 0.6015\n",
      "Epoch 197/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0977 - val_loss: 0.3175\n",
      "Epoch 198/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0741 - val_loss: 0.2279\n",
      "Epoch 199/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0872 - val_loss: 0.4313\n",
      "Epoch 200/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0706 - val_loss: 0.5473\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Mean Squared Error on Test Data (Stacking with GradientBoostingForest): 0.01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Build and train the first neural network (base model 1)\n",
    "model_nn_base_1 = Sequential()\n",
    "model_nn_base_1.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(10, activation='relu'))\n",
    "model_nn_base_1.add(Dense(1, activation='linear'))\n",
    "model_nn_base_1.compile(optimizer=Adam(lr=0.01), loss='mean_squared_error')\n",
    "model_nn_base_1.fit(X_train, y_train, epochs=200, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the first base neural network model\n",
    "y_pred_nn_base_1 = model_nn_base_1.predict(X_test)\n",
    "\n",
    "# Build and train the second neural network (base model 2)\n",
    "model_nn_base_2 = Sequential()\n",
    "model_nn_base_2.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(10, activation='relu'))\n",
    "model_nn_base_2.add(Dense(5, activation='relu'))\n",
    "model_nn_base_2.add(Dense(1, activation='linear'))\n",
    "model_nn_base_2.compile(optimizer=Adam(lr=0.01), loss='mean_squared_error')\n",
    "model_nn_base_2.fit(X_train, y_train, epochs=200, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the second base neural network model\n",
    "y_pred_nn_base_2 = model_nn_base_2.predict(X_test)\n",
    "\n",
    "# Combine predictions from both base models\n",
    "X_meta_train = np.concatenate((y_pred_nn_base_1, y_pred_nn_base_2), axis=1)\n",
    "\n",
    "# Build and train a Random Forest Regressor as a meta-learner\n",
    "model_rf_meta = GradientBoostingRegressor(n_estimators=150, random_state=42)\n",
    "model_rf_meta.fit(X_meta_train, y_test)\n",
    "\n",
    "# Make predictions using the base models and meta-learner\n",
    "X_meta_test = np.concatenate((model_nn_base_1.predict(X_test), model_nn_base_2.predict(X_test)), axis=1)\n",
    "y_pred_stacking_rf = model_rf_meta.predict(X_meta_test)\n",
    "\n",
    "# Evaluate the stacking model with Random Forest as meta-learner\n",
    "mse_stacking_rf = mean_squared_error(y_test, y_pred_stacking_rf)\n",
    "print(f\"Mean Squared Error on Test Data (Stacking with GradientBoostingForest): {mse_stacking_rf:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe98007",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the model on unseen data\n",
    "y_pred_nn_base_1 = model_nn_base_1.predict(X_tr)\n",
    "y_pred_nn_base_2 = model_nn_base_2.predict(X_tr)\n",
    "X_meta_test = np.concatenate((model_nn_base_1.predict(X_tr), model_nn_base_2.predict(X_tr)), axis=1)\n",
    "y_pred_stacking_rf = model_rf_meta.predict(X_meta_test)\n",
    "\n",
    "mse = mean_squared_error(y_pred_stacking_rf, y_tr)\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse_stacking_rf)\n",
    "print(f\"Root Mean Squared Error (RMSE) on unseen data : {rmse:.2f}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape \n",
    "df = pd.read_csv('Ragi.csv')\n",
    "y = df['Yield']\n",
    "rmse = np.sqrt(0.01)\n",
    "p = (rmse / np.mean(y)) * 100 \n",
    "\n",
    "print(f\" Percentage yield for Ragi : {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db9675a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "crp5=pd.read_csv('Tapioca.csv')\n",
    "gj1=crp5.drop(['Crop','Year','Season','State','TotalRainfall'],axis=1)\n",
    "import numpy as np\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "x = crp5[features]\n",
    "y = crp5['Yield']\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sx=StandardScaler().fit(x)\n",
    "y=np.array(y)\n",
    "x=sx.transform(x)\n",
    "X_train=x[:90]\n",
    "X_test=x[90:100]\n",
    "y_train=y[:90]\n",
    "y_test=y[90:100]\n",
    "X_tr=x[100:]\n",
    "y_tr=y[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7347322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 17ms/step - loss: 298.7336 - val_loss: 51.4676\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 77.0985 - val_loss: 72.4565\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 39.6047 - val_loss: 64.8149\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 27.7498 - val_loss: 40.8376\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 26.1279 - val_loss: 39.6516\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 32.8070 - val_loss: 26.6164\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 23.4342 - val_loss: 41.0146\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 18.8410 - val_loss: 50.6770\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.5617 - val_loss: 19.3169\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 17.3955 - val_loss: 36.4353\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 17.1965 - val_loss: 64.9218\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 22.6659 - val_loss: 21.0060\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 15.1257 - val_loss: 19.9095\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 18.2019 - val_loss: 42.4161\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 16.7749 - val_loss: 26.7896\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.4413 - val_loss: 14.9351\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 18.1726 - val_loss: 19.8673\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.1391 - val_loss: 17.0517\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 16.9429 - val_loss: 33.1743\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.4006 - val_loss: 28.2401\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.8895 - val_loss: 18.2920\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.0811 - val_loss: 50.3706\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.7381 - val_loss: 27.5225\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 13.9418 - val_loss: 22.7980\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 13.9388 - val_loss: 19.9014\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.1330 - val_loss: 32.6973\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 17.9962 - val_loss: 20.0749\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.7566 - val_loss: 27.7482\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.5825 - val_loss: 27.0952\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 14.0526 - val_loss: 40.4755\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.3395 - val_loss: 38.8873\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.5655 - val_loss: 19.6887\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 17.1528 - val_loss: 19.1989\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 10.9077 - val_loss: 33.0870\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.6220 - val_loss: 18.4028\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 14.0503 - val_loss: 20.7157\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 16.0260 - val_loss: 21.2463\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 14.2804 - val_loss: 20.9024\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.1384 - val_loss: 22.1990\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.2251 - val_loss: 19.2004\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 13.3559 - val_loss: 24.7088\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.2452 - val_loss: 17.7390\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.9751 - val_loss: 19.3599\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.1224 - val_loss: 19.0573\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 16.9339 - val_loss: 19.0374\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.2672 - val_loss: 17.4155\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 17.3505 - val_loss: 23.9813\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.7980 - val_loss: 42.2320\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.7881 - val_loss: 16.4582\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.0619 - val_loss: 20.1321\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "Epoch 1/50\n",
      "13/13 [==============================] - 1s 16ms/step - loss: 411.0875 - val_loss: 484.6593\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 195.0617 - val_loss: 47.2023\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 54.3692 - val_loss: 47.8831\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 32.8554 - val_loss: 38.6217\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 31.6500 - val_loss: 29.8665\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.2837 - val_loss: 22.1664\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 34.1266 - val_loss: 29.7171\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 37.9868 - val_loss: 21.6201\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.0746 - val_loss: 32.4722\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 18.5048 - val_loss: 37.7106\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.4217 - val_loss: 43.7663\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 18.7245 - val_loss: 57.1208\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.4072 - val_loss: 27.3986\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.4125 - val_loss: 30.7870\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.0546 - val_loss: 49.4594\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 18.0648 - val_loss: 21.2195\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 18.9988 - val_loss: 17.3887\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.3710 - val_loss: 15.9989\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.5772 - val_loss: 26.2341\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.7501 - val_loss: 27.7900\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.3761 - val_loss: 41.9334\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2929 - val_loss: 28.4157\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.9956 - val_loss: 34.0812\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 17.6444 - val_loss: 26.0603\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 18.1982 - val_loss: 22.2514\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 22.4059 - val_loss: 18.5237\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 17.3513 - val_loss: 26.8565\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.3191 - val_loss: 26.3570\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 13.6241 - val_loss: 30.5563\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.8032 - val_loss: 43.5817\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 17.6401 - val_loss: 19.6092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.1733 - val_loss: 45.7513\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 18.5767 - val_loss: 33.2605\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 16.5273 - val_loss: 33.8340\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 16.6181 - val_loss: 43.0612\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.7604 - val_loss: 30.2464\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.6521 - val_loss: 55.0703\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 16.5323 - val_loss: 23.2892\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.0342 - val_loss: 25.9177\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.1017 - val_loss: 24.0948\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 13.9990 - val_loss: 21.1570\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 13.8139 - val_loss: 41.6412\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 16.7946 - val_loss: 66.6931\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 22.4089 - val_loss: 44.8016\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 16.3213 - val_loss: 37.4720\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.1368 - val_loss: 24.1534\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 13.0254 - val_loss: 18.0776\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.4804 - val_loss: 22.5091\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 16.4374 - val_loss: 20.3669\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.7811 - val_loss: 17.0649\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Mean Squared Error on Test Data (Stacking with GradientBoostingForest): 0.02\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Build and train the first neural network (base model 1)\n",
    "model_nn_base_1 = Sequential()\n",
    "model_nn_base_1.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(10, activation='relu'))\n",
    "model_nn_base_1.add(Dense(1, activation='linear'))\n",
    "model_nn_base_1.compile(optimizer=Adam(lr=0.01), loss='mean_squared_error')\n",
    "model_nn_base_1.fit(X_train, y_train, epochs=50, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the first base neural network model\n",
    "y_pred_nn_base_1 = model_nn_base_1.predict(X_test)\n",
    "\n",
    "# Build and train the second neural network (base model 2)\n",
    "model_nn_base_2 = Sequential()\n",
    "model_nn_base_2.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(10, activation='relu'))\n",
    "model_nn_base_2.add(Dense(5, activation='relu'))\n",
    "model_nn_base_2.add(Dense(1, activation='linear'))\n",
    "model_nn_base_2.compile(optimizer=Adam(lr=0.01), loss='mean_squared_error')\n",
    "model_nn_base_2.fit(X_train, y_train, epochs=50, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the second base neural network model\n",
    "y_pred_nn_base_2 = model_nn_base_2.predict(X_test)\n",
    "\n",
    "# Combine predictions from both base models\n",
    "X_meta_train = np.concatenate((y_pred_nn_base_1, y_pred_nn_base_2), axis=1)\n",
    "\n",
    "# Build and train a Random Forest Regressor as a meta-learner\n",
    "model_rf_meta = GradientBoostingRegressor(n_estimators=100,learning_rate=0.05, random_state=42)\n",
    "model_rf_meta.fit(X_meta_train, y_test)\n",
    "\n",
    "# Make predictions using the base models and meta-learner\n",
    "X_meta_test = np.concatenate((model_nn_base_1.predict(X_test), model_nn_base_2.predict(X_test)), axis=1)\n",
    "y_pred_stacking_rf = model_rf_meta.predict(X_meta_test)\n",
    "\n",
    "# Evaluate the stacking model with Random Forest as meta-learner\n",
    "mse_stacking_rf = mean_squared_error(y_test, y_pred_stacking_rf)\n",
    "print(f\"Mean Squared Error on Test Data (Stacking with GradientBoostingForest): {mse_stacking_rf:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a84cdbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Root Mean Squared Error (RMSE) on unseen data : 0.13\n",
      " Percentage yield for Tapioca : 0.8157810742850726\n"
     ]
    }
   ],
   "source": [
    "#Testing the model on unseen data\n",
    "y_pred_nn_base_1 = model_nn_base_1.predict(X_tr)\n",
    "y_pred_nn_base_2 = model_nn_base_2.predict(X_tr)\n",
    "X_meta_test = np.concatenate((model_nn_base_1.predict(X_tr), model_nn_base_2.predict(X_tr)), axis=1)\n",
    "y_pred_stacking_rf = model_rf_meta.predict(X_meta_test)\n",
    "\n",
    "mse = mean_squared_error(y_pred_stacking_rf, y_tr)\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse_stacking_rf)\n",
    "print(f\"Root Mean Squared Error (RMSE) on unseen data : {rmse:.2f}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape \n",
    "df = pd.read_csv('Tapioca.csv')\n",
    "y = df['Yield']\n",
    "rmse = np.sqrt(0.02)\n",
    "p = (rmse / np.mean(y)) * 100 \n",
    "\n",
    "print(f\" Percentage yield for Tapioca : {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cdbaace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "crp5=pd.read_csv('Soyabean.csv')\n",
    "gj1=crp5.drop(['Crop','Year','Season','State','TotalRainfall'],axis=1)\n",
    "import numpy as np\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "x = crp5[features]\n",
    "y = crp5['Yield']\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sx=StandardScaler().fit(x)\n",
    "y=np.array(y)\n",
    "x=sx.transform(x)\n",
    "X_train=x[:160]\n",
    "X_test=x[160:170]\n",
    "y_train=y[:160]\n",
    "y_test=y[160:170]\n",
    "X_tr=x[170:]\n",
    "y_tr=y[170:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ff73ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 10ms/step - loss: 0.4964 - val_loss: 0.2200\n",
      "Epoch 2/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1701 - val_loss: 0.0998\n",
      "Epoch 3/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1443 - val_loss: 0.1084\n",
      "Epoch 4/70\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1438 - val_loss: 0.1150\n",
      "Epoch 5/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1371 - val_loss: 0.0396\n",
      "Epoch 6/70\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1590 - val_loss: 0.0425\n",
      "Epoch 7/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1549 - val_loss: 0.0605\n",
      "Epoch 8/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1420 - val_loss: 0.0795\n",
      "Epoch 9/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1393 - val_loss: 0.0445\n",
      "Epoch 10/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1393 - val_loss: 0.0577\n",
      "Epoch 11/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1299 - val_loss: 0.0119\n",
      "Epoch 12/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1509 - val_loss: 0.0742\n",
      "Epoch 13/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1290 - val_loss: 0.0718\n",
      "Epoch 14/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1373 - val_loss: 0.0253\n",
      "Epoch 15/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1618 - val_loss: 0.0413\n",
      "Epoch 16/70\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1326 - val_loss: 0.0843\n",
      "Epoch 17/70\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1193 - val_loss: 0.1077\n",
      "Epoch 18/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1254 - val_loss: 0.0919\n",
      "Epoch 19/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1110 - val_loss: 0.0301\n",
      "Epoch 20/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1182 - val_loss: 0.0317\n",
      "Epoch 21/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1175 - val_loss: 0.0379\n",
      "Epoch 22/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1185 - val_loss: 0.1172\n",
      "Epoch 23/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1150 - val_loss: 0.1315\n",
      "Epoch 24/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1456 - val_loss: 0.0588\n",
      "Epoch 25/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1226 - val_loss: 0.0708\n",
      "Epoch 26/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1232 - val_loss: 0.0594\n",
      "Epoch 27/70\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.1153 - val_loss: 0.0967\n",
      "Epoch 28/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1426 - val_loss: 0.0248\n",
      "Epoch 29/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1254 - val_loss: 0.0477\n",
      "Epoch 30/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1333 - val_loss: 0.0403\n",
      "Epoch 31/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1329 - val_loss: 0.0358\n",
      "Epoch 32/70\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1153 - val_loss: 0.0295\n",
      "Epoch 33/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1229 - val_loss: 0.0542\n",
      "Epoch 34/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1085 - val_loss: 0.0415\n",
      "Epoch 35/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1150 - val_loss: 0.0737\n",
      "Epoch 36/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1121 - val_loss: 0.0566\n",
      "Epoch 37/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1012 - val_loss: 0.0805\n",
      "Epoch 38/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0995 - val_loss: 0.0838\n",
      "Epoch 39/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1095 - val_loss: 0.0651\n",
      "Epoch 40/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1228 - val_loss: 0.1096\n",
      "Epoch 41/70\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.1141 - val_loss: 0.0617\n",
      "Epoch 42/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1137 - val_loss: 0.0721\n",
      "Epoch 43/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1068 - val_loss: 0.0803\n",
      "Epoch 44/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1055 - val_loss: 0.1078\n",
      "Epoch 45/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1003 - val_loss: 0.0724\n",
      "Epoch 46/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1048 - val_loss: 0.1317\n",
      "Epoch 47/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1037 - val_loss: 0.1059\n",
      "Epoch 48/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0967 - val_loss: 0.1414\n",
      "Epoch 49/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0967 - val_loss: 0.0607\n",
      "Epoch 50/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1003 - val_loss: 0.0777\n",
      "Epoch 51/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1086 - val_loss: 0.0814\n",
      "Epoch 52/70\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1125 - val_loss: 0.0608\n",
      "Epoch 53/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1049 - val_loss: 0.0562\n",
      "Epoch 54/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0971 - val_loss: 0.1085\n",
      "Epoch 55/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1010 - val_loss: 0.0495\n",
      "Epoch 56/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1086 - val_loss: 0.1099\n",
      "Epoch 57/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1176 - val_loss: 0.0736\n",
      "Epoch 58/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1078 - val_loss: 0.1194\n",
      "Epoch 59/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1051 - val_loss: 0.0379\n",
      "Epoch 60/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0964 - val_loss: 0.0869\n",
      "Epoch 61/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0911 - val_loss: 0.1110\n",
      "Epoch 62/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0944 - val_loss: 0.0894\n",
      "Epoch 63/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0903 - val_loss: 0.0661\n",
      "Epoch 64/70\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.1043 - val_loss: 0.2609\n",
      "Epoch 65/70\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.1103 - val_loss: 0.0679\n",
      "Epoch 66/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1102 - val_loss: 0.3100\n",
      "Epoch 67/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1107 - val_loss: 0.1577\n",
      "Epoch 68/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0991 - val_loss: 0.1266\n",
      "Epoch 69/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1057 - val_loss: 0.1772\n",
      "Epoch 70/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1019 - val_loss: 0.1425\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Epoch 1/70\n",
      "20/20 [==============================] - 1s 12ms/step - loss: 0.4262 - val_loss: 0.0527\n",
      "Epoch 2/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1740 - val_loss: 0.1544\n",
      "Epoch 3/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1693 - val_loss: 0.1867\n",
      "Epoch 4/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1481 - val_loss: 0.1208\n",
      "Epoch 5/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1548 - val_loss: 0.1226\n",
      "Epoch 6/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1412 - val_loss: 0.1907\n",
      "Epoch 7/70\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1755 - val_loss: 0.0712\n",
      "Epoch 8/70\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1564 - val_loss: 0.0525\n",
      "Epoch 9/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1489 - val_loss: 0.1442\n",
      "Epoch 10/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1413 - val_loss: 0.0964\n",
      "Epoch 11/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1387 - val_loss: 0.0567\n",
      "Epoch 12/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1353 - val_loss: 0.1217\n",
      "Epoch 13/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1474 - val_loss: 0.0327\n",
      "Epoch 14/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1408 - val_loss: 0.0375\n",
      "Epoch 15/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1304 - val_loss: 0.0405\n",
      "Epoch 16/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1198 - val_loss: 0.0587\n",
      "Epoch 17/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1373 - val_loss: 0.0383\n",
      "Epoch 18/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1327 - val_loss: 0.1231\n",
      "Epoch 19/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1257 - val_loss: 0.0507\n",
      "Epoch 20/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1549 - val_loss: 0.0655\n",
      "Epoch 21/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1289 - val_loss: 0.1171\n",
      "Epoch 22/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1370 - val_loss: 0.1908\n",
      "Epoch 23/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1593 - val_loss: 0.1153\n",
      "Epoch 24/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1310 - val_loss: 0.0858\n",
      "Epoch 25/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1273 - val_loss: 0.0534\n",
      "Epoch 26/70\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1317 - val_loss: 0.0381\n",
      "Epoch 27/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1192 - val_loss: 0.0540\n",
      "Epoch 28/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1104 - val_loss: 0.1228\n",
      "Epoch 29/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1239 - val_loss: 0.0671\n",
      "Epoch 30/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1250 - val_loss: 0.1234\n",
      "Epoch 31/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1169 - val_loss: 0.1017\n",
      "Epoch 32/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1032 - val_loss: 0.0241\n",
      "Epoch 33/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1180 - val_loss: 0.1718\n",
      "Epoch 34/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1275 - val_loss: 0.0392\n",
      "Epoch 35/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1238 - val_loss: 0.0410\n",
      "Epoch 36/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1038 - val_loss: 0.1472\n",
      "Epoch 37/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1101 - val_loss: 0.0580\n",
      "Epoch 38/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1165 - val_loss: 0.0612\n",
      "Epoch 39/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1076 - val_loss: 0.0514\n",
      "Epoch 40/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1109 - val_loss: 0.1182\n",
      "Epoch 41/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1212 - val_loss: 0.0790\n",
      "Epoch 42/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1099 - val_loss: 0.1107\n",
      "Epoch 43/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1124 - val_loss: 0.1061\n",
      "Epoch 44/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1232 - val_loss: 0.1557\n",
      "Epoch 45/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1181 - val_loss: 0.1812\n",
      "Epoch 46/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1087 - val_loss: 0.0277\n",
      "Epoch 47/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1272 - val_loss: 0.1023\n",
      "Epoch 48/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1088 - val_loss: 0.1081\n",
      "Epoch 49/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1180 - val_loss: 0.1076\n",
      "Epoch 50/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1070 - val_loss: 0.0831\n",
      "Epoch 51/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1037 - val_loss: 0.0826\n",
      "Epoch 52/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1125 - val_loss: 0.1436\n",
      "Epoch 53/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1280 - val_loss: 0.0703\n",
      "Epoch 54/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0979 - val_loss: 0.1535\n",
      "Epoch 55/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0991 - val_loss: 0.1272\n",
      "Epoch 56/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0940 - val_loss: 0.0548\n",
      "Epoch 57/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1192 - val_loss: 0.2646\n",
      "Epoch 58/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1625 - val_loss: 0.2081\n",
      "Epoch 59/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1530 - val_loss: 0.0586\n",
      "Epoch 60/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1602 - val_loss: 0.1244\n",
      "Epoch 61/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1398 - val_loss: 0.0855\n",
      "Epoch 62/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1423 - val_loss: 0.0776\n",
      "Epoch 63/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1276 - val_loss: 0.0948\n",
      "Epoch 64/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1286 - val_loss: 0.0609\n",
      "Epoch 65/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1343 - val_loss: 0.0715\n",
      "Epoch 66/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1243 - val_loss: 0.0600\n",
      "Epoch 67/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1281 - val_loss: 0.0285\n",
      "Epoch 68/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1574 - val_loss: 0.0840\n",
      "Epoch 69/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1429 - val_loss: 0.0683\n",
      "Epoch 70/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1290 - val_loss: 0.1473\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Mean Squared Error on Test Data (Stacking with GradientBoostingForest): 0.01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Build and train the first neural network (base model 1)\n",
    "model_nn_base_1 = Sequential()\n",
    "model_nn_base_1.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(10, activation='relu'))\n",
    "model_nn_base_1.add(Dense(1, activation='linear'))\n",
    "model_nn_base_1.compile(optimizer=Adam(lr=0.01), loss='mean_squared_error')\n",
    "model_nn_base_1.fit(X_train, y_train, epochs=70, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the first base neural network model\n",
    "y_pred_nn_base_1 = model_nn_base_1.predict(X_test)\n",
    "\n",
    "# Build and train the second neural network (base model 2)\n",
    "model_nn_base_2 = Sequential()\n",
    "model_nn_base_2.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(10, activation='relu'))\n",
    "model_nn_base_2.add(Dense(5, activation='relu'))\n",
    "model_nn_base_2.add(Dense(1, activation='linear'))\n",
    "model_nn_base_2.compile(optimizer=Adam(lr=0.01), loss='mean_squared_error')\n",
    "model_nn_base_2.fit(X_train, y_train, epochs=70, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the second base neural network model\n",
    "y_pred_nn_base_2 = model_nn_base_2.predict(X_test)\n",
    "\n",
    "# Combine predictions from both base models\n",
    "X_meta_train = np.concatenate((y_pred_nn_base_1, y_pred_nn_base_2), axis=1)\n",
    "\n",
    "# Build and train a Random Forest Regressor as a meta-learner\n",
    "model_rf_meta = GradientBoostingRegressor(n_estimators=100,learning_rate=0.001, random_state=42)\n",
    "model_rf_meta.fit(X_meta_train, y_test)\n",
    "\n",
    "# Make predictions using the base models and meta-learner\n",
    "X_meta_test = np.concatenate((model_nn_base_1.predict(X_test), model_nn_base_2.predict(X_test)), axis=1)\n",
    "y_pred_stacking_rf = model_rf_meta.predict(X_meta_test)\n",
    "\n",
    "# Evaluate the stacking model with Random Forest as meta-learner\n",
    "mse_stacking_rf = mean_squared_error(y_test, y_pred_stacking_rf)\n",
    "print(f\"Mean Squared Error on Test Data (Stacking with GradientBoostingForest): {mse_stacking_rf:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71c05127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Root Mean Squared Error (RMSE) on unseen data : 0.10\n",
      " Percentage yield for Soyabean : 8.761166418869507\n"
     ]
    }
   ],
   "source": [
    "#Testing the model on unseen data\n",
    "y_pred_nn_base_1 = model_nn_base_1.predict(X_tr)\n",
    "y_pred_nn_base_2 = model_nn_base_2.predict(X_tr)\n",
    "X_meta_test = np.concatenate((model_nn_base_1.predict(X_tr), model_nn_base_2.predict(X_tr)), axis=1)\n",
    "y_pred_stacking_rf = model_rf_meta.predict(X_meta_test)\n",
    "\n",
    "mse = mean_squared_error(y_pred_stacking_rf, y_tr)\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse_stacking_rf)\n",
    "print(f\"Root Mean Squared Error (RMSE) on unseen data : {rmse:.2f}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape \n",
    "df = pd.read_csv('Soyabean.csv')\n",
    "y = df['Yield']\n",
    "rmse = np.sqrt(0.01)\n",
    "p = (rmse / np.mean(y)) * 100 \n",
    "\n",
    "print(f\" Percentage yield for Soyabean : {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "466831a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "crp5=pd.read_csv('Potato.csv')\n",
    "gj1=crp5.drop(['Crop','Year','Season','State','TotalRainfall'],axis=1)\n",
    "import numpy as np\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "x = crp5[features]\n",
    "y = crp5['Yield']\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sx=StandardScaler().fit(x)\n",
    "y=np.array(y)\n",
    "x=sx.transform(x)\n",
    "X_train=x[:330]\n",
    "X_test=x[330:340]\n",
    "y_train=y[:330]\n",
    "y_test=y[330:340]\n",
    "X_tr=x[340:]\n",
    "y_tr=y[340:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e131d482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358, 13)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crp5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed84943b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 1s 15ms/step - loss: 79.9557 - val_loss: 39.8615\n",
      "Epoch 2/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 24.1723 - val_loss: 38.5022\n",
      "Epoch 3/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 23.2679 - val_loss: 32.6227\n",
      "Epoch 4/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 25.1694 - val_loss: 59.6427\n",
      "Epoch 5/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 21.6302 - val_loss: 95.8571\n",
      "Epoch 6/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 23.2512 - val_loss: 65.9843\n",
      "Epoch 7/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 20.2222 - val_loss: 63.1805\n",
      "Epoch 8/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 18.2963 - val_loss: 55.3316\n",
      "Epoch 9/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 17.1068 - val_loss: 85.7901\n",
      "Epoch 10/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 17.8002 - val_loss: 71.6022\n",
      "Epoch 11/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 16.6084 - val_loss: 82.6570\n",
      "Epoch 12/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 17.3225 - val_loss: 40.7093\n",
      "Epoch 13/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 16.3116 - val_loss: 71.9509\n",
      "Epoch 14/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 16.8464 - val_loss: 49.6969\n",
      "Epoch 15/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 16.1378 - val_loss: 64.0584\n",
      "Epoch 16/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 17.1001 - val_loss: 68.1083\n",
      "Epoch 17/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 23.0118 - val_loss: 74.6474\n",
      "Epoch 18/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 21.4208 - val_loss: 59.6120\n",
      "Epoch 19/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 16.4158 - val_loss: 60.1672\n",
      "Epoch 20/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 14.6891 - val_loss: 58.2096\n",
      "Epoch 21/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 14.7517 - val_loss: 78.4579\n",
      "Epoch 22/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 14.8332 - val_loss: 67.4102\n",
      "Epoch 23/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 14.8784 - val_loss: 61.5026\n",
      "Epoch 24/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 14.7766 - val_loss: 59.6782\n",
      "Epoch 25/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 15.1771 - val_loss: 70.3451\n",
      "Epoch 26/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 13.8999 - val_loss: 67.9305\n",
      "Epoch 27/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 15.8896 - val_loss: 75.5932\n",
      "Epoch 28/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 15.0294 - val_loss: 60.7726\n",
      "Epoch 29/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 13.8812 - val_loss: 56.4682\n",
      "Epoch 30/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 15.3167 - val_loss: 75.9131\n",
      "Epoch 31/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 14.3460 - val_loss: 45.8876\n",
      "Epoch 32/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 15.3253 - val_loss: 67.9382\n",
      "Epoch 33/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 21.0624 - val_loss: 79.7648\n",
      "Epoch 34/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 18.5727 - val_loss: 57.9753\n",
      "Epoch 35/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 14.1855 - val_loss: 57.7080\n",
      "Epoch 36/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 12.8717 - val_loss: 74.7386\n",
      "Epoch 37/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 12.7161 - val_loss: 62.7934\n",
      "Epoch 38/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 12.6371 - val_loss: 80.8682\n",
      "Epoch 39/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 13.1333 - val_loss: 80.4349\n",
      "Epoch 40/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 14.6765 - val_loss: 56.4216\n",
      "Epoch 41/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 11.3612 - val_loss: 58.4894\n",
      "Epoch 42/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 13.3915 - val_loss: 68.8378\n",
      "Epoch 43/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 12.9987 - val_loss: 67.3635\n",
      "Epoch 44/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 11.5525 - val_loss: 74.6695\n",
      "Epoch 45/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 12.0069 - val_loss: 47.8264\n",
      "Epoch 46/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 15.5443 - val_loss: 53.8458\n",
      "Epoch 47/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 12.4336 - val_loss: 64.3858\n",
      "Epoch 48/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 12.5234 - val_loss: 67.3323\n",
      "Epoch 49/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 13.9363 - val_loss: 56.6975\n",
      "Epoch 50/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 14.4787 - val_loss: 60.2397\n",
      "Epoch 51/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 11.3855 - val_loss: 57.6720\n",
      "Epoch 52/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 10.8198 - val_loss: 56.5830\n",
      "Epoch 53/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 12.4231 - val_loss: 47.8831\n",
      "Epoch 54/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 11.4690 - val_loss: 62.6222\n",
      "Epoch 55/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 10.6178 - val_loss: 56.3513\n",
      "Epoch 56/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 10.6757 - val_loss: 63.2492\n",
      "Epoch 57/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 10.0961 - val_loss: 55.8984\n",
      "Epoch 58/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 13.4109 - val_loss: 87.3407\n",
      "Epoch 59/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 16.1253 - val_loss: 34.3160\n",
      "Epoch 60/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 12.5700 - val_loss: 59.6751\n",
      "Epoch 61/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 11.3709 - val_loss: 56.2549\n",
      "Epoch 62/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 10.0296 - val_loss: 59.0962\n",
      "Epoch 63/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 10.7661 - val_loss: 50.5790\n",
      "Epoch 64/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 10.5537 - val_loss: 62.1642\n",
      "Epoch 65/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 10.4111 - val_loss: 47.8276\n",
      "Epoch 66/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 10.4552 - val_loss: 63.9811\n",
      "Epoch 67/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 10.7638 - val_loss: 51.1552\n",
      "Epoch 68/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 10.4809 - val_loss: 51.6738\n",
      "Epoch 69/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 10.2367 - val_loss: 61.1938\n",
      "Epoch 70/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 10.7289 - val_loss: 55.6711\n",
      "Epoch 71/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 10.4679 - val_loss: 61.1320\n",
      "Epoch 72/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 10.7574 - val_loss: 45.1725\n",
      "Epoch 73/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 9.7903 - val_loss: 88.1754\n",
      "Epoch 74/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 9.3202 - val_loss: 57.1422\n",
      "Epoch 75/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 8.8972 - val_loss: 67.9667\n",
      "Epoch 76/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 10.2733 - val_loss: 52.0719\n",
      "Epoch 77/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 10.2322 - val_loss: 59.1450\n",
      "Epoch 78/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 8.4138 - val_loss: 59.2386\n",
      "Epoch 79/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 8.4889 - val_loss: 57.6673\n",
      "Epoch 80/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 11.2079 - val_loss: 68.8084\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "Epoch 1/80\n",
      "42/42 [==============================] - 1s 8ms/step - loss: 74.0074 - val_loss: 79.6845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 28.8958 - val_loss: 70.0469\n",
      "Epoch 3/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 22.9822 - val_loss: 65.1408\n",
      "Epoch 4/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 27.5004 - val_loss: 148.2482\n",
      "Epoch 5/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 28.9214 - val_loss: 54.7986\n",
      "Epoch 6/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 23.2459 - val_loss: 38.3294\n",
      "Epoch 7/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 20.9308 - val_loss: 77.7362\n",
      "Epoch 8/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 20.3595 - val_loss: 70.5029\n",
      "Epoch 9/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 18.4462 - val_loss: 68.2813\n",
      "Epoch 10/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 17.4346 - val_loss: 58.4529\n",
      "Epoch 11/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 17.0727 - val_loss: 29.3592\n",
      "Epoch 12/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 19.7326 - val_loss: 55.5445\n",
      "Epoch 13/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 15.5667 - val_loss: 59.3702\n",
      "Epoch 14/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 15.4761 - val_loss: 57.0379\n",
      "Epoch 15/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 16.1186 - val_loss: 63.5369\n",
      "Epoch 16/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 15.4683 - val_loss: 68.4907\n",
      "Epoch 17/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 15.5185 - val_loss: 80.6393\n",
      "Epoch 18/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 17.2777 - val_loss: 67.0893\n",
      "Epoch 19/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 17.2026 - val_loss: 74.9848\n",
      "Epoch 20/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 17.7333 - val_loss: 55.9990\n",
      "Epoch 21/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 14.8385 - val_loss: 67.3646\n",
      "Epoch 22/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 15.4256 - val_loss: 65.5489\n",
      "Epoch 23/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 15.8089 - val_loss: 69.4165\n",
      "Epoch 24/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 15.1415 - val_loss: 64.2304\n",
      "Epoch 25/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 13.7479 - val_loss: 59.1162\n",
      "Epoch 26/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 14.3655 - val_loss: 46.3227\n",
      "Epoch 27/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 13.9063 - val_loss: 52.8423\n",
      "Epoch 28/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 14.3038 - val_loss: 76.2199\n",
      "Epoch 29/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 15.9722 - val_loss: 51.7100\n",
      "Epoch 30/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 14.1031 - val_loss: 56.8250\n",
      "Epoch 31/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 14.5111 - val_loss: 76.6350\n",
      "Epoch 32/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 18.5827 - val_loss: 75.4500\n",
      "Epoch 33/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 14.3012 - val_loss: 74.9545\n",
      "Epoch 34/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 17.8232 - val_loss: 73.4704\n",
      "Epoch 35/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 14.1712 - val_loss: 79.9355\n",
      "Epoch 36/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 13.7749 - val_loss: 75.7364\n",
      "Epoch 37/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 13.8880 - val_loss: 76.6782\n",
      "Epoch 38/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 14.3990 - val_loss: 113.7005\n",
      "Epoch 39/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 17.0470 - val_loss: 93.0375\n",
      "Epoch 40/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 14.0243 - val_loss: 84.0737\n",
      "Epoch 41/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 14.5551 - val_loss: 57.7408\n",
      "Epoch 42/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 13.7769 - val_loss: 72.7231\n",
      "Epoch 43/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 13.8084 - val_loss: 49.9845\n",
      "Epoch 44/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 14.9750 - val_loss: 62.6198\n",
      "Epoch 45/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 12.9304 - val_loss: 70.0838\n",
      "Epoch 46/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 13.0553 - val_loss: 64.0375\n",
      "Epoch 47/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 13.3147 - val_loss: 71.9280\n",
      "Epoch 48/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 13.1683 - val_loss: 73.0357\n",
      "Epoch 49/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 16.2315 - val_loss: 60.5324\n",
      "Epoch 50/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 14.2164 - val_loss: 62.9651\n",
      "Epoch 51/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 11.9874 - val_loss: 76.0141\n",
      "Epoch 52/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 13.1505 - val_loss: 65.1866\n",
      "Epoch 53/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 11.8592 - val_loss: 58.4263\n",
      "Epoch 54/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 10.2634 - val_loss: 66.1200\n",
      "Epoch 55/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 10.8243 - val_loss: 73.7049\n",
      "Epoch 56/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 11.2657 - val_loss: 75.2121\n",
      "Epoch 57/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 14.0831 - val_loss: 63.8625\n",
      "Epoch 58/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 10.5651 - val_loss: 37.9763\n",
      "Epoch 59/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 10.5506 - val_loss: 57.1108\n",
      "Epoch 60/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 9.9580 - val_loss: 51.4122\n",
      "Epoch 61/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 10.0311 - val_loss: 61.6231\n",
      "Epoch 62/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 9.7017 - val_loss: 57.8031\n",
      "Epoch 63/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 11.4385 - val_loss: 47.1857\n",
      "Epoch 64/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 12.4764 - val_loss: 48.0277\n",
      "Epoch 65/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 9.5510 - val_loss: 48.8083\n",
      "Epoch 66/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 11.0761 - val_loss: 84.3231\n",
      "Epoch 67/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 11.6006 - val_loss: 43.9955\n",
      "Epoch 68/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 10.7446 - val_loss: 46.7572\n",
      "Epoch 69/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 10.0641 - val_loss: 52.4417\n",
      "Epoch 70/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 11.5216 - val_loss: 56.3133\n",
      "Epoch 71/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 10.3581 - val_loss: 49.7853\n",
      "Epoch 72/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 9.7707 - val_loss: 50.2516\n",
      "Epoch 73/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 10.0172 - val_loss: 44.6687\n",
      "Epoch 74/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 8.6329 - val_loss: 48.2052\n",
      "Epoch 75/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 10.7479 - val_loss: 40.3513\n",
      "Epoch 76/80\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 8.5641 - val_loss: 56.1813\n",
      "Epoch 77/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 8.2472 - val_loss: 61.5344\n",
      "Epoch 78/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 8.1408 - val_loss: 61.6741\n",
      "Epoch 79/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 8.1169 - val_loss: 57.7141\n",
      "Epoch 80/80\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 9.5530 - val_loss: 38.7820\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Mean Squared Error on Test Data (Stacking with GradientBoostingForest): 0.01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Build and train the first neural network (base model 1)\n",
    "model_nn_base_1 = Sequential()\n",
    "model_nn_base_1.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(10, activation='relu'))\n",
    "model_nn_base_1.add(Dense(1, activation='linear'))\n",
    "model_nn_base_1.compile(optimizer=Adam(lr=0.01), loss='mean_squared_error')\n",
    "model_nn_base_1.fit(X_train, y_train, epochs=80, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the first base neural network model\n",
    "y_pred_nn_base_1 = model_nn_base_1.predict(X_test)\n",
    "\n",
    "# Build and train the second neural network (base model 2)\n",
    "model_nn_base_2 = Sequential()\n",
    "model_nn_base_2.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(10, activation='relu'))\n",
    "model_nn_base_2.add(Dense(5, activation='relu'))\n",
    "model_nn_base_2.add(Dense(1, activation='linear'))\n",
    "model_nn_base_2.compile(optimizer=Adam(lr=0.01), loss='mean_squared_error')\n",
    "model_nn_base_2.fit(X_train, y_train, epochs=80, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the second base neural network model\n",
    "y_pred_nn_base_2 = model_nn_base_2.predict(X_test)\n",
    "\n",
    "# Combine predictions from both base models\n",
    "X_meta_train = np.concatenate((y_pred_nn_base_1, y_pred_nn_base_2), axis=1)\n",
    "\n",
    "# Build and train a Random Forest Regressor as a meta-learner\n",
    "model_rf_meta = GradientBoostingRegressor(n_estimators=50,learning_rate=0.09 ,random_state=42)\n",
    "model_rf_meta.fit(X_meta_train, y_test)\n",
    "\n",
    "# Make predictions using the base models and meta-learner\n",
    "X_meta_test = np.concatenate((model_nn_base_1.predict(X_test), model_nn_base_2.predict(X_test)), axis=1)\n",
    "y_pred_stacking_rf = model_rf_meta.predict(X_meta_test)\n",
    "\n",
    "# Evaluate the stacking model with Random Forest as meta-learner\n",
    "mse_stacking_rf = mean_squared_error(y_test, y_pred_stacking_rf)\n",
    "print(f\"Mean Squared Error on Test Data (Stacking with GradientBoostingForest): {mse_stacking_rf:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0e90b020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Root Mean Squared Error (RMSE) on unseen data : 0.08\n",
      " Percentage yield for Potato : 0.8155846509808989\n"
     ]
    }
   ],
   "source": [
    "#Testing the model on unseen data\n",
    "y_pred_nn_base_1 = model_nn_base_1.predict(X_tr)\n",
    "y_pred_nn_base_2 = model_nn_base_2.predict(X_tr)\n",
    "X_meta_test = np.concatenate((model_nn_base_1.predict(X_tr), model_nn_base_2.predict(X_tr)), axis=1)\n",
    "y_pred_stacking_rf = model_rf_meta.predict(X_meta_test)\n",
    "\n",
    "mse = mean_squared_error(y_pred_stacking_rf, y_tr)\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse_stacking_rf)\n",
    "print(f\"Root Mean Squared Error (RMSE) on unseen data : {rmse:.2f}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape \n",
    "df = pd.read_csv('Potato.csv')\n",
    "y = df['Yield']\n",
    "rmse = np.sqrt(0.01)\n",
    "p = (rmse / np.mean(y)) * 100 \n",
    "\n",
    "print(f\" Percentage yield for Potato : {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f1a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "crp5=pd.read_csv('Rice.csv')\n",
    "gj1=crp5.drop(['Crop','Year','Season','State','TotalRainfall'],axis=1)\n",
    "import numpy as np\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "x = crp5[features]\n",
    "y = crp5['Yield']\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sx=StandardScaler().fit(x)\n",
    "y=np.array(y)\n",
    "x=sx.transform(x)\n",
    "X_train=x[:550]\n",
    "X_test=x[550:560]\n",
    "y_train=y[:550]\n",
    "y_test=y[550:560]\n",
    "X_tr=x[560:]\n",
    "y_tr=y[560:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4419a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Build and train the first neural network (base model 1)\n",
    "model_nn_base_1 = Sequential()\n",
    "model_nn_base_1.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(10, activation='relu'))\n",
    "model_nn_base_1.add(Dense(1, activation='linear'))\n",
    "model_nn_base_1.compile(optimizer=Adam(lr=0.01), loss='mean_squared_error')\n",
    "model_nn_base_1.fit(X_train, y_train, epochs=200, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the first base neural network model\n",
    "y_pred_nn_base_1 = model_nn_base_1.predict(X_test)\n",
    "\n",
    "# Build and train the second neural network (base model 2)\n",
    "model_nn_base_2 = Sequential()\n",
    "model_nn_base_2.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(10, activation='relu'))\n",
    "model_nn_base_2.add(Dense(5, activation='relu'))\n",
    "model_nn_base_2.add(Dense(1, activation='linear'))\n",
    "model_nn_base_2.compile(optimizer=Adam(lr=0.01), loss='mean_squared_error')\n",
    "model_nn_base_2.fit(X_train, y_train, epochs=200, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the second base neural network model\n",
    "y_pred_nn_base_2 = model_nn_base_2.predict(X_test)\n",
    "\n",
    "# Combine predictions from both base models\n",
    "X_meta_train = np.concatenate((y_pred_nn_base_1, y_pred_nn_base_2), axis=1)\n",
    "\n",
    "# Build and train a Random Forest Regressor as a meta-learner\n",
    "model_rf_meta = GradientBoostingRegressor(n_estimators=100,learning_rate=0.01, random_state=42)\n",
    "model_rf_meta.fit(X_meta_train, y_test)\n",
    "\n",
    "# Make predictions using the base models and meta-learner\n",
    "X_meta_test = np.concatenate((model_nn_base_1.predict(X_test), model_nn_base_2.predict(X_test)), axis=1)\n",
    "y_pred_stacking_rf = model_rf_meta.predict(X_meta_test)\n",
    "\n",
    "# Evaluate the stacking model with Random Forest as meta-learner\n",
    "mse_stacking_rf = mean_squared_error(y_test, y_pred_stacking_rf)\n",
    "print(f\"Mean Squared Error on Test Data (Stacking with GradientBoostingForest): {mse_stacking_rf:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a41e93dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "crp5=pd.read_csv('Onion.csv')\n",
    "gj1=crp5.drop(['Crop','Year','Season','State','TotalRainfall'],axis=1)\n",
    "import numpy as np\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "x = crp5[features]\n",
    "y = crp5['Yield']\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sx=StandardScaler().fit(x)\n",
    "y=np.array(y)\n",
    "x=sx.transform(x)\n",
    "X_train=x[:250]\n",
    "X_test=x[250:260]\n",
    "y_train=y[:250]\n",
    "y_test=y[250:260]\n",
    "X_tr=x[260:]\n",
    "y_tr=y[260:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7cd89e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 9ms/step - loss: 1032.9072 - val_loss: 11.1393\n",
      "Epoch 2/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 929.5829 - val_loss: 5.3019\n",
      "Epoch 3/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 947.2470 - val_loss: 41.9962\n",
      "Epoch 4/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 912.6780 - val_loss: 54.3751\n",
      "Epoch 5/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 910.5188 - val_loss: 27.1914\n",
      "Epoch 6/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 907.9985 - val_loss: 78.3333\n",
      "Epoch 7/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 894.8038 - val_loss: 13.5814\n",
      "Epoch 8/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 910.1392 - val_loss: 27.3836\n",
      "Epoch 9/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 895.6884 - val_loss: 21.0541\n",
      "Epoch 10/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 881.9788 - val_loss: 66.5852\n",
      "Epoch 11/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 903.7654 - val_loss: 64.5002\n",
      "Epoch 12/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 875.5477 - val_loss: 22.1709\n",
      "Epoch 13/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 849.6394 - val_loss: 20.4884\n",
      "Epoch 14/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 879.4064 - val_loss: 4.4861\n",
      "Epoch 15/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 922.2957 - val_loss: 44.9856\n",
      "Epoch 16/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 867.4399 - val_loss: 145.0465\n",
      "Epoch 17/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 760.9940 - val_loss: 68.2613\n",
      "Epoch 18/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 897.9924 - val_loss: 7.9297\n",
      "Epoch 19/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 900.4291 - val_loss: 32.8310\n",
      "Epoch 20/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 863.4974 - val_loss: 35.5591\n",
      "Epoch 21/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 896.2708 - val_loss: 8.7566\n",
      "Epoch 22/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 945.9683 - val_loss: 34.5292\n",
      "Epoch 23/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 912.6939 - val_loss: 51.5778\n",
      "Epoch 24/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 909.8753 - val_loss: 45.3662\n",
      "Epoch 25/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 898.0226 - val_loss: 49.4453\n",
      "Epoch 26/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 897.1488 - val_loss: 26.6413\n",
      "Epoch 27/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 895.5635 - val_loss: 39.1390\n",
      "Epoch 28/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 882.3179 - val_loss: 62.3228\n",
      "Epoch 29/200\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 871.7101 - val_loss: 61.2181\n",
      "Epoch 30/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 876.5158 - val_loss: 80.2974\n",
      "Epoch 31/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 889.2862 - val_loss: 62.3008\n",
      "Epoch 32/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 857.3199 - val_loss: 93.1045\n",
      "Epoch 33/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 858.7521 - val_loss: 72.4359\n",
      "Epoch 34/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 918.0677 - val_loss: 87.4714\n",
      "Epoch 35/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 863.3320 - val_loss: 112.2915\n",
      "Epoch 36/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 828.1310 - val_loss: 114.9801\n",
      "Epoch 37/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 820.6816 - val_loss: 55.3849\n",
      "Epoch 38/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 983.3152 - val_loss: 104.2999\n",
      "Epoch 39/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 904.7359 - val_loss: 33.7550\n",
      "Epoch 40/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 882.5201 - val_loss: 31.5746\n",
      "Epoch 41/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 890.3927 - val_loss: 34.1180\n",
      "Epoch 42/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 857.2238 - val_loss: 21.7516\n",
      "Epoch 43/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 887.0090 - val_loss: 22.7833\n",
      "Epoch 44/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 911.9833 - val_loss: 105.9634\n",
      "Epoch 45/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 867.1594 - val_loss: 36.1061\n",
      "Epoch 46/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 797.8502 - val_loss: 25.7137\n",
      "Epoch 47/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 886.4041 - val_loss: 70.1120\n",
      "Epoch 48/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 868.6373 - val_loss: 53.3271\n",
      "Epoch 49/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 882.8510 - val_loss: 70.1853\n",
      "Epoch 50/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 741.4982 - val_loss: 17.5013\n",
      "Epoch 51/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 795.8361 - val_loss: 28.8755\n",
      "Epoch 52/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 710.7229 - val_loss: 43.0452\n",
      "Epoch 53/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 840.5680 - val_loss: 58.3121\n",
      "Epoch 54/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 887.9668 - val_loss: 24.3073\n",
      "Epoch 55/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 831.8082 - val_loss: 53.5160\n",
      "Epoch 56/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 826.6742 - val_loss: 45.3770\n",
      "Epoch 57/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 816.2524 - val_loss: 46.6272\n",
      "Epoch 58/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 810.6614 - val_loss: 36.6244\n",
      "Epoch 59/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 815.4308 - val_loss: 93.8981\n",
      "Epoch 60/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 777.2739 - val_loss: 16.1980\n",
      "Epoch 61/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 849.6803 - val_loss: 101.8900\n",
      "Epoch 62/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 588.8423 - val_loss: 29.8637\n",
      "Epoch 63/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 485.3585 - val_loss: 40.9424\n",
      "Epoch 64/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 907.2878 - val_loss: 22.9457\n",
      "Epoch 65/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 482.5309 - val_loss: 34.2666\n",
      "Epoch 66/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 816.3484 - val_loss: 20.3277\n",
      "Epoch 67/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 696.6053 - val_loss: 53.9222\n",
      "Epoch 68/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 378.8387 - val_loss: 54.5835\n",
      "Epoch 69/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 433.0030 - val_loss: 52.0872\n",
      "Epoch 70/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 429.1319 - val_loss: 18.7562\n",
      "Epoch 71/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 901.3116 - val_loss: 51.1224\n",
      "Epoch 72/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 718.5826 - val_loss: 37.6911\n",
      "Epoch 73/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 603.5625 - val_loss: 34.0792\n",
      "Epoch 74/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 1028.6471 - val_loss: 59.4192\n",
      "Epoch 75/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 893.9932 - val_loss: 37.4670\n",
      "Epoch 76/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 880.4489 - val_loss: 5.9214\n",
      "Epoch 77/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 898.9286 - val_loss: 32.5204\n",
      "Epoch 78/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 889.4762 - val_loss: 24.2965\n",
      "Epoch 79/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 886.2534 - val_loss: 37.2120\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 4ms/step - loss: 882.7973 - val_loss: 13.1871\n",
      "Epoch 81/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 886.5778 - val_loss: 31.4479\n",
      "Epoch 82/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 887.2095 - val_loss: 15.3116\n",
      "Epoch 83/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 882.0403 - val_loss: 25.6620\n",
      "Epoch 84/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 883.1530 - val_loss: 41.4373\n",
      "Epoch 85/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 878.2267 - val_loss: 25.6499\n",
      "Epoch 86/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 881.7567 - val_loss: 6.0391\n",
      "Epoch 87/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 885.5162 - val_loss: 39.3021\n",
      "Epoch 88/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 888.8102 - val_loss: 34.1785\n",
      "Epoch 89/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 882.0421 - val_loss: 13.8142\n",
      "Epoch 90/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 879.9265 - val_loss: 33.2268\n",
      "Epoch 91/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 877.7781 - val_loss: 17.0934\n",
      "Epoch 92/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 878.3695 - val_loss: 27.1369\n",
      "Epoch 93/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 875.4824 - val_loss: 22.9425\n",
      "Epoch 94/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 879.4888 - val_loss: 28.9174\n",
      "Epoch 95/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 875.2445 - val_loss: 24.4513\n",
      "Epoch 96/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 874.0627 - val_loss: 29.2340\n",
      "Epoch 97/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 881.9933 - val_loss: 34.3835\n",
      "Epoch 98/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 874.9322 - val_loss: 26.3679\n",
      "Epoch 99/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 875.0930 - val_loss: 38.7189\n",
      "Epoch 100/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 873.1131 - val_loss: 38.2307\n",
      "Epoch 101/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 873.3831 - val_loss: 9.2505\n",
      "Epoch 102/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 876.0717 - val_loss: 21.5858\n",
      "Epoch 103/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 951.9068 - val_loss: 16.8626\n",
      "Epoch 104/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 879.5812 - val_loss: 64.1126\n",
      "Epoch 105/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 863.8395 - val_loss: 21.8098\n",
      "Epoch 106/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 874.7072 - val_loss: 33.8913\n",
      "Epoch 107/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 900.3888 - val_loss: 9.2212\n",
      "Epoch 108/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 912.6227 - val_loss: 56.0389\n",
      "Epoch 109/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 874.4965 - val_loss: 37.0835\n",
      "Epoch 110/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 820.2861 - val_loss: 11.8928\n",
      "Epoch 111/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 756.9973 - val_loss: 63.4203\n",
      "Epoch 112/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 562.8976 - val_loss: 6.2504\n",
      "Epoch 113/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 1004.6421 - val_loss: 5.1141\n",
      "Epoch 114/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 943.2075 - val_loss: 79.2953\n",
      "Epoch 115/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 975.8399 - val_loss: 67.9951\n",
      "Epoch 116/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 878.5531 - val_loss: 45.0817\n",
      "Epoch 117/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 863.4856 - val_loss: 40.7366\n",
      "Epoch 118/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 886.7020 - val_loss: 15.8674\n",
      "Epoch 119/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 924.3436 - val_loss: 110.3927\n",
      "Epoch 120/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 910.3409 - val_loss: 146.5639\n",
      "Epoch 121/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 890.2984 - val_loss: 29.4787\n",
      "Epoch 122/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 880.0041 - val_loss: 17.1286\n",
      "Epoch 123/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 878.1127 - val_loss: 105.9078\n",
      "Epoch 124/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 855.3792 - val_loss: 74.5660\n",
      "Epoch 125/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 853.1931 - val_loss: 10.4849\n",
      "Epoch 126/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 872.6777 - val_loss: 25.8916\n",
      "Epoch 127/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 862.3085 - val_loss: 16.7169\n",
      "Epoch 128/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 849.0268 - val_loss: 47.7316\n",
      "Epoch 129/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 826.0853 - val_loss: 94.6684\n",
      "Epoch 130/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 831.9311 - val_loss: 57.7925\n",
      "Epoch 131/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 800.3463 - val_loss: 51.5931\n",
      "Epoch 132/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 771.7317 - val_loss: 26.9120\n",
      "Epoch 133/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 754.3889 - val_loss: 25.4492\n",
      "Epoch 134/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 725.1552 - val_loss: 59.0481\n",
      "Epoch 135/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 687.8945 - val_loss: 41.7249\n",
      "Epoch 136/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 727.2866 - val_loss: 34.6111\n",
      "Epoch 137/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 683.7609 - val_loss: 24.6232\n",
      "Epoch 138/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 544.3703 - val_loss: 28.6383\n",
      "Epoch 139/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 487.9986 - val_loss: 24.6616\n",
      "Epoch 140/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 356.7593 - val_loss: 42.9381\n",
      "Epoch 141/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 259.3372 - val_loss: 108.0798\n",
      "Epoch 142/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 404.3057 - val_loss: 72.2123\n",
      "Epoch 143/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 281.6767 - val_loss: 33.4012\n",
      "Epoch 144/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 946.0543 - val_loss: 54.0281\n",
      "Epoch 145/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 202.5166 - val_loss: 62.7505\n",
      "Epoch 146/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 383.0773 - val_loss: 66.6611\n",
      "Epoch 147/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 379.2249 - val_loss: 51.4349\n",
      "Epoch 148/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 375.9452 - val_loss: 84.5440\n",
      "Epoch 149/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 373.2159 - val_loss: 49.9479\n",
      "Epoch 150/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 376.7067 - val_loss: 49.0061\n",
      "Epoch 151/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 374.0518 - val_loss: 56.8074\n",
      "Epoch 152/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 369.4481 - val_loss: 91.4588\n",
      "Epoch 153/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 366.0160 - val_loss: 72.7432\n",
      "Epoch 154/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 365.6007 - val_loss: 108.9124\n",
      "Epoch 155/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 368.3247 - val_loss: 65.9089\n",
      "Epoch 156/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 167.5678 - val_loss: 66.9756\n",
      "Epoch 157/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 168.1089 - val_loss: 63.1869\n",
      "Epoch 158/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 165.7607 - val_loss: 50.0037\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 4ms/step - loss: 163.1355 - val_loss: 61.7428\n",
      "Epoch 160/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 206.0267 - val_loss: 57.4344\n",
      "Epoch 161/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 583.9976 - val_loss: 59.5379\n",
      "Epoch 162/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 251.3100 - val_loss: 39.1850\n",
      "Epoch 163/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 362.5802 - val_loss: 34.0664\n",
      "Epoch 164/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 873.6680 - val_loss: 33.3106\n",
      "Epoch 165/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 947.8705 - val_loss: 36.2950\n",
      "Epoch 166/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 933.0634 - val_loss: 77.9658\n",
      "Epoch 167/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 907.5458 - val_loss: 55.5360\n",
      "Epoch 168/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 887.8979 - val_loss: 70.3418\n",
      "Epoch 169/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 883.4802 - val_loss: 42.9464\n",
      "Epoch 170/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 879.6882 - val_loss: 47.4525\n",
      "Epoch 171/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 873.6033 - val_loss: 70.3098\n",
      "Epoch 172/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 869.6232 - val_loss: 51.9327\n",
      "Epoch 173/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 866.3421 - val_loss: 60.0841\n",
      "Epoch 174/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 864.3370 - val_loss: 38.8577\n",
      "Epoch 175/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 863.1603 - val_loss: 54.3665\n",
      "Epoch 176/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 859.5668 - val_loss: 58.5117\n",
      "Epoch 177/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 850.8293 - val_loss: 84.8046\n",
      "Epoch 178/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 849.2537 - val_loss: 59.3994\n",
      "Epoch 179/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 845.9611 - val_loss: 56.3180\n",
      "Epoch 180/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 838.9760 - val_loss: 57.9444\n",
      "Epoch 181/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 836.1909 - val_loss: 64.3663\n",
      "Epoch 182/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 832.7397 - val_loss: 68.1663\n",
      "Epoch 183/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 828.6072 - val_loss: 58.6844\n",
      "Epoch 184/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 822.3483 - val_loss: 64.1678\n",
      "Epoch 185/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 811.9124 - val_loss: 66.6808\n",
      "Epoch 186/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 745.4186 - val_loss: 72.2703\n",
      "Epoch 187/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 332.9396 - val_loss: 48.2395\n",
      "Epoch 188/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 314.4160 - val_loss: 66.6324\n",
      "Epoch 189/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 211.1627 - val_loss: 45.8021\n",
      "Epoch 190/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 188.6210 - val_loss: 71.3012\n",
      "Epoch 191/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 184.2692 - val_loss: 49.3429\n",
      "Epoch 192/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 182.6820 - val_loss: 45.1300\n",
      "Epoch 193/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 182.7802 - val_loss: 60.3954\n",
      "Epoch 194/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 178.6258 - val_loss: 36.3653\n",
      "Epoch 195/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 180.4512 - val_loss: 51.5039\n",
      "Epoch 196/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 175.3905 - val_loss: 33.3300\n",
      "Epoch 197/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 176.7132 - val_loss: 52.5704\n",
      "Epoch 198/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 175.1557 - val_loss: 62.0093\n",
      "Epoch 199/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 177.3604 - val_loss: 43.4698\n",
      "Epoch 200/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 176.8639 - val_loss: 70.7426\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Epoch 1/200\n",
      "32/32 [==============================] - 1s 9ms/step - loss: 1044.3832 - val_loss: 8.7818\n",
      "Epoch 2/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 927.0022 - val_loss: 53.9456\n",
      "Epoch 3/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 931.0347 - val_loss: 55.0243\n",
      "Epoch 4/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 921.1724 - val_loss: 10.0457\n",
      "Epoch 5/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 942.7387 - val_loss: 12.9988\n",
      "Epoch 6/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 932.6828 - val_loss: 10.1188\n",
      "Epoch 7/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 943.4897 - val_loss: 77.2600\n",
      "Epoch 8/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 918.6874 - val_loss: 31.6819\n",
      "Epoch 9/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 900.9495 - val_loss: 15.7221\n",
      "Epoch 10/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 907.3510 - val_loss: 88.9531\n",
      "Epoch 11/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 908.0911 - val_loss: 33.0522\n",
      "Epoch 12/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 891.4891 - val_loss: 42.9336\n",
      "Epoch 13/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 882.6498 - val_loss: 26.5308\n",
      "Epoch 14/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 859.8990 - val_loss: 48.9671\n",
      "Epoch 15/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 910.3601 - val_loss: 115.4750\n",
      "Epoch 16/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 918.8734 - val_loss: 45.2243\n",
      "Epoch 17/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 837.8663 - val_loss: 58.7659\n",
      "Epoch 18/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 873.8395 - val_loss: 43.7078\n",
      "Epoch 19/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 847.2155 - val_loss: 6.8610\n",
      "Epoch 20/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 889.3530 - val_loss: 32.5935\n",
      "Epoch 21/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 933.9047 - val_loss: 87.2439\n",
      "Epoch 22/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 883.2023 - val_loss: 57.9558\n",
      "Epoch 23/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 882.1403 - val_loss: 37.5140\n",
      "Epoch 24/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 820.5120 - val_loss: 35.8102\n",
      "Epoch 25/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 821.5528 - val_loss: 9.6553\n",
      "Epoch 26/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 965.9547 - val_loss: 33.9301\n",
      "Epoch 27/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 834.2549 - val_loss: 27.4360\n",
      "Epoch 28/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 845.3489 - val_loss: 38.7579\n",
      "Epoch 29/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 711.1882 - val_loss: 27.0050\n",
      "Epoch 30/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 786.2228 - val_loss: 46.1531\n",
      "Epoch 31/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 670.7909 - val_loss: 18.7704\n",
      "Epoch 32/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 653.8109 - val_loss: 23.9189\n",
      "Epoch 33/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 704.3924 - val_loss: 8.5228\n",
      "Epoch 34/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 891.9749 - val_loss: 34.3612\n",
      "Epoch 35/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 693.5739 - val_loss: 70.4724\n",
      "Epoch 36/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 729.2872 - val_loss: 6.6177\n",
      "Epoch 37/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 954.1901 - val_loss: 27.4040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 842.0006 - val_loss: 58.4910\n",
      "Epoch 39/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 890.5250 - val_loss: 49.4067\n",
      "Epoch 40/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 774.1297 - val_loss: 22.4017\n",
      "Epoch 41/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 740.4539 - val_loss: 8.7504\n",
      "Epoch 42/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 797.8666 - val_loss: 39.7966\n",
      "Epoch 43/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 677.8231 - val_loss: 6.5467\n",
      "Epoch 44/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 682.5132 - val_loss: 43.0413\n",
      "Epoch 45/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 634.3812 - val_loss: 13.2884\n",
      "Epoch 46/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 649.1137 - val_loss: 28.8493\n",
      "Epoch 47/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 703.3166 - val_loss: 52.9858\n",
      "Epoch 48/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 662.4282 - val_loss: 20.5118\n",
      "Epoch 49/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 629.5414 - val_loss: 14.9341\n",
      "Epoch 50/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 630.8329 - val_loss: 19.9357\n",
      "Epoch 51/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 698.9547 - val_loss: 57.5991\n",
      "Epoch 52/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 730.6426 - val_loss: 8.4060\n",
      "Epoch 53/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 892.0038 - val_loss: 38.0912\n",
      "Epoch 54/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 791.8159 - val_loss: 34.0810\n",
      "Epoch 55/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 640.5293 - val_loss: 72.1599\n",
      "Epoch 56/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 769.7883 - val_loss: 29.8386\n",
      "Epoch 57/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 845.7488 - val_loss: 52.4550\n",
      "Epoch 58/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 756.8723 - val_loss: 42.3801\n",
      "Epoch 59/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 666.1485 - val_loss: 57.2271\n",
      "Epoch 60/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 657.0429 - val_loss: 16.5675\n",
      "Epoch 61/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 817.9617 - val_loss: 63.1048\n",
      "Epoch 62/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 749.2428 - val_loss: 15.7332\n",
      "Epoch 63/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 715.6667 - val_loss: 21.5620\n",
      "Epoch 64/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 753.0867 - val_loss: 67.2180\n",
      "Epoch 65/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 563.5477 - val_loss: 30.2268\n",
      "Epoch 66/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 528.9474 - val_loss: 47.8679\n",
      "Epoch 67/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 613.3452 - val_loss: 16.9346\n",
      "Epoch 68/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 705.3246 - val_loss: 30.9916\n",
      "Epoch 69/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 615.8961 - val_loss: 89.1928\n",
      "Epoch 70/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 590.0086 - val_loss: 26.3401\n",
      "Epoch 71/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 812.5945 - val_loss: 38.4841\n",
      "Epoch 72/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 640.7743 - val_loss: 19.0048\n",
      "Epoch 73/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 506.4621 - val_loss: 50.3746\n",
      "Epoch 74/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 745.3677 - val_loss: 27.4464\n",
      "Epoch 75/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 565.4647 - val_loss: 44.7458\n",
      "Epoch 76/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 580.9473 - val_loss: 72.2174\n",
      "Epoch 77/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 495.2274 - val_loss: 15.7692\n",
      "Epoch 78/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 806.8513 - val_loss: 43.2586\n",
      "Epoch 79/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 683.9340 - val_loss: 31.8713\n",
      "Epoch 80/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 562.8653 - val_loss: 51.6836\n",
      "Epoch 81/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 435.1147 - val_loss: 38.7562\n",
      "Epoch 82/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 420.4991 - val_loss: 37.3997\n",
      "Epoch 83/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 886.7260 - val_loss: 56.4822\n",
      "Epoch 84/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 879.3156 - val_loss: 64.2351\n",
      "Epoch 85/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 877.3364 - val_loss: 82.3605\n",
      "Epoch 86/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 882.6086 - val_loss: 70.6938\n",
      "Epoch 87/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 875.8276 - val_loss: 53.2762\n",
      "Epoch 88/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 875.7816 - val_loss: 32.0065\n",
      "Epoch 89/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 896.4103 - val_loss: 74.9043\n",
      "Epoch 90/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 882.1047 - val_loss: 26.6933\n",
      "Epoch 91/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 874.0769 - val_loss: 27.4116\n",
      "Epoch 92/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 875.7683 - val_loss: 47.6744\n",
      "Epoch 93/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 866.6949 - val_loss: 25.8528\n",
      "Epoch 94/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 865.5298 - val_loss: 29.6362\n",
      "Epoch 95/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 863.5465 - val_loss: 30.4073\n",
      "Epoch 96/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 864.7474 - val_loss: 23.8616\n",
      "Epoch 97/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 876.9326 - val_loss: 643.1997\n",
      "Epoch 98/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 878.6459 - val_loss: 28.7684\n",
      "Epoch 99/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 873.9703 - val_loss: 110.0215\n",
      "Epoch 100/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 869.2999 - val_loss: 150.3664\n",
      "Epoch 101/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 875.3823 - val_loss: 424.7790\n",
      "Epoch 102/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 865.7488 - val_loss: 214.2237\n",
      "Epoch 103/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 862.4724 - val_loss: 71.8363\n",
      "Epoch 104/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 862.5358 - val_loss: 51.7543\n",
      "Epoch 105/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 864.4814 - val_loss: 52.5671\n",
      "Epoch 106/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 870.4716 - val_loss: 26.7647\n",
      "Epoch 107/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 890.8743 - val_loss: 33.8636\n",
      "Epoch 108/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 857.9771 - val_loss: 43.3157\n",
      "Epoch 109/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 861.9201 - val_loss: 35.1183\n",
      "Epoch 110/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 857.0398 - val_loss: 32.6501\n",
      "Epoch 111/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 863.3616 - val_loss: 30.8354\n",
      "Epoch 112/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 852.0512 - val_loss: 34.0491\n",
      "Epoch 113/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 854.5662 - val_loss: 54.6889\n",
      "Epoch 114/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 881.4127 - val_loss: 30.6739\n",
      "Epoch 115/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 864.6918 - val_loss: 32.4454\n",
      "Epoch 116/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 849.4372 - val_loss: 31.7297\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 5ms/step - loss: 845.2250 - val_loss: 26.5219\n",
      "Epoch 118/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 841.6539 - val_loss: 113.4084\n",
      "Epoch 119/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 852.0646 - val_loss: 40.0858\n",
      "Epoch 120/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 883.5391 - val_loss: 97.3764\n",
      "Epoch 121/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 881.5455 - val_loss: 30.7805\n",
      "Epoch 122/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 853.1121 - val_loss: 35.2993\n",
      "Epoch 123/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 856.0878 - val_loss: 73.6548\n",
      "Epoch 124/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 856.5193 - val_loss: 23.3427\n",
      "Epoch 125/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 853.9149 - val_loss: 19.4468\n",
      "Epoch 126/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 861.9226 - val_loss: 33.1137\n",
      "Epoch 127/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 878.1012 - val_loss: 27.8609\n",
      "Epoch 128/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 810.9926 - val_loss: 51.0707\n",
      "Epoch 129/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 870.0595 - val_loss: 27.6438\n",
      "Epoch 130/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 864.9088 - val_loss: 34.7599\n",
      "Epoch 131/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 900.9167 - val_loss: 38.2789\n",
      "Epoch 132/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 896.9927 - val_loss: 44.2391\n",
      "Epoch 133/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 886.2563 - val_loss: 61.0130\n",
      "Epoch 134/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 875.4655 - val_loss: 82.4950\n",
      "Epoch 135/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 864.9072 - val_loss: 27.0416\n",
      "Epoch 136/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 869.3611 - val_loss: 27.7002\n",
      "Epoch 137/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 837.5297 - val_loss: 82.2209\n",
      "Epoch 138/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 762.8526 - val_loss: 57.1719\n",
      "Epoch 139/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 911.3195 - val_loss: 9.8879\n",
      "Epoch 140/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 697.5878 - val_loss: 11.3658\n",
      "Epoch 141/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 722.8038 - val_loss: 35.2708\n",
      "Epoch 142/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 914.3685 - val_loss: 51.6179\n",
      "Epoch 143/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 902.4811 - val_loss: 66.2896\n",
      "Epoch 144/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 892.5952 - val_loss: 12.3582\n",
      "Epoch 145/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 898.6633 - val_loss: 39.2071\n",
      "Epoch 146/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 880.4943 - val_loss: 21.4201\n",
      "Epoch 147/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 880.3829 - val_loss: 21.5491\n",
      "Epoch 148/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 881.2614 - val_loss: 35.6323\n",
      "Epoch 149/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 870.9002 - val_loss: 58.8830\n",
      "Epoch 150/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 868.0943 - val_loss: 40.7693\n",
      "Epoch 151/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 877.6188 - val_loss: 47.5398\n",
      "Epoch 152/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 872.1853 - val_loss: 65.9222\n",
      "Epoch 153/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 836.4812 - val_loss: 90.1220\n",
      "Epoch 154/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 822.3741 - val_loss: 89.2356\n",
      "Epoch 155/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 799.6837 - val_loss: 93.0530\n",
      "Epoch 156/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 794.7637 - val_loss: 121.7579\n",
      "Epoch 157/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 729.6655 - val_loss: 85.7713\n",
      "Epoch 158/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 669.3831 - val_loss: 77.2278\n",
      "Epoch 159/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 902.3345 - val_loss: 160.3685\n",
      "Epoch 160/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 611.7691 - val_loss: 62.5043\n",
      "Epoch 161/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 759.6500 - val_loss: 32.3043\n",
      "Epoch 162/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 706.9001 - val_loss: 42.3035\n",
      "Epoch 163/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 694.3578 - val_loss: 45.7774\n",
      "Epoch 164/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 711.8090 - val_loss: 53.1895\n",
      "Epoch 165/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 689.1729 - val_loss: 82.7404\n",
      "Epoch 166/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 678.7191 - val_loss: 40.2117\n",
      "Epoch 167/200\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 684.7030 - val_loss: 43.3479\n",
      "Epoch 168/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 674.6526 - val_loss: 38.7796\n",
      "Epoch 169/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 689.2328 - val_loss: 46.9921\n",
      "Epoch 170/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 671.2975 - val_loss: 84.2633\n",
      "Epoch 171/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 694.1054 - val_loss: 36.8164\n",
      "Epoch 172/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 691.8438 - val_loss: 49.3391\n",
      "Epoch 173/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 683.6734 - val_loss: 35.9859\n",
      "Epoch 174/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 685.8133 - val_loss: 47.8451\n",
      "Epoch 175/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 685.5645 - val_loss: 48.9157\n",
      "Epoch 176/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 697.7576 - val_loss: 41.7093\n",
      "Epoch 177/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 716.2792 - val_loss: 56.9273\n",
      "Epoch 178/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 681.0449 - val_loss: 32.5353\n",
      "Epoch 179/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 684.7598 - val_loss: 52.4574\n",
      "Epoch 180/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 681.4978 - val_loss: 32.2452\n",
      "Epoch 181/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 682.9513 - val_loss: 63.4059\n",
      "Epoch 182/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 682.2121 - val_loss: 38.0110\n",
      "Epoch 183/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 681.3480 - val_loss: 74.4314\n",
      "Epoch 184/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 680.2883 - val_loss: 35.2086\n",
      "Epoch 185/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 679.3000 - val_loss: 40.1531\n",
      "Epoch 186/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 677.6791 - val_loss: 33.0029\n",
      "Epoch 187/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 681.9719 - val_loss: 64.0032\n",
      "Epoch 188/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 664.1375 - val_loss: 40.3212\n",
      "Epoch 189/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 671.8610 - val_loss: 59.8064\n",
      "Epoch 190/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 683.5102 - val_loss: 29.6933\n",
      "Epoch 191/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 682.1260 - val_loss: 38.1060\n",
      "Epoch 192/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 662.6231 - val_loss: 48.0477\n",
      "Epoch 193/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 666.2323 - val_loss: 33.5644\n",
      "Epoch 194/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 658.0837 - val_loss: 49.7038\n",
      "Epoch 195/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 685.1309 - val_loss: 43.0993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 679.9327 - val_loss: 54.4916\n",
      "Epoch 197/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 677.4350 - val_loss: 42.6558\n",
      "Epoch 198/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 678.6061 - val_loss: 42.8348\n",
      "Epoch 199/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 676.3378 - val_loss: 40.5768\n",
      "Epoch 200/200\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 677.6658 - val_loss: 44.2981\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Mean Squared Error on Test Data (Stacking with GradientBoostingForest): 0.13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Build and train the first neural network (base model 1)\n",
    "model_nn_base_1 = Sequential()\n",
    "model_nn_base_1.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(10, activation='relu'))\n",
    "model_nn_base_1.add(Dense(1, activation='linear'))\n",
    "model_nn_base_1.compile(optimizer=Adam(lr=0.01), loss='mean_squared_error')\n",
    "model_nn_base_1.fit(X_train, y_train, epochs=200, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the first base neural network model\n",
    "y_pred_nn_base_1 = model_nn_base_1.predict(X_test)\n",
    "\n",
    "# Build and train the second neural network (base model 2)\n",
    "model_nn_base_2 = Sequential()\n",
    "model_nn_base_2.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(10, activation='relu'))\n",
    "model_nn_base_2.add(Dense(5, activation='relu'))\n",
    "model_nn_base_2.add(Dense(1, activation='linear'))\n",
    "model_nn_base_2.compile(optimizer=Adam(lr=0.01), loss='mean_squared_error')\n",
    "model_nn_base_2.fit(X_train, y_train, epochs=200, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the second base neural network model\n",
    "y_pred_nn_base_2 = model_nn_base_2.predict(X_test)\n",
    "\n",
    "# Combine predictions from both base models\n",
    "X_meta_train = np.concatenate((y_pred_nn_base_1, y_pred_nn_base_2), axis=1)\n",
    "\n",
    "# Build and train a Random Forest Regressor as a meta-learner\n",
    "model_rf_meta = GradientBoostingRegressor(n_estimators=250, random_state=42)\n",
    "model_rf_meta.fit(X_meta_train, y_test)\n",
    "\n",
    "# Make predictions using the base models and meta-learner\n",
    "X_meta_test = np.concatenate((model_nn_base_1.predict(X_test), model_nn_base_2.predict(X_test)), axis=1)\n",
    "y_pred_stacking_rf = model_rf_meta.predict(X_meta_test)\n",
    "\n",
    "# Evaluate the stacking model with Random Forest as meta-learner\n",
    "mse_stacking_rf = mean_squared_error(y_test, y_pred_stacking_rf)\n",
    "print(f\"Mean Squared Error on Test Data (Stacking with GradientBoostingForest): {mse_stacking_rf:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0aef47fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Root Mean Squared Error (RMSE) on unseen data : 0.36\n",
      " Percentage yield for Onion : 2.3881919157430143\n"
     ]
    }
   ],
   "source": [
    "#Testing the model on unseen data\n",
    "y_pred_nn_base_1 = model_nn_base_1.predict(X_tr)\n",
    "y_pred_nn_base_2 = model_nn_base_2.predict(X_tr)\n",
    "X_meta_test = np.concatenate((model_nn_base_1.predict(X_tr), model_nn_base_2.predict(X_tr)), axis=1)\n",
    "y_pred_stacking_rf = model_rf_meta.predict(X_meta_test)\n",
    "\n",
    "mse = mean_squared_error(y_pred_stacking_rf, y_tr)\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse_stacking_rf)\n",
    "print(f\"Root Mean Squared Error (RMSE) on unseen data : {rmse:.2f}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape \n",
    "df = pd.read_csv('Onion.csv')\n",
    "y = df['Yield']\n",
    "rmse = np.sqrt(0.13)\n",
    "p = (rmse / np.mean(y)) * 100 \n",
    "\n",
    "print(f\" Percentage yield for Onion : {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ecbe6c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 1s 6ms/step - loss: 1.0886 - val_loss: 1.1142\n",
      "Epoch 2/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.5278 - val_loss: 0.7187\n",
      "Epoch 3/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.5764 - val_loss: 0.5425\n",
      "Epoch 4/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.5116 - val_loss: 0.5621\n",
      "Epoch 5/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4928 - val_loss: 0.3207\n",
      "Epoch 6/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.5158 - val_loss: 0.6251\n",
      "Epoch 7/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.5154 - val_loss: 0.5628\n",
      "Epoch 8/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4560 - val_loss: 0.5683\n",
      "Epoch 9/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4785 - val_loss: 0.6662\n",
      "Epoch 10/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4584 - val_loss: 0.4034\n",
      "Epoch 11/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4502 - val_loss: 0.4494\n",
      "Epoch 12/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4435 - val_loss: 0.7722\n",
      "Epoch 13/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4325 - val_loss: 0.6766\n",
      "Epoch 14/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4269 - val_loss: 0.8421\n",
      "Epoch 15/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4250 - val_loss: 0.6229\n",
      "Epoch 16/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4168 - val_loss: 0.6330\n",
      "Epoch 17/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4350 - val_loss: 0.3835\n",
      "Epoch 18/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4266 - val_loss: 0.5401\n",
      "Epoch 19/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4305 - val_loss: 0.6011\n",
      "Epoch 20/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4204 - val_loss: 0.4802\n",
      "Epoch 21/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4111 - val_loss: 0.7047\n",
      "Epoch 22/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4141 - val_loss: 0.5837\n",
      "Epoch 23/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3906 - val_loss: 0.5451\n",
      "Epoch 24/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4294 - val_loss: 0.6564\n",
      "Epoch 25/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4149 - val_loss: 0.5846\n",
      "Epoch 26/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4119 - val_loss: 0.5791\n",
      "Epoch 27/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4036 - val_loss: 0.5996\n",
      "Epoch 28/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3751 - val_loss: 0.3937\n",
      "Epoch 29/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3940 - val_loss: 0.4728\n",
      "Epoch 30/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3748 - val_loss: 0.4576\n",
      "Epoch 31/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3738 - val_loss: 0.4517\n",
      "Epoch 32/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3773 - val_loss: 0.6660\n",
      "Epoch 33/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.4148 - val_loss: 0.5518\n",
      "Epoch 34/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3743 - val_loss: 0.5709\n",
      "Epoch 35/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3653 - val_loss: 0.8083\n",
      "Epoch 36/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3503 - val_loss: 0.6257\n",
      "Epoch 37/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3260 - val_loss: 0.7774\n",
      "Epoch 38/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3377 - val_loss: 0.6055\n",
      "Epoch 39/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3269 - val_loss: 0.6148\n",
      "Epoch 40/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3911 - val_loss: 0.7338\n",
      "Epoch 41/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3396 - val_loss: 0.6832\n",
      "Epoch 42/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3368 - val_loss: 0.6138\n",
      "Epoch 43/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3528 - val_loss: 0.5540\n",
      "Epoch 44/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3265 - val_loss: 0.7197\n",
      "Epoch 45/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3422 - val_loss: 0.4880\n",
      "Epoch 46/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3552 - val_loss: 0.4736\n",
      "Epoch 47/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3773 - val_loss: 0.7411\n",
      "Epoch 48/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3859 - val_loss: 0.5523\n",
      "Epoch 49/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3696 - val_loss: 0.7096\n",
      "Epoch 50/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3736 - val_loss: 0.5623\n",
      "Epoch 51/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3643 - val_loss: 0.6039\n",
      "Epoch 52/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3411 - val_loss: 0.7635\n",
      "Epoch 53/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3838 - val_loss: 0.4584\n",
      "Epoch 54/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3331 - val_loss: 0.5540\n",
      "Epoch 55/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3425 - val_loss: 0.4764\n",
      "Epoch 56/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3244 - val_loss: 0.6802\n",
      "Epoch 57/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3233 - val_loss: 0.4482\n",
      "Epoch 58/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3069 - val_loss: 0.5911\n",
      "Epoch 59/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3215 - val_loss: 0.5561\n",
      "Epoch 60/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3351 - val_loss: 0.6548\n",
      "Epoch 61/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3121 - val_loss: 0.5536\n",
      "Epoch 62/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3097 - val_loss: 0.6031\n",
      "Epoch 63/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3245 - val_loss: 0.7126\n",
      "Epoch 64/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2883 - val_loss: 0.7151\n",
      "Epoch 65/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2814 - val_loss: 0.5725\n",
      "Epoch 66/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2934 - val_loss: 0.5562\n",
      "Epoch 67/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2781 - val_loss: 0.6059\n",
      "Epoch 68/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2867 - val_loss: 0.5153\n",
      "Epoch 69/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3068 - val_loss: 0.5552\n",
      "Epoch 70/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2811 - val_loss: 0.6422\n",
      "Epoch 71/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2660 - val_loss: 0.6599\n",
      "Epoch 72/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2815 - val_loss: 0.5736\n",
      "Epoch 73/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2845 - val_loss: 0.5774\n",
      "Epoch 74/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2965 - val_loss: 0.3554\n",
      "Epoch 75/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2939 - val_loss: 0.4995\n",
      "Epoch 76/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2664 - val_loss: 0.4490\n",
      "Epoch 77/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2753 - val_loss: 0.4196\n",
      "Epoch 78/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2869 - val_loss: 0.5335\n",
      "Epoch 79/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2932 - val_loss: 0.5973\n",
      "Epoch 80/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2600 - val_loss: 0.5698\n",
      "Epoch 81/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2826 - val_loss: 0.5974\n",
      "Epoch 82/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2556 - val_loss: 0.1796\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2896 - val_loss: 0.3775\n",
      "Epoch 84/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2856 - val_loss: 0.5615\n",
      "Epoch 85/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3050 - val_loss: 0.4150\n",
      "Epoch 86/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2854 - val_loss: 0.7429\n",
      "Epoch 87/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3573 - val_loss: 0.5034\n",
      "Epoch 88/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3145 - val_loss: 0.8577\n",
      "Epoch 89/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2779 - val_loss: 0.9824\n",
      "Epoch 90/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2497 - val_loss: 0.8234\n",
      "Epoch 91/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2603 - val_loss: 1.1102\n",
      "Epoch 92/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2861 - val_loss: 0.5821\n",
      "Epoch 93/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2770 - val_loss: 0.7284\n",
      "Epoch 94/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2758 - val_loss: 0.6189\n",
      "Epoch 95/200\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.2438 - val_loss: 0.6287\n",
      "Epoch 96/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2265 - val_loss: 0.8143\n",
      "Epoch 97/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2489 - val_loss: 0.6561\n",
      "Epoch 98/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2436 - val_loss: 0.8112\n",
      "Epoch 99/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2378 - val_loss: 0.7410\n",
      "Epoch 100/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2557 - val_loss: 0.6453\n",
      "Epoch 101/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2463 - val_loss: 0.8189\n",
      "Epoch 102/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2502 - val_loss: 0.7589\n",
      "Epoch 103/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2407 - val_loss: 0.4591\n",
      "Epoch 104/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2375 - val_loss: 0.8497\n",
      "Epoch 105/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2429 - val_loss: 0.6270\n",
      "Epoch 106/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2642 - val_loss: 0.7818\n",
      "Epoch 107/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2506 - val_loss: 0.8905\n",
      "Epoch 108/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2549 - val_loss: 0.9201\n",
      "Epoch 109/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2347 - val_loss: 0.5336\n",
      "Epoch 110/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2445 - val_loss: 0.5300\n",
      "Epoch 111/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2211 - val_loss: 0.5512\n",
      "Epoch 112/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2987 - val_loss: 0.7793\n",
      "Epoch 113/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2301 - val_loss: 0.6284\n",
      "Epoch 114/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2210 - val_loss: 0.5776\n",
      "Epoch 115/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2507 - val_loss: 0.5887\n",
      "Epoch 116/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2072 - val_loss: 0.6119\n",
      "Epoch 117/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2514 - val_loss: 0.5986\n",
      "Epoch 118/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2872 - val_loss: 0.5532\n",
      "Epoch 119/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2461 - val_loss: 0.5692\n",
      "Epoch 120/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2514 - val_loss: 0.3672\n",
      "Epoch 121/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2485 - val_loss: 0.8167\n",
      "Epoch 122/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2539 - val_loss: 0.4700\n",
      "Epoch 123/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2662 - val_loss: 0.5430\n",
      "Epoch 124/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2449 - val_loss: 0.3992\n",
      "Epoch 125/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2210 - val_loss: 0.3797\n",
      "Epoch 126/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2222 - val_loss: 0.4726\n",
      "Epoch 127/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2117 - val_loss: 0.5153\n",
      "Epoch 128/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1993 - val_loss: 0.4950\n",
      "Epoch 129/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2231 - val_loss: 0.6422\n",
      "Epoch 130/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2404 - val_loss: 0.4585\n",
      "Epoch 131/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3075 - val_loss: 0.3287\n",
      "Epoch 132/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2863 - val_loss: 0.3850\n",
      "Epoch 133/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2523 - val_loss: 0.3794\n",
      "Epoch 134/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2544 - val_loss: 0.4195\n",
      "Epoch 135/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2454 - val_loss: 0.3913\n",
      "Epoch 136/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2219 - val_loss: 0.3729\n",
      "Epoch 137/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2630 - val_loss: 0.4972\n",
      "Epoch 138/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.3081 - val_loss: 0.2972\n",
      "Epoch 139/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2659 - val_loss: 0.7436\n",
      "Epoch 140/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2461 - val_loss: 0.4183\n",
      "Epoch 141/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2402 - val_loss: 0.7222\n",
      "Epoch 142/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2348 - val_loss: 0.5486\n",
      "Epoch 143/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2288 - val_loss: 0.5078\n",
      "Epoch 144/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2518 - val_loss: 0.5545\n",
      "Epoch 145/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2306 - val_loss: 0.8075\n",
      "Epoch 146/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2573 - val_loss: 0.7140\n",
      "Epoch 147/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2154 - val_loss: 0.6773\n",
      "Epoch 148/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2210 - val_loss: 0.5561\n",
      "Epoch 149/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2384 - val_loss: 0.9367\n",
      "Epoch 150/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2487 - val_loss: 0.8046\n",
      "Epoch 151/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2691 - val_loss: 0.4465\n",
      "Epoch 152/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2389 - val_loss: 0.6220\n",
      "Epoch 153/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2735 - val_loss: 0.6971\n",
      "Epoch 154/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2860 - val_loss: 0.4229\n",
      "Epoch 155/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2475 - val_loss: 0.5416\n",
      "Epoch 156/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2453 - val_loss: 0.4976\n",
      "Epoch 157/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2352 - val_loss: 0.2359\n",
      "Epoch 158/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2509 - val_loss: 0.1982\n",
      "Epoch 159/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2282 - val_loss: 0.3374\n",
      "Epoch 160/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2304 - val_loss: 0.4026\n",
      "Epoch 161/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2422 - val_loss: 0.4508\n",
      "Epoch 162/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2250 - val_loss: 0.2435\n",
      "Epoch 163/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2170 - val_loss: 0.4506\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2285 - val_loss: 0.4577\n",
      "Epoch 165/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1983 - val_loss: 0.6244\n",
      "Epoch 166/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2550 - val_loss: 0.7198\n",
      "Epoch 167/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2398 - val_loss: 0.6954\n",
      "Epoch 168/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2387 - val_loss: 0.4596\n",
      "Epoch 169/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2067 - val_loss: 0.5261\n",
      "Epoch 170/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2022 - val_loss: 0.5436\n",
      "Epoch 171/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2185 - val_loss: 0.4340\n",
      "Epoch 172/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2267 - val_loss: 0.4132\n",
      "Epoch 173/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2134 - val_loss: 0.4296\n",
      "Epoch 174/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2182 - val_loss: 0.6483\n",
      "Epoch 175/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2709 - val_loss: 0.5806\n",
      "Epoch 176/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2213 - val_loss: 0.3991\n",
      "Epoch 177/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1991 - val_loss: 0.5652\n",
      "Epoch 178/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2086 - val_loss: 0.5406\n",
      "Epoch 179/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2180 - val_loss: 0.5691\n",
      "Epoch 180/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2624 - val_loss: 0.5812\n",
      "Epoch 181/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2207 - val_loss: 0.5746\n",
      "Epoch 182/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2103 - val_loss: 0.6008\n",
      "Epoch 183/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2013 - val_loss: 0.4224\n",
      "Epoch 184/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1924 - val_loss: 0.8269\n",
      "Epoch 185/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2093 - val_loss: 0.4935\n",
      "Epoch 186/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2016 - val_loss: 0.5662\n",
      "Epoch 187/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1897 - val_loss: 0.6543\n",
      "Epoch 188/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2004 - val_loss: 0.6863\n",
      "Epoch 189/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2095 - val_loss: 0.9412\n",
      "Epoch 190/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2120 - val_loss: 0.6999\n",
      "Epoch 191/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1944 - val_loss: 0.4964\n",
      "Epoch 192/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1908 - val_loss: 0.6170\n",
      "Epoch 193/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1935 - val_loss: 0.6813\n",
      "Epoch 194/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1982 - val_loss: 0.5019\n",
      "Epoch 195/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1824 - val_loss: 0.7701\n",
      "Epoch 196/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1898 - val_loss: 0.5380\n",
      "Epoch 197/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2469 - val_loss: 0.4430\n",
      "Epoch 198/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2115 - val_loss: 0.4885\n",
      "Epoch 199/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2173 - val_loss: 0.5514\n",
      "Epoch 200/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.2091 - val_loss: 0.6220\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "Epoch 1/200\n",
      "69/69 [==============================] - 1s 6ms/step - loss: 4.6629 - val_loss: 3.9491\n",
      "Epoch 2/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 2.6047 - val_loss: 2.1458\n",
      "Epoch 3/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 1.4793 - val_loss: 1.1326\n",
      "Epoch 4/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.9430 - val_loss: 0.6291\n",
      "Epoch 5/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.7224 - val_loss: 0.4005\n",
      "Epoch 6/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6435 - val_loss: 0.2965\n",
      "Epoch 7/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6194 - val_loss: 0.2512\n",
      "Epoch 8/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6135 - val_loss: 0.2269\n",
      "Epoch 9/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6127 - val_loss: 0.2137\n",
      "Epoch 10/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6124 - val_loss: 0.2174\n",
      "Epoch 11/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6121 - val_loss: 0.2131\n",
      "Epoch 12/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6123 - val_loss: 0.2133\n",
      "Epoch 13/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6127 - val_loss: 0.2101\n",
      "Epoch 14/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6126 - val_loss: 0.2162\n",
      "Epoch 15/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6122 - val_loss: 0.2169\n",
      "Epoch 16/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6124 - val_loss: 0.2182\n",
      "Epoch 17/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6122 - val_loss: 0.2112\n",
      "Epoch 18/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6128 - val_loss: 0.2134\n",
      "Epoch 19/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6131 - val_loss: 0.2194\n",
      "Epoch 20/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6126 - val_loss: 0.2115\n",
      "Epoch 21/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6124 - val_loss: 0.2225\n",
      "Epoch 22/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6126 - val_loss: 0.2090\n",
      "Epoch 23/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6123 - val_loss: 0.2159\n",
      "Epoch 24/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6122 - val_loss: 0.2150\n",
      "Epoch 25/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6140 - val_loss: 0.2093\n",
      "Epoch 26/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6129 - val_loss: 0.2178\n",
      "Epoch 27/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6138 - val_loss: 0.2114\n",
      "Epoch 28/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6122 - val_loss: 0.2157\n",
      "Epoch 29/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6133 - val_loss: 0.2145\n",
      "Epoch 30/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6132 - val_loss: 0.2197\n",
      "Epoch 31/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6130 - val_loss: 0.2075\n",
      "Epoch 32/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6122 - val_loss: 0.2148\n",
      "Epoch 33/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6125 - val_loss: 0.2147\n",
      "Epoch 34/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6126 - val_loss: 0.2096\n",
      "Epoch 35/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6134 - val_loss: 0.2102\n",
      "Epoch 36/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6136 - val_loss: 0.2202\n",
      "Epoch 37/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6127 - val_loss: 0.2017\n",
      "Epoch 38/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6142 - val_loss: 0.2155\n",
      "Epoch 39/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6140 - val_loss: 0.2223\n",
      "Epoch 40/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6125 - val_loss: 0.2132\n",
      "Epoch 41/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6139 - val_loss: 0.2153\n",
      "Epoch 42/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6126 - val_loss: 0.2132\n",
      "Epoch 43/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6130 - val_loss: 0.2090\n",
      "Epoch 44/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6145 - val_loss: 0.2268\n",
      "Epoch 45/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6143 - val_loss: 0.2084\n",
      "Epoch 46/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6137 - val_loss: 0.2165\n",
      "Epoch 47/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6143 - val_loss: 0.2152\n",
      "Epoch 48/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6135 - val_loss: 0.2349\n",
      "Epoch 49/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6129 - val_loss: 0.2102\n",
      "Epoch 50/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6132 - val_loss: 0.2210\n",
      "Epoch 51/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6130 - val_loss: 0.2169\n",
      "Epoch 52/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6138 - val_loss: 0.2285\n",
      "Epoch 53/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6142 - val_loss: 0.2202\n",
      "Epoch 54/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6126 - val_loss: 0.2115\n",
      "Epoch 55/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6137 - val_loss: 0.2230\n",
      "Epoch 56/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6132 - val_loss: 0.2049\n",
      "Epoch 57/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6141 - val_loss: 0.2072\n",
      "Epoch 58/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6133 - val_loss: 0.2184\n",
      "Epoch 59/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6135 - val_loss: 0.2309\n",
      "Epoch 60/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6135 - val_loss: 0.2088\n",
      "Epoch 61/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6138 - val_loss: 0.2061\n",
      "Epoch 62/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6140 - val_loss: 0.2105\n",
      "Epoch 63/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6131 - val_loss: 0.2159\n",
      "Epoch 64/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6140 - val_loss: 0.2209\n",
      "Epoch 65/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6154 - val_loss: 0.2407\n",
      "Epoch 66/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6157 - val_loss: 0.2227\n",
      "Epoch 67/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6143 - val_loss: 0.2007\n",
      "Epoch 68/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6131 - val_loss: 0.2214\n",
      "Epoch 69/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6125 - val_loss: 0.2126\n",
      "Epoch 70/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6128 - val_loss: 0.2160\n",
      "Epoch 71/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6134 - val_loss: 0.2023\n",
      "Epoch 72/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6137 - val_loss: 0.2119\n",
      "Epoch 73/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6129 - val_loss: 0.2171\n",
      "Epoch 74/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6133 - val_loss: 0.2085\n",
      "Epoch 75/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6134 - val_loss: 0.2010\n",
      "Epoch 76/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6124 - val_loss: 0.2257\n",
      "Epoch 77/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6131 - val_loss: 0.2137\n",
      "Epoch 78/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6136 - val_loss: 0.2234\n",
      "Epoch 79/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6126 - val_loss: 0.2094\n",
      "Epoch 80/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6135 - val_loss: 0.2136\n",
      "Epoch 81/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6130 - val_loss: 0.1989\n",
      "Epoch 82/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6160 - val_loss: 0.1985\n",
      "Epoch 83/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6131 - val_loss: 0.2237\n",
      "Epoch 84/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6139 - val_loss: 0.2105\n",
      "Epoch 85/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6150 - val_loss: 0.2026\n",
      "Epoch 86/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6132 - val_loss: 0.2165\n",
      "Epoch 87/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6132 - val_loss: 0.1989\n",
      "Epoch 88/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6132 - val_loss: 0.2058\n",
      "Epoch 89/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6135 - val_loss: 0.1980\n",
      "Epoch 90/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6132 - val_loss: 0.2077\n",
      "Epoch 91/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6130 - val_loss: 0.2169\n",
      "Epoch 92/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6146 - val_loss: 0.2325\n",
      "Epoch 93/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6141 - val_loss: 0.2192\n",
      "Epoch 94/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6141 - val_loss: 0.2218\n",
      "Epoch 95/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6139 - val_loss: 0.2178\n",
      "Epoch 96/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6158 - val_loss: 0.1900\n",
      "Epoch 97/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6138 - val_loss: 0.2158\n",
      "Epoch 98/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6138 - val_loss: 0.2158\n",
      "Epoch 99/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6131 - val_loss: 0.2195\n",
      "Epoch 100/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6130 - val_loss: 0.2030\n",
      "Epoch 101/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6133 - val_loss: 0.2078\n",
      "Epoch 102/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6127 - val_loss: 0.2221\n",
      "Epoch 103/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6133 - val_loss: 0.2071\n",
      "Epoch 104/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6130 - val_loss: 0.2107\n",
      "Epoch 105/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6138 - val_loss: 0.2073\n",
      "Epoch 106/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6143 - val_loss: 0.2237\n",
      "Epoch 107/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6147 - val_loss: 0.2178\n",
      "Epoch 108/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6123 - val_loss: 0.2135\n",
      "Epoch 109/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6125 - val_loss: 0.2177\n",
      "Epoch 110/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6132 - val_loss: 0.2032\n",
      "Epoch 111/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6133 - val_loss: 0.2153\n",
      "Epoch 112/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6132 - val_loss: 0.2217\n",
      "Epoch 113/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6146 - val_loss: 0.2019\n",
      "Epoch 114/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6146 - val_loss: 0.2196\n",
      "Epoch 115/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6133 - val_loss: 0.2114\n",
      "Epoch 116/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6127 - val_loss: 0.2303\n",
      "Epoch 117/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6133 - val_loss: 0.2253\n",
      "Epoch 118/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6128 - val_loss: 0.2144\n",
      "Epoch 119/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6132 - val_loss: 0.2213\n",
      "Epoch 120/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6129 - val_loss: 0.2198\n",
      "Epoch 121/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6139 - val_loss: 0.2259\n",
      "Epoch 122/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6134 - val_loss: 0.2038\n",
      "Epoch 123/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6126 - val_loss: 0.2197\n",
      "Epoch 124/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6139 - val_loss: 0.2182\n",
      "Epoch 125/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6138 - val_loss: 0.2261\n",
      "Epoch 126/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6135 - val_loss: 0.2159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6128 - val_loss: 0.2224\n",
      "Epoch 128/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6135 - val_loss: 0.2240\n",
      "Epoch 129/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6132 - val_loss: 0.2121\n",
      "Epoch 130/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6137 - val_loss: 0.2199\n",
      "Epoch 131/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6146 - val_loss: 0.2252\n",
      "Epoch 132/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6132 - val_loss: 0.2170\n",
      "Epoch 133/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6129 - val_loss: 0.2191\n",
      "Epoch 134/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6137 - val_loss: 0.2246\n",
      "Epoch 135/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6136 - val_loss: 0.2292\n",
      "Epoch 136/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6145 - val_loss: 0.2228\n",
      "Epoch 137/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6134 - val_loss: 0.2180\n",
      "Epoch 138/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6131 - val_loss: 0.2169\n",
      "Epoch 139/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6128 - val_loss: 0.2052\n",
      "Epoch 140/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6150 - val_loss: 0.2372\n",
      "Epoch 141/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6143 - val_loss: 0.2282\n",
      "Epoch 142/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6137 - val_loss: 0.2174\n",
      "Epoch 143/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6133 - val_loss: 0.2049\n",
      "Epoch 144/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6134 - val_loss: 0.2133\n",
      "Epoch 145/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6139 - val_loss: 0.2290\n",
      "Epoch 146/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6132 - val_loss: 0.2128\n",
      "Epoch 147/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6143 - val_loss: 0.2088\n",
      "Epoch 148/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6131 - val_loss: 0.2230\n",
      "Epoch 149/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6152 - val_loss: 0.2266\n",
      "Epoch 150/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6133 - val_loss: 0.2106\n",
      "Epoch 151/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6163 - val_loss: 0.2192\n",
      "Epoch 152/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6133 - val_loss: 0.2156\n",
      "Epoch 153/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6161 - val_loss: 0.2013\n",
      "Epoch 154/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6140 - val_loss: 0.2107\n",
      "Epoch 155/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6131 - val_loss: 0.2103\n",
      "Epoch 156/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6156 - val_loss: 0.1979\n",
      "Epoch 157/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6148 - val_loss: 0.2127\n",
      "Epoch 158/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6130 - val_loss: 0.2201\n",
      "Epoch 159/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6149 - val_loss: 0.2173\n",
      "Epoch 160/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6135 - val_loss: 0.2171\n",
      "Epoch 161/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6136 - val_loss: 0.2230\n",
      "Epoch 162/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6144 - val_loss: 0.1985\n",
      "Epoch 163/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6143 - val_loss: 0.2072\n",
      "Epoch 164/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6135 - val_loss: 0.2255\n",
      "Epoch 165/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6128 - val_loss: 0.2153\n",
      "Epoch 166/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6125 - val_loss: 0.2159\n",
      "Epoch 167/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6131 - val_loss: 0.1992\n",
      "Epoch 168/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6140 - val_loss: 0.2030\n",
      "Epoch 169/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6139 - val_loss: 0.2284\n",
      "Epoch 170/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6138 - val_loss: 0.2091\n",
      "Epoch 171/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6141 - val_loss: 0.2068\n",
      "Epoch 172/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6132 - val_loss: 0.2375\n",
      "Epoch 173/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6128 - val_loss: 0.2144\n",
      "Epoch 174/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6124 - val_loss: 0.2072\n",
      "Epoch 175/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6128 - val_loss: 0.2207\n",
      "Epoch 176/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6126 - val_loss: 0.2155\n",
      "Epoch 177/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6132 - val_loss: 0.2073\n",
      "Epoch 178/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6129 - val_loss: 0.2122\n",
      "Epoch 179/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6146 - val_loss: 0.2208\n",
      "Epoch 180/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6129 - val_loss: 0.1889\n",
      "Epoch 181/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6151 - val_loss: 0.2165\n",
      "Epoch 182/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6148 - val_loss: 0.2166\n",
      "Epoch 183/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6129 - val_loss: 0.2164\n",
      "Epoch 184/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6136 - val_loss: 0.2214\n",
      "Epoch 185/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6140 - val_loss: 0.2060\n",
      "Epoch 186/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6135 - val_loss: 0.2231\n",
      "Epoch 187/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6140 - val_loss: 0.2202\n",
      "Epoch 188/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6137 - val_loss: 0.2118\n",
      "Epoch 189/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6135 - val_loss: 0.2052\n",
      "Epoch 190/200\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6125 - val_loss: 0.2092\n",
      "Epoch 191/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6131 - val_loss: 0.2223\n",
      "Epoch 192/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6142 - val_loss: 0.2203\n",
      "Epoch 193/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6143 - val_loss: 0.2236\n",
      "Epoch 194/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6133 - val_loss: 0.2154\n",
      "Epoch 195/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6130 - val_loss: 0.1998\n",
      "Epoch 196/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6136 - val_loss: 0.1984\n",
      "Epoch 197/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6141 - val_loss: 0.2047\n",
      "Epoch 198/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6129 - val_loss: 0.2198\n",
      "Epoch 199/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6123 - val_loss: 0.2092\n",
      "Epoch 200/200\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6134 - val_loss: 0.2070\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Mean Squared Error on Test Data (Stacking with GradientBoostingForest): 0.04\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "crp5=pd.read_csv('Rice.csv')\n",
    "gj1=crp5.drop(['Crop','Year','Season','State','TotalRainfall'],axis=1)\n",
    "import numpy as np\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "x = crp5[features]\n",
    "y = crp5['Yield']\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sx=StandardScaler().fit(x)\n",
    "y=np.array(y)\n",
    "x=sx.transform(x)\n",
    "X_train=x[:550]\n",
    "X_test=x[550:560]\n",
    "y_train=y[:550]\n",
    "y_test=y[550:560]\n",
    "X_tr=x[560:]\n",
    "y_tr=y[560:]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Build and train the first neural network (base model 1)\n",
    "model_nn_base_1 = Sequential()\n",
    "model_nn_base_1.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(10, activation='relu'))\n",
    "model_nn_base_1.add(Dense(1, activation='linear'))\n",
    "model_nn_base_1.compile(optimizer=Adam(lr=0.01), loss='mean_squared_error')\n",
    "model_nn_base_1.fit(X_train, y_train, epochs=200, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the first base neural network model\n",
    "y_pred_nn_base_1 = model_nn_base_1.predict(X_test)\n",
    "\n",
    "# Build and train the second neural network (base model 2)\n",
    "model_nn_base_2 = Sequential()\n",
    "model_nn_base_2.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(10, activation='relu'))\n",
    "model_nn_base_2.add(Dense(5, activation='relu'))\n",
    "model_nn_base_2.add(Dense(1, activation='linear'))\n",
    "model_nn_base_2.compile(optimizer=Adam(lr=0.01), loss='mean_squared_error')\n",
    "model_nn_base_2.fit(X_train, y_train, epochs=200, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the second base neural network model\n",
    "y_pred_nn_base_2 = model_nn_base_2.predict(X_test)\n",
    "\n",
    "# Combine predictions from both base models\n",
    "X_meta_train = np.concatenate((y_pred_nn_base_1, y_pred_nn_base_2), axis=1)\n",
    "\n",
    "# Build and train a Random Forest Regressor as a meta-learner\n",
    "model_rf_meta = GradientBoostingRegressor(n_estimators=150,learning_rate = 0.008 ,random_state=42)\n",
    "model_rf_meta.fit(X_meta_train, y_test)\n",
    "\n",
    "# Make predictions using the base models and meta-learner\n",
    "X_meta_test = np.concatenate((model_nn_base_1.predict(X_test), model_nn_base_2.predict(X_test)), axis=1)\n",
    "y_pred_stacking_rf = model_rf_meta.predict(X_meta_test)\n",
    "\n",
    "# Evaluate the stacking model with Random Forest as meta-learner\n",
    "mse_stacking_rf = mean_squared_error(y_test, y_pred_stacking_rf)\n",
    "print(f\"Mean Squared Error on Test Data (Stacking with GradientBoostingForest): {mse_stacking_rf:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "28f31091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Root Mean Squared Error (RMSE) on unseen data : 0.20\n",
      " Percentage yield for Rice: 15.32376438225398\n"
     ]
    }
   ],
   "source": [
    "#Testing the model on unseen data\n",
    "y_pred_nn_base_1 = model_nn_base_1.predict(X_tr)\n",
    "y_pred_nn_base_2 = model_nn_base_2.predict(X_tr)\n",
    "X_meta_test = np.concatenate((model_nn_base_1.predict(X_tr), model_nn_base_2.predict(X_tr)), axis=1)\n",
    "y_pred_stacking_rf = model_rf_meta.predict(X_meta_test)\n",
    "\n",
    "mse = mean_squared_error(y_pred_stacking_rf, y_tr)\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse_stacking_rf)\n",
    "print(f\"Root Mean Squared Error (RMSE) on unseen data : {rmse:.2f}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape \n",
    "df = pd.read_csv('Rice.csv')\n",
    "y = df['Yield']\n",
    "rmse = np.sqrt(0.13)\n",
    "p = (rmse / np.mean(y)) * 100 \n",
    "\n",
    "print(f\" Percentage yield for Rice: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4386dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
