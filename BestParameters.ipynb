{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce635f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "319afa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Onion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8411192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall','Fertilizer','Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df[features]\n",
    "y = df['Yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b46f629f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_3312\\4136503804.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[features] = scaler.fit_transform(X[features])\n",
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_3312\\4136503804.py:39: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_model = KerasRegressor(build_fn=build_model, epochs=200, batch_size=16, verbose=0)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Data: 75.79\n",
      "Prediction: 12.63, Actual: 8.55\n",
      "Prediction: 11.62, Actual: 14.46\n",
      "Prediction: 24.48, Actual: 8.50\n",
      "Prediction: 9.38, Actual: 5.00\n",
      "Prediction: 9.70, Actual: 2.79\n",
      "Prediction: 3.30, Actual: 8.01\n",
      "Prediction: 24.48, Actual: 14.75\n",
      "Prediction: 24.48, Actual: 8.03\n",
      "Prediction: 3.94, Actual: 2.21\n",
      "Prediction: 8.19, Actual: 5.13\n",
      "Best Hyperparameters: {'activation': ('sigmoid', 'sigmoid', 'sigmoid'), 'hidden_layer_sizes': (200, 100, 50), 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Assuming 'df' is your DataFrame with the provided data\n",
    "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
    "df = pd.read_csv('Onion.csv')\n",
    "# Select relevant features for training\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df[features]\n",
    "y = df['Yield']\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X[features] = scaler.fit_transform(X[features])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model function\n",
    "def build_model(hidden_layer_sizes=(200, 100, 50), learning_rate=0.001, activation=('relu', 'relu', 'relu')):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_sizes[0], activation=activation[0], input_dim=len(features)))\n",
    "    model.add(Dense(hidden_layer_sizes[1], activation=activation[1]))\n",
    "    model.add(Dense(hidden_layer_sizes[2], activation=activation[2]))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Wrap Keras model using KerasRegressor\n",
    "keras_model = KerasRegressor(build_fn=build_model, epochs=200, batch_size=16, verbose=0)\n",
    "activation_combinations = [\n",
    "    ('relu', 'relu', 'relu'),\n",
    "    ('relu', 'relu', 'tanh'),\n",
    "    ('relu', 'relu', 'sigmoid'),\n",
    "    ('relu', 'tanh', 'relu'),\n",
    "    ('relu', 'tanh', 'tanh'),\n",
    "    ('relu', 'tanh', 'sigmoid'),\n",
    "    ('relu', 'sigmoid', 'relu'),\n",
    "    ('relu', 'sigmoid', 'tanh'),\n",
    "    ('relu', 'sigmoid', 'sigmoid'),\n",
    "    ('tanh', 'relu', 'relu'),\n",
    "    ('tanh', 'relu', 'tanh'),\n",
    "    ('tanh', 'relu', 'sigmoid'),\n",
    "    ('tanh', 'tanh', 'relu'),\n",
    "    ('tanh', 'tanh', 'tanh'),\n",
    "    ('tanh', 'tanh', 'sigmoid'),\n",
    "    ('tanh', 'sigmoid', 'relu'),\n",
    "    ('tanh', 'sigmoid', 'tanh'),\n",
    "    ('tanh', 'sigmoid', 'sigmoid'),\n",
    "    ('sigmoid', 'relu', 'relu'),\n",
    "    ('sigmoid', 'relu', 'tanh'),\n",
    "    ('sigmoid', 'relu', 'sigmoid'),\n",
    "    ('sigmoid', 'tanh', 'relu'),\n",
    "    ('sigmoid', 'tanh', 'tanh'),\n",
    "    ('sigmoid', 'tanh', 'sigmoid'),\n",
    "    ('sigmoid', 'sigmoid', 'relu'),\n",
    "    ('sigmoid', 'sigmoid', 'tanh'),\n",
    "    ('sigmoid', 'sigmoid', 'sigmoid')\n",
    "]\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(200, 100, 50), (100, 50, 25), (150, 75, 30)],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'activation': activation_combinations\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=keras_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "\n",
    "# Display the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd173a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_3312\\1483187854.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[features] = scaler.fit_transform(X[features])\n",
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_3312\\1483187854.py:39: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_model = KerasRegressor(build_fn=build_model, epochs=200, batch_size=16, verbose=0)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Assuming 'df' is your DataFrame with the provided data\n",
    "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
    "df = pd.read_csv('Potato.csv')\n",
    "# Select relevant features for training\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df[features]\n",
    "y = df['Yield']\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X[features] = scaler.fit_transform(X[features])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model function\n",
    "def build_model(hidden_layer_sizes=(200, 100, 50), learning_rate=0.001, activation=('relu', 'relu', 'relu')):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_sizes[0], activation=activation[0], input_dim=len(features)))\n",
    "    model.add(Dense(hidden_layer_sizes[1], activation=activation[1]))\n",
    "    model.add(Dense(hidden_layer_sizes[2], activation=activation[2]))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Wrap Keras model using KerasRegressor\n",
    "keras_model = KerasRegressor(build_fn=build_model, epochs=200, batch_size=16, verbose=0)\n",
    "activation_combinations = [\n",
    "    ('relu', 'relu', 'relu'),\n",
    "    ('relu', 'relu', 'tanh'),\n",
    "    ('relu', 'relu', 'sigmoid'),\n",
    "    ('relu', 'tanh', 'relu'),\n",
    "    ('relu', 'tanh', 'tanh'),\n",
    "    ('relu', 'tanh', 'sigmoid'),\n",
    "    ('relu', 'sigmoid', 'relu'),\n",
    "    ('relu', 'sigmoid', 'tanh'),\n",
    "    ('relu', 'sigmoid', 'sigmoid'),\n",
    "    ('tanh', 'relu', 'relu'),\n",
    "    ('tanh', 'relu', 'tanh'),\n",
    "    ('tanh', 'relu', 'sigmoid'),\n",
    "    ('tanh', 'tanh', 'relu'),\n",
    "    ('tanh', 'tanh', 'tanh'),\n",
    "    ('tanh', 'tanh', 'sigmoid'),\n",
    "    ('tanh', 'sigmoid', 'relu'),\n",
    "    ('tanh', 'sigmoid', 'tanh'),\n",
    "    ('tanh', 'sigmoid', 'sigmoid'),\n",
    "    ('sigmoid', 'relu', 'relu'),\n",
    "    ('sigmoid', 'relu', 'tanh'),\n",
    "    ('sigmoid', 'relu', 'sigmoid'),\n",
    "    ('sigmoid', 'tanh', 'relu'),\n",
    "    ('sigmoid', 'tanh', 'tanh'),\n",
    "    ('sigmoid', 'tanh', 'sigmoid'),\n",
    "    ('sigmoid', 'sigmoid', 'relu'),\n",
    "    ('sigmoid', 'sigmoid', 'tanh'),\n",
    "    ('sigmoid', 'sigmoid', 'sigmoid')\n",
    "]\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(200, 100, 50), (100, 50, 25), (150, 75, 30)],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'activation': activation_combinations\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=keras_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "\n",
    "# Display the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d0337e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Pesticide', 'Fertilizer']\n",
    "df_without_columns = X.drop(columns=columns_to_drop,axis=1)\n",
    "X = df_without_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a0976c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(278, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df1 = pd.DataFrame(data)\n",
    "\n",
    "matrix_data = X.values\n",
    "\n",
    "print(matrix_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "538f6006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.53088889e+01 1.84063158e+01 1.58561111e+01 2.50905263e+01\n",
      " 1.77862500e+01 1.82145000e+01 1.34182353e+01 2.45289474e+01\n",
      " 1.53528571e+01 2.53437500e+01 1.84018182e+01 1.96354545e+01\n",
      " 1.78454545e+01 1.91209091e+01 1.66590909e+01 1.49958333e+01\n",
      " 2.10541667e+01 2.21100000e+01 3.81420000e+02 4.46200000e+01\n",
      " 2.47572727e+02 8.02680000e+01 2.22244444e+01 2.17675000e+01\n",
      " 1.01544000e+01 9.87040000e+00 1.11670370e+01 1.13196154e+01\n",
      " 8.30461538e+00 8.11407407e+00 8.55111111e+00 9.52416667e+00\n",
      " 6.71107143e+00 1.02769231e+01 5.26740741e+00 9.71416667e+00\n",
      " 9.68407407e+00 8.91269231e+00 5.87851852e+00 9.08814815e+00\n",
      " 7.23740741e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 2.66050000e+01 2.66927778e+01 2.60365000e+01 2.46695652e+01\n",
      " 2.69650000e+01 2.33210000e+01 2.20940000e+01 3.07339130e+01\n",
      " 2.79612500e+01 2.81872414e+01 2.65060000e+01 2.69415385e+01\n",
      " 2.61790000e+01 2.63870370e+01 2.65553333e+01 2.87637931e+01\n",
      " 2.51940000e+01 2.77052000e+01 2.75400000e+01 2.88925000e+01\n",
      " 2.84640000e+01 1.88266667e+01 3.18137500e+01 1.50514286e+01\n",
      " 2.68455556e+01 1.35075000e+01 2.89588889e+01 1.56852941e+01\n",
      " 3.11192593e+01 1.81583333e+01 3.46142857e+01 1.28200000e+01\n",
      " 2.81551852e+01 2.19547826e+01 3.28596667e+01 1.78653077e+02\n",
      " 1.93578571e+01 1.99564286e+01 1.67075000e+01 2.59690909e+01\n",
      " 2.24571429e+01 2.25604762e+01 2.31633333e+01 2.48300000e+01\n",
      " 2.39766667e+01 2.47386364e+01 2.45418182e+01 1.20858000e+01\n",
      " 1.31430000e+01 1.47518000e+01 1.77412000e+01 1.97530000e+01\n",
      " 1.72543137e+01 2.24301961e+01 2.39613726e+01 2.30056863e+01\n",
      " 2.30839216e+01 2.32174510e+01 1.17172857e+01 1.69332394e+01\n",
      " 1.37182609e+01 1.83990141e+01 2.28756944e+01 1.07907042e+01\n",
      " 2.30008108e+01 1.08554930e+01 2.03362667e+01 1.33398630e+01\n",
      " 1.52939726e+01 1.44189041e+01 1.17680556e+01 1.31457746e+01\n",
      " 1.39327397e+01 1.32854286e+01 1.45101351e+01 1.41459155e+01\n",
      " 1.58014667e+01 1.18767606e+01 1.44617333e+01 1.61510606e+01\n",
      " 7.00000000e+00 7.00000000e+00 1.80000000e+01 1.80000000e+01\n",
      " 1.80000000e+01 1.80000000e+01 9.00000000e+00 9.67000000e+00\n",
      " 6.00000000e+00 5.00000000e+00 5.00000000e+00 5.00000000e+00\n",
      " 3.00000000e+00 6.00000000e+00 5.00000000e+00 5.00000000e+00\n",
      " 5.13461539e+00 4.62583333e+00 5.69583333e+00 5.80846154e+00\n",
      " 5.80916667e+00 5.75416667e+00 5.74416667e+00 5.50583333e+00\n",
      " 7.25833333e+00 1.64400000e+01 1.61438462e+01 1.61461539e+01\n",
      " 1.41846154e+01 8.77571429e+00 1.07086487e+01 1.37842857e+01\n",
      " 1.17691667e+01 1.26052632e+01 3.00000000e+00 4.00000000e+00\n",
      " 1.46000000e+01 3.63000000e+00 7.24000000e+00 4.29250000e+00\n",
      " 3.76000000e+00 1.22141379e+01 4.96434783e+00 4.76380952e+00\n",
      " 6.59000000e+00 6.49950000e+00 6.11782609e+00 2.17695652e+01\n",
      " 6.09375000e+00 8.01818182e+00 7.85666667e+00 5.79500000e+00\n",
      " 5.74400000e+00 9.19800000e+00 9.48200000e+00 9.46666667e+00\n",
      " 9.47100000e+00 9.95400000e+00 6.06000000e+00 6.00100000e+00\n",
      " 7.29000000e+00 5.71760000e+00 6.77136364e+00 5.57952381e+00\n",
      " 4.29680000e+00 7.08950000e+00 7.65000000e+00 6.89680000e+00\n",
      " 7.21904762e+00 1.05980952e+01 5.51840000e+00 8.49545454e+00\n",
      " 6.21428571e+00 5.53880000e+00 7.05666667e+00 5.65095238e+00\n",
      " 6.96880000e+00 7.31260870e+00 8.03333333e+00 5.48000000e+00\n",
      " 7.59909091e+00 5.11409091e+00 4.71680000e+00 6.20708333e+00\n",
      " 4.53666667e+00 7.58720000e+00 1.31262500e+01 1.42069565e+01\n",
      " 1.12825000e+01 1.34941667e+01 1.18077273e+01 8.40240000e+00\n",
      " 1.20608333e+01 1.03610000e+01 8.25615385e+00 1.32188461e+01\n",
      " 1.13733333e+01 2.38148148e+00 2.37518518e+00 2.30851852e+00\n",
      " 3.02962963e+00 3.26518518e+00 3.34703704e+00 3.38259259e+00\n",
      " 6.91629630e+00 7.14111111e+00 7.29703704e+00 7.45518519e+00\n",
      " 8.02296296e+00 8.00600000e+00 8.33100000e+00 8.42700000e+00\n",
      " 8.42700000e+00 8.42900000e+00 8.43100000e+00 5.27312500e+00\n",
      " 4.29266667e+00 5.32625000e+00 4.76478261e+00 3.12041667e+00\n",
      " 2.03300000e+01 1.34600000e+01 6.47360000e+00 8.96086956e+00\n",
      " 5.97304348e+00 2.78695652e+00 6.25086957e+00 5.64181818e+00\n",
      " 5.07080000e+00 2.44750000e+00 1.02714286e+00 2.41333333e+00\n",
      " 9.84285714e-01 2.20714286e+00 9.32500000e-01 2.20571429e+00\n",
      " 9.40000000e-01 2.39142857e+00 2.39142857e+00 8.40000000e-01\n",
      " 6.00000000e-01 2.40500000e+00 5.00000000e-01 2.42555556e+00\n",
      " 5.00000000e-01 2.40000000e+00 5.00000000e-01 1.99250000e+00\n",
      " 3.35000000e-01 2.42666667e+00]\n"
     ]
    }
   ],
   "source": [
    "matrix_data1 = y.values\n",
    "print(matrix_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ad5a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = matrix_data1.reshape(-1,1)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c3b48c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = matrix_data\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "547e1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape\n",
    "testingvalues = a[5:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "66529737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens = torch.tensor([[0.02, 0.1, 0.15],\n",
    "                            [0.7, 0.6, 0.8],\n",
    "                            [1.5, 1.2, 1.7],\n",
    "                            [3.2, 2.9, 3.1]])\n",
    "tens.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "95e86238",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_outputs = torch.tensor([[0.1],\n",
    "                             [0.6],\n",
    "                             [1.3],\n",
    "                             [2.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5685db58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0200, 0.1000, 0.1500, 0.5000],\n",
      "        [0.7000, 0.6000, 0.8000, 1.0000],\n",
      "        [1.5000, 1.2000, 1.7000, 2.0000],\n",
      "        [3.2000, 2.9000, 3.1000, 4.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Existing tensor\n",
    "existing_tensor = torch.tensor([[0.02, 0.1, 0.15],\n",
    "                                [0.7, 0.6, 0.8],\n",
    "                                [1.5, 1.2, 1.7],\n",
    "                                [3.2, 2.9, 3.1]])\n",
    "\n",
    "# New column data\n",
    "new_column_data = torch.tensor([[0.5],\n",
    "                                [1.0],\n",
    "                                [2.0],\n",
    "                                [4.0]])\n",
    "\n",
    "# Add the new column\n",
    "new_tensor = torch.cat((existing_tensor, new_column_data), dim=1)\n",
    "\n",
    "print(new_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60d68baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ddfd2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f094bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Onion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb3f7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall','Fertilizer','Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df[features]\n",
    "y = df['Yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e3f54509",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs = torch.tensor(a)\n",
    "\n",
    "# Data outputs\n",
    "data_outputs = torch.tensor(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a2310329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_outputs.dtype == data_inputs.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fead68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = a.astype(np.float32)\n",
    "b = b.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5047336a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "10fc3c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278, 2)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newa = a[:,:2]\n",
    "newa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e64a89e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d0f26f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.7200001e+01, 6.7000000e+01, 9.4459998e+02, 3.8224580e+06,\n",
       "        2.4050701e+03],\n",
       "       [2.7200001e+01, 6.7000000e+01, 9.4459998e+02, 1.9649405e+06,\n",
       "        1.2363300e+03],\n",
       "       [2.6500000e+01, 6.4000000e+01, 7.1100000e+02, 3.5766922e+06,\n",
       "        3.9021799e+03],\n",
       "       [2.6500000e+01, 6.4000000e+01, 7.1100000e+02, 2.1203985e+06,\n",
       "        2.3133601e+03],\n",
       "       [2.7799999e+01, 6.9000000e+01, 1.2970000e+03, 4.2544095e+06,\n",
       "        6.1468799e+03]], dtype=float32)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2e4ed835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15.308888],\n",
       "       [18.406315],\n",
       "       [15.856112],\n",
       "       [25.090527],\n",
       "       [17.78625 ]], dtype=float32)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8cb06a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_input = torch.tensor([[27.2, 67, 944.6, 3822457.92, 2405.07],\n",
    "                              [27.2, 67, 944.6, 1964940.48, 1236.33],\n",
    "                              [26.5, 64, 711.0, 3576692.28, 3902.18],\n",
    "                              [26.5, 64, 711.0, 2120398.56, 2313.36],\n",
    "                              [27.8, 69, 1297.0, 4254409.32, 6146.88]])\n",
    "\n",
    "rounded_output = torch.tensor([[15.31],\n",
    "                               [18.41],\n",
    "                               [15.86],\n",
    "                               [25.09],\n",
    "                               [17.79]])\n",
    "\n",
    "rounded_input_first_two_columns = torch.tensor([[27.2, 67],\n",
    "                                                [27.2, 67],\n",
    "                                                [26.5, 64],\n",
    "                                                [26.5, 64],\n",
    "                                                [27.8, 69]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dcd36780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=5, out_features=2, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "Generation = 1\n",
      "Fitness    = -3.8171720505760742\n",
      "Generation = 2\n",
      "Fitness    = -3.8171720505760742\n",
      "Generation = 3\n",
      "Fitness    = -3.8171720505760742\n",
      "Generation = 4\n",
      "Fitness    = -3.8171720505760742\n",
      "Generation = 5\n",
      "Fitness    = -3.776333808998926\n",
      "Generation = 6\n",
      "Fitness    = -3.776333808998926\n",
      "Generation = 7\n",
      "Fitness    = -3.776333808998926\n",
      "Generation = 8\n",
      "Fitness    = -3.776333808998926\n",
      "Generation = 9\n",
      "Fitness    = -3.776333808998926\n",
      "Generation = 10\n",
      "Fitness    = -3.776333808998926\n",
      "Generation = 11\n",
      "Fitness    = -3.776333808998926\n",
      "Generation = 12\n",
      "Fitness    = -3.776333808998926\n",
      "Generation = 13\n",
      "Fitness    = -3.744821310143335\n",
      "Generation = 14\n",
      "Fitness    = -3.744821310143335\n",
      "Generation = 15\n",
      "Fitness    = -3.744821310143335\n",
      "Generation = 16\n",
      "Fitness    = -3.744821310143335\n",
      "Generation = 17\n",
      "Fitness    = -3.744821310143335\n",
      "Generation = 18\n",
      "Fitness    = -3.744821310143335\n",
      "Generation = 19\n",
      "Fitness    = -3.744821310143335\n",
      "Generation = 20\n",
      "Fitness    = -3.6825571061180664\n",
      "Generation = 21\n",
      "Fitness    = -3.6825571061180664\n",
      "Generation = 22\n",
      "Fitness    = -3.6825571061180664\n",
      "Generation = 23\n",
      "Fitness    = -3.676560163597925\n",
      "Generation = 24\n",
      "Fitness    = -3.676560163597925\n",
      "Generation = 25\n",
      "Fitness    = -3.651904821495874\n",
      "Generation = 26\n",
      "Fitness    = -3.6364095212029053\n",
      "Generation = 27\n",
      "Fitness    = -3.6261930466698242\n",
      "Generation = 28\n",
      "Fitness    = -3.6261930466698242\n",
      "Generation = 29\n",
      "Fitness    = -3.5431785584496094\n",
      "Generation = 30\n",
      "Fitness    = -3.5431785584496094\n",
      "Generation = 31\n",
      "Fitness    = -3.5431785584496094\n",
      "Generation = 32\n",
      "Fitness    = -3.5431785584496094\n",
      "Generation = 33\n",
      "Fitness    = -3.5431785584496094\n",
      "Generation = 34\n",
      "Fitness    = -3.5431785584496094\n",
      "Generation = 35\n",
      "Fitness    = -3.5431785584496094\n",
      "Generation = 36\n",
      "Fitness    = -3.5431785584496094\n",
      "Generation = 37\n",
      "Fitness    = -3.5431785584496094\n",
      "Generation = 38\n",
      "Fitness    = -3.5431785584496094\n",
      "Generation = 39\n",
      "Fitness    = -3.5431785584496094\n",
      "Generation = 40\n",
      "Fitness    = -3.5431785584496094\n",
      "Generation = 41\n",
      "Fitness    = -3.5431785584496094\n",
      "Generation = 42\n",
      "Fitness    = -3.5431785584496094\n",
      "Generation = 43\n",
      "Fitness    = -3.5210037232445313\n",
      "Generation = 44\n",
      "Fitness    = -3.5210037232445313\n",
      "Generation = 45\n",
      "Fitness    = -3.5210037232445313\n",
      "Generation = 46\n",
      "Fitness    = -3.5210037232445313\n",
      "Generation = 47\n",
      "Fitness    = -3.5210037232445313\n",
      "Generation = 48\n",
      "Fitness    = -3.5210037232445313\n",
      "Generation = 49\n",
      "Fitness    = -3.4689266682671143\n",
      "Generation = 50\n",
      "Fitness    = -3.4689266682671143\n",
      "Generation = 51\n",
      "Fitness    = -3.4689266682671143\n",
      "Generation = 52\n",
      "Fitness    = -3.4689266682671143\n",
      "Generation = 53\n",
      "Fitness    = -3.4689266682671143\n",
      "Generation = 54\n",
      "Fitness    = -3.4689266682671143\n",
      "Generation = 55\n",
      "Fitness    = -3.4689266682671143\n",
      "Generation = 56\n",
      "Fitness    = -3.4689266682671143\n",
      "Generation = 57\n",
      "Fitness    = -3.4689266682671143\n",
      "Generation = 58\n",
      "Fitness    = -3.4689266682671143\n",
      "Generation = 59\n",
      "Fitness    = -3.4689266682671143\n",
      "Generation = 60\n",
      "Fitness    = -3.4473111630486084\n",
      "Generation = 61\n",
      "Fitness    = -3.4473111630486084\n",
      "Generation = 62\n",
      "Fitness    = -3.4473111630486084\n",
      "Generation = 63\n",
      "Fitness    = -3.4473111630486084\n",
      "Generation = 64\n",
      "Fitness    = -3.4473111630486084\n",
      "Generation = 65\n",
      "Fitness    = -3.4473111630486084\n",
      "Generation = 66\n",
      "Fitness    = -3.4473111630486084\n",
      "Generation = 67\n",
      "Fitness    = -3.4473111630486084\n",
      "Generation = 68\n",
      "Fitness    = -3.445265770058496\n",
      "Generation = 69\n",
      "Fitness    = -3.445265770058496\n",
      "Generation = 70\n",
      "Fitness    = -3.443803310494287\n",
      "Generation = 71\n",
      "Fitness    = -3.443803310494287\n",
      "Generation = 72\n",
      "Fitness    = -3.443803310494287\n",
      "Generation = 73\n",
      "Fitness    = -3.443803310494287\n",
      "Generation = 74\n",
      "Fitness    = -3.443803310494287\n",
      "Generation = 75\n",
      "Fitness    = -3.443803310494287\n",
      "Generation = 76\n",
      "Fitness    = -3.443803310494287\n",
      "Generation = 77\n",
      "Fitness    = -3.443803310494287\n",
      "Generation = 78\n",
      "Fitness    = -3.441615104775293\n",
      "Generation = 79\n",
      "Fitness    = -3.441615104775293\n",
      "Generation = 80\n",
      "Fitness    = -3.441615104775293\n",
      "Generation = 81\n",
      "Fitness    = -3.441615104775293\n",
      "Generation = 82\n",
      "Fitness    = -3.441615104775293\n",
      "Generation = 83\n",
      "Fitness    = -3.37034010897146\n",
      "Generation = 84\n",
      "Fitness    = -3.3633315564201904\n",
      "Generation = 85\n",
      "Fitness    = -3.3633315564201904\n",
      "Generation = 86\n",
      "Fitness    = -3.3633315564201904\n",
      "Generation = 87\n",
      "Fitness    = -3.3633315564201904\n",
      "Generation = 88\n",
      "Fitness    = -3.3633315564201904\n",
      "Generation = 89\n",
      "Fitness    = -3.2888314724968506\n",
      "Generation = 90\n",
      "Fitness    = -3.2888314724968506\n",
      "Generation = 91\n",
      "Fitness    = -3.2888314724968506\n",
      "Generation = 92\n",
      "Fitness    = -3.2888314724968506\n",
      "Generation = 93\n",
      "Fitness    = -3.2888314724968506\n",
      "Generation = 94\n",
      "Fitness    = -3.2888314724968506\n",
      "Generation = 95\n",
      "Fitness    = -3.2888314724968506\n",
      "Generation = 96\n",
      "Fitness    = -3.279942035775049\n",
      "Generation = 97\n",
      "Fitness    = -3.2614939213799072\n",
      "Generation = 98\n",
      "Fitness    = -3.2614939213799072\n",
      "Generation = 99\n",
      "Fitness    = -3.2614939213799072\n",
      "Generation = 100\n",
      "Fitness    = -3.2523131371544434\n",
      "Generation = 101\n",
      "Fitness    = -3.2523131371544434\n",
      "Generation = 102\n",
      "Fitness    = -3.2523131371544434\n",
      "Generation = 103\n",
      "Fitness    = -3.2523131371544434\n",
      "Generation = 104\n",
      "Fitness    = -3.2523131371544434\n",
      "Generation = 105\n",
      "Fitness    = -3.2523131371544434\n",
      "Generation = 106\n",
      "Fitness    = -3.2523131371544434\n",
      "Generation = 107\n",
      "Fitness    = -3.2523131371544434\n",
      "Generation = 108\n",
      "Fitness    = -3.241988182167871\n",
      "Generation = 109\n",
      "Fitness    = -3.241988182167871\n",
      "Generation = 110\n",
      "Fitness    = -3.241988182167871\n",
      "Generation = 111\n",
      "Fitness    = -3.241988182167871\n",
      "Generation = 112\n",
      "Fitness    = -3.241988182167871\n",
      "Generation = 113\n",
      "Fitness    = -3.241988182167871\n",
      "Generation = 114\n",
      "Fitness    = -3.241988182167871\n",
      "Generation = 115\n",
      "Fitness    = -3.1987178326653076\n",
      "Generation = 116\n",
      "Fitness    = -3.1987178326653076\n",
      "Generation = 117\n",
      "Fitness    = -3.1987178326653076\n",
      "Generation = 118\n",
      "Fitness    = -3.183941602806909\n",
      "Generation = 119\n",
      "Fitness    = -3.183941602806909\n",
      "Generation = 120\n",
      "Fitness    = -3.183941602806909\n",
      "Generation = 121\n",
      "Fitness    = -3.183941602806909\n",
      "Generation = 122\n",
      "Fitness    = -3.183941602806909\n",
      "Generation = 123\n",
      "Fitness    = -3.183941602806909\n",
      "Generation = 124\n",
      "Fitness    = -3.183941602806909\n",
      "Generation = 125\n",
      "Fitness    = -3.183941602806909\n",
      "Generation = 126\n",
      "Fitness    = -3.183941602806909\n",
      "Generation = 127\n",
      "Fitness    = -3.183941602806909\n",
      "Generation = 128\n",
      "Fitness    = -3.183941602806909\n",
      "Generation = 129\n",
      "Fitness    = -3.183941602806909\n",
      "Generation = 130\n",
      "Fitness    = -3.183941602806909\n",
      "Generation = 131\n",
      "Fitness    = -3.1664445401238037\n",
      "Generation = 132\n",
      "Fitness    = -3.1664445401238037\n",
      "Generation = 133\n",
      "Fitness    = -3.1664445401238037\n",
      "Generation = 134\n",
      "Fitness    = -3.1664445401238037\n",
      "Generation = 135\n",
      "Fitness    = -3.1664445401238037\n",
      "Generation = 136\n",
      "Fitness    = -3.1664445401238037\n",
      "Generation = 137\n",
      "Fitness    = -3.156731605629785\n",
      "Generation = 138\n",
      "Fitness    = -3.156731605629785\n",
      "Generation = 139\n",
      "Fitness    = -3.156731605629785\n",
      "Generation = 140\n",
      "Fitness    = -3.156731605629785\n",
      "Generation = 141\n",
      "Fitness    = -3.156731605629785\n",
      "Generation = 142\n",
      "Fitness    = -3.156731605629785\n",
      "Generation = 143\n",
      "Fitness    = -3.1538994313286377\n",
      "Generation = 144\n",
      "Fitness    = -3.1538994313286377\n",
      "Generation = 145\n",
      "Fitness    = -3.1538994313286377\n",
      "Generation = 146\n",
      "Fitness    = -3.1538994313286377\n",
      "Generation = 147\n",
      "Fitness    = -3.1538994313286377\n",
      "Generation = 148\n",
      "Fitness    = -3.1538994313286377\n",
      "Generation = 149\n",
      "Fitness    = -3.1515161992119385\n",
      "Generation = 150\n",
      "Fitness    = -3.1515161992119385\n",
      "Generation = 151\n",
      "Fitness    = -3.1515161992119385\n",
      "Generation = 152\n",
      "Fitness    = -3.1515161992119385\n",
      "Generation = 153\n",
      "Fitness    = -3.1515161992119385\n",
      "Generation = 154\n",
      "Fitness    = -3.1515161992119385\n",
      "Generation = 155\n",
      "Fitness    = -3.1504499913261963\n",
      "Generation = 156\n",
      "Fitness    = -3.1416354180382324\n",
      "Generation = 157\n",
      "Fitness    = -3.1416354180382324\n",
      "Generation = 158\n",
      "Fitness    = -3.1416354180382324\n",
      "Generation = 159\n",
      "Fitness    = -3.1416354180382324\n",
      "Generation = 160\n",
      "Fitness    = -3.1416354180382324\n",
      "Generation = 161\n",
      "Fitness    = -3.1416354180382324\n",
      "Generation = 162\n",
      "Fitness    = -3.1416354180382324\n",
      "Generation = 163\n",
      "Fitness    = -3.1402065754936768\n",
      "Generation = 164\n",
      "Fitness    = -3.1402065754936768\n",
      "Generation = 165\n",
      "Fitness    = -3.1402065754936768\n",
      "Generation = 166\n",
      "Fitness    = -3.1402065754936768\n",
      "Generation = 167\n",
      "Fitness    = -3.1402065754936768\n",
      "Generation = 168\n",
      "Fitness    = -3.1402065754936768\n",
      "Generation = 169\n",
      "Fitness    = -3.1402065754936768\n",
      "Generation = 170\n",
      "Fitness    = -3.1402065754936768\n",
      "Generation = 171\n",
      "Fitness    = -3.1402065754936768\n",
      "Generation = 172\n",
      "Fitness    = -3.139338493447168\n",
      "Generation = 173\n",
      "Fitness    = -3.139338493447168\n",
      "Generation = 174\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 175\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 176\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitness    = -3.1385977269218994\n",
      "Generation = 178\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 179\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 180\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 181\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 182\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 183\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 184\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 185\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 186\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 187\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 188\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 189\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 190\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 191\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 192\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 193\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 194\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 195\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 196\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 197\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 198\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 199\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 200\n",
      "Fitness    = -3.1385977269218994\n",
      "Generation = 201\n",
      "Fitness    = -3.1385014058159424\n",
      "Generation = 202\n",
      "Fitness    = -3.1385014058159424\n",
      "Generation = 203\n",
      "Fitness    = -3.1385014058159424\n",
      "Generation = 204\n",
      "Fitness    = -3.1385014058159424\n",
      "Generation = 205\n",
      "Fitness    = -3.1385014058159424\n",
      "Generation = 206\n",
      "Fitness    = -3.1385014058159424\n",
      "Generation = 207\n",
      "Fitness    = -3.1385014058159424\n",
      "Generation = 208\n",
      "Fitness    = -3.1385014058159424\n",
      "Generation = 209\n",
      "Fitness    = -3.1385014058159424\n",
      "Generation = 210\n",
      "Fitness    = -3.1385014058159424\n",
      "Generation = 211\n",
      "Fitness    = -3.1385014058159424\n",
      "Generation = 212\n",
      "Fitness    = -3.1385014058159424\n",
      "Generation = 213\n",
      "Fitness    = -3.1385014058159424\n",
      "Generation = 214\n",
      "Fitness    = -3.1385014058159424\n",
      "Generation = 215\n",
      "Fitness    = -3.1385014058159424\n",
      "Generation = 216\n",
      "Fitness    = -3.1385014058159424\n",
      "Generation = 217\n",
      "Fitness    = -3.1385014058159424\n",
      "Generation = 218\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 219\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 220\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 221\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 222\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 223\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 224\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 225\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 226\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 227\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 228\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 229\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 230\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 231\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 232\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 233\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 234\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 235\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 236\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 237\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 238\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 239\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 240\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 241\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 242\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 243\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 244\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 245\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 246\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 247\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 248\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 249\n",
      "Fitness    = -3.138479709725244\n",
      "Generation = 250\n",
      "Fitness    = -3.138479709725244\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitness value of the best solution = -3.138479709725244\n",
      "Index of the best solution : 0\n",
      "Predictions : n [[9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]\n",
      " [9.690201]]\n",
      "rmse :  9.850055\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pygad\n",
    "import pygad.torchga as torchga\n",
    "\n",
    "\n",
    "def fitness_func(solution, sol_idx):\n",
    "    global data_inputs, data_outputs, torch_ga, model, loss_function\n",
    "\n",
    "    model_weights_dict = torchga.model_weights_as_dict(model=model,\n",
    "                                                       weights_vector=solution)\n",
    "\n",
    "    # Use the current solution as the model parameters.\n",
    "    model.load_state_dict(model_weights_dict)\n",
    "\n",
    "    predictions = model(data_inputs)\n",
    "    mse = loss_function(predictions, data_outputs)\n",
    "    rmse = torch.sqrt(mse).detach().numpy() + 1e-10  # Adding a small value to avoid division by zero\n",
    "\n",
    "    # Use RMSE as the fitness directly (minimize RMSE)\n",
    "    solution_fitness = -rmse\n",
    "\n",
    "    return solution_fitness\n",
    "\n",
    "\n",
    "def callback_generation(ga_instance):\n",
    "    print(\"Generation = {generation}\".format(generation=ga_instance.generations_completed))\n",
    "    print(\"Fitness    = {fitness}\".format(fitness=ga_instance.best_solution()[1]))\n",
    "\n",
    "# Create the PyTorch model.\n",
    "input_layer = torch.nn.Linear(5, 2)\n",
    "relu_layer = torch.nn.ReLU()\n",
    "output_layer = torch.nn.Linear(2, 1)\n",
    "\n",
    "model = torch.nn.Sequential(input_layer,\n",
    "                            relu_layer,\n",
    "                            output_layer)\n",
    "print(model)\n",
    "weights = model.state_dict()\n",
    "\n",
    "# Create an instance of the pygad.torchga.TorchGA class to build the initial population.\n",
    "torch_ga = pygad.torchga.TorchGA(model=model,\n",
    "                                num_solutions=8)\n",
    "\n",
    "loss_function = torch.nn.L1Loss()\n",
    "\n",
    "# Data inputs\n",
    "data_inputs = torch.tensor(a)\n",
    "\n",
    "# Data outputs\n",
    "data_outputs = torch.tensor(b)\n",
    "# Prepare the PyGAD parameters. Check the documentation for more information: https://pygad.readthedocs.io/en/latest/README_pygad_ReadTheDocs.html#pygad-ga-class\n",
    "num_generations = 250 # Number of generations.\n",
    "num_parents_mating = 5 # Number of solutions to be selected as parents in the mating pool.\n",
    "initial_population = torch_ga.population_weights # Initial population of network weights\n",
    "parent_selection_type = \"sss\" # Type of parent selection.\n",
    "crossover_type = \"single_point\" # Type of the crossover operator.\n",
    "mutation_type = \"random\" # Type of theutation operator.\n",
    "mutation_percent_genes = 20 # Percentage of genes to mutate. This parameter has no action if the parameter mutation_num_genes exists.\n",
    "keep_parents = -1 # Number of parents to keep in the next population. -1 means keep all parents and 0 means keep nothing.\n",
    "\n",
    "ga_instance = pygad.GA(num_generations=num_generations,\n",
    "                       num_parents_mating=num_parents_mating,\n",
    "                       initial_population=initial_population,\n",
    "                       fitness_func=fitness_func,\n",
    "                       parent_selection_type=parent_selection_type,\n",
    "                       crossover_type=crossover_type,\n",
    "                       mutation_type=mutation_type,\n",
    "                       mutation_percent_genes=mutation_percent_genes,\n",
    "                       keep_parents=keep_parents,\n",
    "                       on_generation=callback_generation)\n",
    "\n",
    "ga_instance.run()\n",
    "\n",
    "# After the generations complete, some plots are showed that summarize how the outputs/fitness values evolve over generations.\n",
    "ga_instance.plot_result(title=\"PyGAD & PyTorch - Iteration vs. Fitness\", linewidth=4)\n",
    "\n",
    "# Returning the details of the best solution.\n",
    "solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "print(\"Fitness value of the best solution = {solution_fitness}\".format(solution_fitness=solution_fitness))\n",
    "print(\"Index of the best solution : {solution_idx}\".format(solution_idx=solution_idx))\n",
    "\n",
    "# Fetch the parameters of the best solution.\n",
    "best_solution_weights = torchga.model_weights_as_dict(model=model,\n",
    "                                                      weights_vector=solution)\n",
    "model.load_state_dict(best_solution_weights)\n",
    "predictions = model(data_inputs)\n",
    "print(\"Predictions : n\", predictions.detach().numpy())\n",
    "\n",
    "abs_error = loss_function(predictions, data_outputs)\n",
    "print(\"rmse : \", abs_error.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41ade8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.5427, -3.1640, -3.1179, -1.3226, -1.5888],\n",
       "                      [-1.3795,  2.4841, -4.4485, -3.0042,  0.8054]])),\n",
       "             ('0.bias', tensor([ 3.2967, -1.3168])),\n",
       "             ('2.weight', tensor([[ 0.2494, -2.6753]])),\n",
       "             ('2.bias', tensor([9.6902]))])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79e54c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.0286,  1.2038, -0.0541, -0.1058, -0.5733],\n",
       "                      [-1.9636,  2.7938, -1.3331, -0.0527,  1.4355]])),\n",
       "             ('0.bias', tensor([ 0.3224, -1.3254])),\n",
       "             ('2.weight', tensor([[0.5459, 0.9219]])),\n",
       "             ('2.bias', tensor([9.5640]))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "993200e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(best_solution_weights,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3262818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(OrderedDict([('0.weight', tensor([[-0.0286,  1.2038, -0.0541, -0.1058, -0.5733],\n",
       "        [-1.9636,  2.7938, -1.3331, -0.0527,  1.4355]])), ('0.bias', tensor([ 0.3224, -1.3254])), ('2.weight', tensor([[0.5459, 0.9219]])), ('2.bias', tensor([9.5640]))]),\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e0afaa60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      15.308889\n",
       "1      18.406316\n",
       "2      15.856111\n",
       "3      25.090526\n",
       "4      17.786250\n",
       "         ...    \n",
       "273     2.400000\n",
       "274     0.500000\n",
       "275     1.992500\n",
       "276     0.335000\n",
       "277     2.426667\n",
       "Name: Yield, Length: 278, dtype: float64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b1e7944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.31620538, -0.29258573,  0.20236933, -0.31960332,  0.0130693 ,\n",
       "         -0.31227386, -0.37106612, -0.05511627,  0.18388897, -0.37293535,\n",
       "         -0.37991986, -0.18592969,  0.05207247, -0.31794697,  0.1247707 ,\n",
       "          0.33410126,  0.28934014,  0.04623881,  0.3704105 , -0.00588068,\n",
       "          0.08182499, -0.01254711,  0.3274693 , -0.2193876 ,  0.12116849,\n",
       "          0.22033137,  0.30973947,  0.04411104,  0.24510342,  0.01483589,\n",
       "          0.18622607,  0.03599516],\n",
       "        [-0.38121703,  0.39423037,  0.05928481,  0.16629821, -0.2549731 ,\n",
       "         -0.11640498, -0.3627325 , -0.31724486, -0.1255494 ,  0.04783872,\n",
       "          0.02364495, -0.35826114,  0.3213637 ,  0.31709337,  0.01663837,\n",
       "          0.11481696,  0.06943101, -0.3092922 ,  0.24337995, -0.04715112,\n",
       "         -0.17543669, -0.29290035,  0.2598192 ,  0.04896906, -0.35847446,\n",
       "         -0.07153323, -0.08269483,  0.3298669 ,  0.20276076, -0.15930957,\n",
       "         -0.01962811, -0.05599666],\n",
       "        [-0.30972835,  0.29775828, -0.19779512, -0.22133583, -0.38531637,\n",
       "          0.07906374, -0.28802797,  0.33225358, -0.03608963, -0.23124364,\n",
       "         -0.12568307,  0.21000469, -0.20941988,  0.15214568, -0.08081776,\n",
       "         -0.18262137,  0.09245855,  0.27270442, -0.10940385,  0.27738482,\n",
       "          0.34038627,  0.1764862 ,  0.04212999,  0.22406596,  0.1898483 ,\n",
       "          0.01728633,  0.3209175 , -0.31095988, -0.06258753, -0.36545053,\n",
       "          0.0687938 , -0.06197894],\n",
       "        [ 0.27588516,  0.16570747,  0.06444129, -0.30929372, -0.2373753 ,\n",
       "          0.09253266, -0.25683916,  0.38243836, -0.13488156,  0.06905973,\n",
       "          0.27596927,  0.32590723,  0.09215024,  0.33377147, -0.12329078,\n",
       "          0.02333841,  0.25926942, -0.37291354,  0.13830072, -0.164335  ,\n",
       "         -0.14403108, -0.3027136 ,  0.24007732,  0.03544119, -0.39460328,\n",
       "          0.12318718, -0.04691023, -0.16625279,  0.13834625, -0.3968332 ,\n",
       "         -0.33544606,  0.02924013],\n",
       "        [ 0.31326324, -0.21918637, -0.32545224,  0.10124004,  0.344943  ,\n",
       "         -0.29841697, -0.19552854,  0.29179722,  0.3255968 ,  0.26905388,\n",
       "         -0.35125655,  0.31302655,  0.1151067 ,  0.09319213, -0.2859747 ,\n",
       "          0.35108578,  0.34600705, -0.38185877, -0.3351281 , -0.21815541,\n",
       "          0.23130155, -0.07542238, -0.2138586 , -0.24104719,  0.13227218,\n",
       "         -0.31450358, -0.39143065,  0.1454016 ,  0.21305901, -0.16472979,\n",
       "          0.26744342,  0.09625092]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " array([[ 0.10978574],\n",
       "        [ 0.24369383],\n",
       "        [ 0.18123388],\n",
       "        [-0.1453268 ],\n",
       "        [ 0.3590147 ],\n",
       "        [ 0.2673452 ],\n",
       "        [ 0.24333668],\n",
       "        [-0.01542631],\n",
       "        [ 0.30591637],\n",
       "        [-0.37784734],\n",
       "        [-0.10286987],\n",
       "        [ 0.35666656],\n",
       "        [ 0.22843856],\n",
       "        [ 0.06716809],\n",
       "        [-0.351645  ],\n",
       "        [-0.13229981],\n",
       "        [ 0.1698609 ],\n",
       "        [ 0.09157515],\n",
       "        [-0.2653222 ],\n",
       "        [-0.00239271],\n",
       "        [-0.04525077],\n",
       "        [-0.29505765],\n",
       "        [-0.01002946],\n",
       "        [ 0.3899806 ],\n",
       "        [-0.1477994 ],\n",
       "        [ 0.20704985],\n",
       "        [ 0.12832785],\n",
       "        [ 0.07806873],\n",
       "        [ 0.01970261],\n",
       "        [ 0.18682218],\n",
       "        [-0.14726833],\n",
       "        [-0.232957  ]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7516fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat_to_vector(mat_pop_weights):\n",
    "    pop_weights_vector = []\n",
    "    for sol_idx in range(len(mat_pop_weights)):\n",
    "        for layer_weights in mat_pop_weights[sol_idx]:\n",
    "            pop_weights_vector.extend(layer_weights.flatten())\n",
    "    return np.array(pop_weights_vector)\n",
    "\n",
    "def vector_to_mat(vector_pop_weights, mat_pop_weights):\n",
    "    mat_weights = []\n",
    "    start = 0\n",
    "    for layer_weights in mat_pop_weights:\n",
    "        end = start + layer_weights.size\n",
    "        mat_layer_weights = np.reshape(vector_pop_weights[start:end], layer_weights.shape)\n",
    "        mat_weights.append(mat_layer_weights)\n",
    "        start = end\n",
    "    return mat_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee845b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mat_to_vector(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4df8acd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation = 1\n",
      "Fitness    = 0.23747516853911924\n",
      "Generation = 2\n",
      "Fitness    = 0.23747516853911924\n",
      "Generation = 3\n",
      "Fitness    = 0.237541607822672\n",
      "Generation = 4\n",
      "Fitness    = 0.26817443859362483\n",
      "Generation = 5\n",
      "Fitness    = 0.26817443859362483\n",
      "Generation = 6\n",
      "Fitness    = 0.26817443859362483\n",
      "Generation = 7\n",
      "Fitness    = 0.26817443859362483\n",
      "Generation = 8\n",
      "Fitness    = 0.26817443859362483\n",
      "Generation = 9\n",
      "Fitness    = 0.26817443859362483\n",
      "Generation = 10\n",
      "Fitness    = 0.26817443859362483\n",
      "Generation = 11\n",
      "Fitness    = 0.26817443859362483\n",
      "Generation = 12\n",
      "Fitness    = 0.26817443859362483\n",
      "Generation = 13\n",
      "Fitness    = 0.26817443859362483\n",
      "Generation = 14\n",
      "Fitness    = 0.2719023016985308\n",
      "Generation = 15\n",
      "Fitness    = 0.2753515647773086\n",
      "Generation = 16\n",
      "Fitness    = 0.27642426770402784\n",
      "Generation = 17\n",
      "Fitness    = 0.27642426770402784\n",
      "Generation = 18\n",
      "Fitness    = 0.27642426770402784\n",
      "Generation = 19\n",
      "Fitness    = 0.27642426770402784\n",
      "Generation = 20\n",
      "Fitness    = 0.2780945657128391\n",
      "Generation = 21\n",
      "Fitness    = 0.2780945657128391\n",
      "Generation = 22\n",
      "Fitness    = 0.2780945657128391\n",
      "Generation = 23\n",
      "Fitness    = 0.2780945657128391\n",
      "Generation = 24\n",
      "Fitness    = 0.2780945657128391\n",
      "Generation = 25\n",
      "Fitness    = 0.2780945657128391\n",
      "Generation = 26\n",
      "Fitness    = 0.2780945657128391\n",
      "Generation = 27\n",
      "Fitness    = 0.27815782408299006\n",
      "Generation = 28\n",
      "Fitness    = 0.27815782408299006\n",
      "Generation = 29\n",
      "Fitness    = 0.27815782408299006\n",
      "Generation = 30\n",
      "Fitness    = 0.27815782408299006\n",
      "Generation = 31\n",
      "Fitness    = 0.28473210001454613\n",
      "Generation = 32\n",
      "Fitness    = 0.28473210001454613\n",
      "Generation = 33\n",
      "Fitness    = 0.28473210001454613\n",
      "Generation = 34\n",
      "Fitness    = 0.28473210001454613\n",
      "Generation = 35\n",
      "Fitness    = 0.28473210001454613\n",
      "Generation = 36\n",
      "Fitness    = 0.2861571255723906\n",
      "Generation = 37\n",
      "Fitness    = 0.2861571255723906\n",
      "Generation = 38\n",
      "Fitness    = 0.2861571255723906\n",
      "Generation = 39\n",
      "Fitness    = 0.2861571255723906\n",
      "Generation = 40\n",
      "Fitness    = 0.2861571255723906\n",
      "Generation = 41\n",
      "Fitness    = 0.2861571255723906\n",
      "Generation = 42\n",
      "Fitness    = 0.2861571255723906\n",
      "Generation = 43\n",
      "Fitness    = 0.2861571255723906\n",
      "Generation = 44\n",
      "Fitness    = 0.2861571255723906\n",
      "Generation = 45\n",
      "Fitness    = 0.2861571255723906\n",
      "Generation = 46\n",
      "Fitness    = 0.28752044680752764\n",
      "Generation = 47\n",
      "Fitness    = 0.28752044680752764\n",
      "Generation = 48\n",
      "Fitness    = 0.28752044680752764\n",
      "Generation = 49\n",
      "Fitness    = 0.29519663181240097\n",
      "Generation = 50\n",
      "Fitness    = 0.29519663181240097\n",
      "Generation = 51\n",
      "Fitness    = 0.29519663181240097\n",
      "Generation = 52\n",
      "Fitness    = 0.2992467433679618\n",
      "Generation = 53\n",
      "Fitness    = 0.2992467433679618\n",
      "Generation = 54\n",
      "Fitness    = 0.2992467433679618\n",
      "Generation = 55\n",
      "Fitness    = 0.2992467433679618\n",
      "Generation = 56\n",
      "Fitness    = 0.2992467433679618\n",
      "Generation = 57\n",
      "Fitness    = 0.2992467433679618\n",
      "Generation = 58\n",
      "Fitness    = 0.2992467433679618\n",
      "Generation = 59\n",
      "Fitness    = 0.2992467433679618\n",
      "Generation = 60\n",
      "Fitness    = 0.2992467433679618\n",
      "Generation = 61\n",
      "Fitness    = 0.2992467433679618\n",
      "Generation = 62\n",
      "Fitness    = 0.2992467433679618\n",
      "Generation = 63\n",
      "Fitness    = 0.30013606755606936\n",
      "Generation = 64\n",
      "Fitness    = 0.30013606755606936\n",
      "Generation = 65\n",
      "Fitness    = 0.30013606755606936\n",
      "Generation = 66\n",
      "Fitness    = 0.3042716454014081\n",
      "Generation = 67\n",
      "Fitness    = 0.30624752979848857\n",
      "Generation = 68\n",
      "Fitness    = 0.30624752979848857\n",
      "Generation = 69\n",
      "Fitness    = 0.30624752979848857\n",
      "Generation = 70\n",
      "Fitness    = 0.30624752979848857\n",
      "Generation = 71\n",
      "Fitness    = 0.30983802569253416\n",
      "Generation = 72\n",
      "Fitness    = 0.30983802569253416\n",
      "Generation = 73\n",
      "Fitness    = 0.30983802569253416\n",
      "Generation = 74\n",
      "Fitness    = 0.30983802569253416\n",
      "Generation = 75\n",
      "Fitness    = 0.3135346866426632\n",
      "Generation = 76\n",
      "Fitness    = 0.3135346866426632\n",
      "Generation = 77\n",
      "Fitness    = 0.3135346866426632\n",
      "Generation = 78\n",
      "Fitness    = 0.31630834433688326\n",
      "Generation = 79\n",
      "Fitness    = 0.31630834433688326\n",
      "Generation = 80\n",
      "Fitness    = 0.31630834433688326\n",
      "Generation = 81\n",
      "Fitness    = 0.31630834433688326\n",
      "Generation = 82\n",
      "Fitness    = 0.31630834433688326\n",
      "Generation = 83\n",
      "Fitness    = 0.31659274879032906\n",
      "Generation = 84\n",
      "Fitness    = 0.31659274879032906\n",
      "Generation = 85\n",
      "Fitness    = 0.31659274879032906\n",
      "Generation = 86\n",
      "Fitness    = 0.3178162739740445\n",
      "Generation = 87\n",
      "Fitness    = 0.31847983899683086\n",
      "Generation = 88\n",
      "Fitness    = 0.31847983899683086\n",
      "Generation = 89\n",
      "Fitness    = 0.31847983899683086\n",
      "Generation = 90\n",
      "Fitness    = 0.31847983899683086\n",
      "Generation = 91\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 92\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 93\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 94\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 95\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 96\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 97\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 98\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 99\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 100\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 101\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 102\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 103\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 104\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 105\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 106\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 107\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 108\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 109\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 110\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 111\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 112\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 113\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 114\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 115\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 116\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 117\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 118\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 119\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 120\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 121\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 122\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 123\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 124\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 125\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 126\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 127\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 128\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 129\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 130\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 131\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 132\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 133\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 134\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 135\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 136\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 137\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 138\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 139\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 140\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 141\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 142\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 143\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 144\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 145\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 146\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 147\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 148\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 149\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 150\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 151\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 152\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 153\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 154\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 155\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 156\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 157\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 158\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 159\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 160\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 161\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 162\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 163\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 164\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 165\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 166\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 167\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 168\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 169\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 170\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 171\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 172\n",
      "Fitness    = 0.31861345583690087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation = 173\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 174\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 175\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 176\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 177\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 178\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 179\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 180\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 181\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 182\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 183\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 184\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 185\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 186\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 187\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 188\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 189\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 190\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 191\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 192\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 193\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 194\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 195\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 196\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 197\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 198\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 199\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 200\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 201\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 202\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 203\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 204\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 205\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 206\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 207\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 208\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 209\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 210\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 211\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 212\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 213\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 214\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 215\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 216\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 217\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 218\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 219\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 220\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 221\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 222\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 223\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 224\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 225\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 226\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 227\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 228\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 229\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 230\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 231\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 232\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 233\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 234\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 235\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 236\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 237\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 238\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 239\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 240\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 241\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 242\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 243\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 244\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 245\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 246\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 247\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 248\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 249\n",
      "Fitness    = 0.31861345583690087\n",
      "Generation = 250\n",
      "Fitness    = 0.31861345583690087\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitness value of the best solution = 0.31861345583690087\n",
      "Index of the best solution : 0\n",
      "Predictions : n [[9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]\n",
      " [9.754395]]\n",
      "Rmse :  9.850805\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pygad\n",
    "import pygad.torchga as torchga\n",
    "\n",
    "\n",
    "def fitness_func(solution, sol_idx):\n",
    "    global data_inputs, data_outputs, torch_ga, model, loss_function\n",
    "\n",
    "    model_weights_dict = torchga.model_weights_as_dict(model=model, weights_vector=solution)\n",
    "\n",
    "    # Use the current solution as the model parameters.\n",
    "    model.load_state_dict(model_weights_dict)\n",
    "\n",
    "    predictions = model(data_inputs)\n",
    "    \n",
    "    # Calculate the RMSE\n",
    "    mse = loss_function(predictions, data_outputs)\n",
    "    rmse = torch.sqrt(mse).detach().numpy() + 1e-10  # Adding a small value to avoid division by zero\n",
    "\n",
    "    # Use the inverse of RMSE as the fitness\n",
    "    solution_fitness = 1.0 / rmse\n",
    "\n",
    "    return solution_fitness\n",
    "\n",
    "def callback_generation(ga_instance):\n",
    "    print(\"Generation = {generation}\".format(generation=ga_instance.generations_completed))\n",
    "    print(\"Fitness    = {fitness}\".format(fitness=ga_instance.best_solution()[1]))\n",
    "\n",
    "# Create the PyTorch model.\n",
    "input_layer = torch.nn.Linear(5, 2)\n",
    "relu_layer = torch.nn.ReLU()\n",
    "output_layer = torch.nn.Linear(2, 1)\n",
    "\n",
    "model = torch.nn.Sequential(input_layer,\n",
    "                            relu_layer,\n",
    "                            output_layer)\n",
    "# print(model)\n",
    "\n",
    "# Create an instance of the pygad.torchga.TorchGA class to build the initial population.\n",
    "torch_ga = pygad.torchga.TorchGA(model=model,\n",
    "                                num_solutions=10)\n",
    "\n",
    "loss_function = torch.nn.L1Loss()\n",
    "\n",
    "# Data inputs\n",
    "data_inputs = torch.tensor(a)\n",
    "\n",
    "# Data outputs\n",
    "data_outputs = torch.tensor(b)\n",
    "# Prepare the PyGAD parameters. Check the documentation for more information: https://pygad.readthedocs.io/en/latest/README_pygad_ReadTheDocs.html#pygad-ga-class\n",
    "num_generations = 250 # Number of generations.\n",
    "num_parents_mating = 5 # Number of solutions to be selected as parents in the mating pool.\n",
    "initial_population = torch_ga.population_weights # Initial population of network weights\n",
    "parent_selection_type = \"sss\" # Type of parent selection.\n",
    "crossover_type = \"single_point\" # Type of the crossover operator.\n",
    "mutation_type = \"random\" # Type of the mutation operator.\n",
    "mutation_percent_genes = 10 # Percentage of genes to mutate. This parameter has no action if the parameter mutation_num_genes exists.\n",
    "keep_parents = -1 # Number of parents to keep in the next population. -1 means keep all parents and 0 means keep nothing.\n",
    "\n",
    "ga_instance = pygad.GA(num_generations=num_generations,\n",
    "                       num_parents_mating=num_parents_mating,\n",
    "                       initial_population=initial_population,\n",
    "                       fitness_func=fitness_func,\n",
    "                       parent_selection_type=parent_selection_type,\n",
    "                       crossover_type=crossover_type,\n",
    "                       mutation_type=mutation_type,\n",
    "                       mutation_percent_genes=mutation_percent_genes,\n",
    "                       keep_parents=keep_parents,\n",
    "                       on_generation=callback_generation)\n",
    "\n",
    "ga_instance.run()\n",
    "\n",
    "# After the generations complete, some plots are showed that summarize how the outputs/fitness values evolve over generations.\n",
    "ga_instance.plot_result(title=\"PyGAD & PyTorch - Iteration vs. Fitness\", linewidth=4)\n",
    "\n",
    "# Returning the details of the best solution.\n",
    "solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "print(\"Fitness value of the best solution = {solution_fitness}\".format(solution_fitness=solution_fitness))\n",
    "print(\"Index of the best solution : {solution_idx}\".format(solution_idx=solution_idx))\n",
    "\n",
    "# Fetch the parameters of the best solution.\n",
    "best_solution_weights = torchga.model_weights_as_dict(model=model,\n",
    "                                                      weights_vector=solution)\n",
    "model.load_state_dict(best_solution_weights)\n",
    "predictions = model(data_inputs)\n",
    "print(\"Predictions : n\", predictions.detach().numpy())\n",
    "\n",
    "abs_error = loss_function(predictions, data_outputs)\n",
    "print(\"Rmse : \", abs_error.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d2bd42e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0af30c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model(torch.tensor(testingvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "65eebff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 5), dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testingvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "91f2cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tval = a[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d8c99af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.7200001e+01, 6.7000000e+01, 9.4459998e+02, 3.8224580e+06,\n",
       "        2.4050701e+03],\n",
       "       [2.7200001e+01, 6.7000000e+01, 9.4459998e+02, 1.9649405e+06,\n",
       "        1.2363300e+03],\n",
       "       [2.6500000e+01, 6.4000000e+01, 7.1100000e+02, 3.5766922e+06,\n",
       "        3.9021799e+03],\n",
       "       [2.6500000e+01, 6.4000000e+01, 7.1100000e+02, 2.1203985e+06,\n",
       "        2.3133601e+03],\n",
       "       [2.7799999e+01, 6.9000000e+01, 1.2970000e+03, 4.2544095e+06,\n",
       "        6.1468799e+03],\n",
       "       [2.7799999e+01, 6.9000000e+01, 1.2970000e+03, 2.9545985e+06,\n",
       "        4.2688799e+03],\n",
       "       [2.6900000e+01, 6.6000000e+01, 8.6190002e+02, 6.6359700e+06,\n",
       "        1.3072290e+04],\n",
       "       [2.6900000e+01, 6.6000000e+01, 8.6190002e+02, 2.9111625e+06,\n",
       "        5.7347402e+03],\n",
       "       [2.7500000e+01, 6.8000000e+01, 9.6870001e+02, 3.7700000e+06,\n",
       "        7.7500000e+03],\n",
       "       [2.7500000e+01, 6.8000000e+01, 9.6870001e+02, 2.1112000e+06,\n",
       "        4.3400000e+03],\n",
       "       [2.7000000e+01, 6.5000000e+01, 1.0623000e+03, 3.0283660e+06,\n",
       "        5.6589302e+03],\n",
       "       [2.7000000e+01, 6.5000000e+01, 1.0623000e+03, 7.6955375e+05,\n",
       "        1.4380200e+03],\n",
       "       [2.8100000e+01, 7.0000000e+01, 6.8759998e+02, 4.4450170e+06,\n",
       "        9.7168496e+03],\n",
       "       [2.8100000e+01, 7.0000000e+01, 6.8759998e+02, 6.0127369e+05,\n",
       "        1.3143900e+03],\n",
       "       [2.6700001e+01, 6.3000000e+01, 9.4070001e+02, 5.3479380e+06,\n",
       "        1.1176110e+04],\n",
       "       [2.6700001e+01, 6.3000000e+01, 9.4070001e+02, 1.1645862e+06,\n",
       "        2.4337500e+03],\n",
       "       [2.7900000e+01, 6.8000000e+01, 8.9000000e+02, 4.9049195e+06,\n",
       "        1.1202100e+04],\n",
       "       [2.7900000e+01, 6.8000000e+01, 8.9000000e+02, 5.3806075e+05,\n",
       "        1.2288500e+03],\n",
       "       [2.6799999e+01, 6.4000000e+01, 8.6820001e+02, 2.7293798e+06,\n",
       "        6.5876802e+03],\n",
       "       [2.6799999e+01, 6.4000000e+01, 8.6820001e+02, 1.1622221e+06,\n",
       "        2.8051599e+03],\n",
       "       [2.8200001e+01, 7.1000000e+01, 1.2205000e+03, 4.0144500e+06,\n",
       "        8.6625000e+03],\n",
       "       [2.8200001e+01, 7.1000000e+01, 1.2205000e+03, 8.1440619e+05,\n",
       "        1.7573500e+03],\n",
       "       [2.7100000e+01, 6.6000000e+01, 8.9920001e+02, 4.0988808e+06,\n",
       "        8.8296797e+03],\n",
       "       [2.7100000e+01, 6.6000000e+01, 8.9920001e+02, 2.1040600e+06,\n",
       "        4.5325000e+03],\n",
       "       [2.7500000e+01, 7.2000000e+01, 1.2019000e+03, 4.3276750e+06,\n",
       "        2.7229500e+03],\n",
       "       [2.6799999e+01, 6.9000000e+01, 9.2850000e+02, 4.8341595e+06,\n",
       "        5.2740801e+03],\n",
       "       [2.7200001e+01, 7.1000000e+01, 9.2650000e+02, 5.8486260e+06,\n",
       "        1.1521290e+04],\n",
       "       [2.6900000e+01, 6.8000000e+01, 7.4090002e+02, 1.3154370e+06,\n",
       "        2.4580801e+03],\n",
       "       [2.6900000e+01, 6.8000000e+01, 7.4090002e+02, 2.2949348e+06,\n",
       "        4.2884102e+03],\n",
       "       [2.8200001e+01, 7.5000000e+01, 9.1129999e+02, 1.7866116e+06,\n",
       "        3.9055500e+03],\n",
       "       [2.8200001e+01, 7.5000000e+01, 9.1129999e+02, 2.4561192e+06,\n",
       "        5.3691001e+03],\n",
       "       [2.6500000e+01, 6.7000000e+01, 1.2019000e+03, 2.5286128e+06,\n",
       "        5.2842900e+03],\n",
       "       [2.6500000e+01, 6.7000000e+01, 1.2019000e+03, 3.1259862e+06,\n",
       "        6.5326802e+03],\n",
       "       [2.8000000e+01, 7.2000000e+01, 9.1240002e+02, 2.0845065e+06,\n",
       "        4.7607002e+03],\n",
       "       [2.8000000e+01, 7.2000000e+01, 9.1240002e+02, 2.6967402e+06,\n",
       "        6.1589502e+03],\n",
       "       [2.7299999e+01, 7.0000000e+01, 9.7090002e+02, 9.9076994e+05,\n",
       "        2.3913401e+03],\n",
       "       [2.7299999e+01, 7.0000000e+01, 9.7090002e+02, 2.6005940e+06,\n",
       "        6.2768398e+03],\n",
       "       [2.8400000e+01, 7.6000000e+01, 9.8620001e+02, 2.0135508e+06,\n",
       "        4.3448999e+03],\n",
       "       [2.8400000e+01, 7.6000000e+01, 9.8620001e+02, 3.1260805e+06,\n",
       "        6.7455498e+03],\n",
       "       [2.7000000e+01, 6.9000000e+01, 9.1009998e+02, 1.9128911e+06,\n",
       "        4.1206899e+03],\n",
       "       [2.7000000e+01, 6.9000000e+01, 9.1009998e+02, 3.8925970e+06,\n",
       "        8.3853096e+03],\n",
       "       [2.6299999e+01, 7.8000000e+01, 2.1876001e+03, 6.0320000e+03,\n",
       "        1.2400000e+01],\n",
       "       [2.5600000e+01, 7.4000000e+01, 3.2553999e+03, 1.4882470e+04,\n",
       "        2.7809999e+01],\n",
       "       [2.6000000e+01, 7.7000000e+01, 3.0463999e+03, 9.2085596e+03,\n",
       "        2.0129999e+01],\n",
       "       [2.5900000e+01, 7.4000000e+01, 2.6028999e+03, 4.2635698e+03,\n",
       "        8.9099998e+00],\n",
       "       [2.6500000e+01, 7.8000000e+01, 2.9243000e+03, 4.7507500e+03,\n",
       "        1.0850000e+01],\n",
       "       [2.6000000e+01, 7.6000000e+01, 2.6648000e+03, 1.4169600e+03,\n",
       "        3.4200001e+00],\n",
       "       [2.6600000e+01, 7.9000000e+01, 2.9897000e+03, 8.4506201e+02,\n",
       "        1.8235000e+00],\n",
       "       [2.7600000e+01, 6.2000000e+01, 7.4609998e+02, 7.7813760e+06,\n",
       "        4.8960000e+03],\n",
       "       [2.7200001e+01, 6.1000000e+01, 6.1800000e+02, 5.9990700e+06,\n",
       "        6.5450000e+03],\n",
       "       [2.8000000e+01, 6.3000000e+01, 1.1075000e+03, 1.1145981e+07,\n",
       "        1.6104000e+04],\n",
       "       [2.7500000e+01, 6.2000000e+01, 8.9050000e+02, 1.0654272e+07,\n",
       "        2.0988000e+04],\n",
       "       [2.7500000e+01, 6.2000000e+01, 8.9050000e+02, 1.5411840e+06,\n",
       "        3.0360000e+03],\n",
       "       [2.8100000e+01, 6.4000000e+01, 4.6060001e+02, 2.1866000e+06,\n",
       "        4.4950000e+03],\n",
       "       [2.8100000e+01, 6.4000000e+01, 4.6060001e+02, 3.3176000e+05,\n",
       "        6.8200000e+02],\n",
       "       [2.7400000e+01, 6.1000000e+01, 1.0065000e+03, 9.9247290e+06,\n",
       "        1.8545760e+04],\n",
       "       [2.7400000e+01, 6.1000000e+01, 1.0065000e+03, 1.5909794e+06,\n",
       "        2.9729700e+03],\n",
       "       [2.8299999e+01, 6.5000000e+01, 6.0559998e+02, 5.8586065e+06,\n",
       "        1.2806970e+04],\n",
       "       [2.8299999e+01, 6.5000000e+01, 6.0559998e+02, 1.2356076e+06,\n",
       "        2.7010500e+03],\n",
       "       [2.7700001e+01, 6.3000000e+01, 5.8429999e+02, 8.3510705e+06,\n",
       "        1.7452051e+04],\n",
       "       [2.7700001e+01, 6.3000000e+01, 5.8429999e+02, 1.5934698e+06,\n",
       "        3.3300300e+03],\n",
       "       [2.8500000e+01, 6.4000000e+01, 7.1050000e+02, 6.4990260e+06,\n",
       "        1.4842800e+04],\n",
       "       [2.8500000e+01, 6.4000000e+01, 7.1050000e+02, 2.0671892e+06,\n",
       "        4.7211499e+03],\n",
       "       [2.7799999e+01, 6.2000000e+01, 8.1479999e+02, 6.7176500e+06,\n",
       "        1.6213840e+04],\n",
       "       [2.7799999e+01, 6.2000000e+01, 8.1479999e+02, 1.8579495e+06,\n",
       "        4.4843799e+03],\n",
       "       [2.8700001e+01, 6.6000000e+01, 1.1254000e+03, 3.8679835e+06,\n",
       "        8.3464502e+03],\n",
       "       [2.8700001e+01, 6.6000000e+01, 1.1254000e+03, 3.0071881e+05,\n",
       "        6.4890002e+02],\n",
       "       [2.8000000e+01, 6.4000000e+01, 1.0678000e+03, 7.1122380e+06,\n",
       "        1.5320960e+04],\n",
       "       [2.8000000e+01, 6.4000000e+01, 1.0678000e+03, 1.2325498e+06,\n",
       "        2.6551201e+03],\n",
       "       [2.7500000e+01, 6.2000000e+01, 1.3487000e+03, 5.7564819e+05,\n",
       "        1.0756801e+03],\n",
       "       [2.7500000e+01, 6.2000000e+01, 1.3487000e+03, 1.5596250e+06,\n",
       "        2.9143799e+03],\n",
       "       [2.8400000e+01, 6.6000000e+01, 7.4640002e+02, 8.3812994e+05,\n",
       "        1.8321600e+03],\n",
       "       [2.8400000e+01, 6.6000000e+01, 7.4640002e+02, 1.2630824e+06,\n",
       "        2.7611101e+03],\n",
       "       [2.7799999e+01, 6.4000000e+01, 7.4790002e+02, 8.3202781e+05,\n",
       "        1.7387700e+03],\n",
       "       [2.7799999e+01, 6.4000000e+01, 7.4790002e+02, 1.6520544e+06,\n",
       "        3.4524600e+03],\n",
       "       [2.8600000e+01, 6.5000000e+01, 9.4259998e+02, 9.4294725e+05,\n",
       "        2.1535500e+03],\n",
       "       [2.8600000e+01, 6.5000000e+01, 9.4259998e+02, 1.4440748e+06,\n",
       "        3.2980500e+03],\n",
       "       [2.7900000e+01, 6.4000000e+01, 8.1570001e+02, 6.7998338e+05,\n",
       "        1.6412200e+03],\n",
       "       [2.7900000e+01, 6.4000000e+01, 8.1570001e+02, 1.4764724e+06,\n",
       "        3.5636399e+03],\n",
       "       [2.8700001e+01, 6.6000000e+01, 1.3503000e+03, 7.8050638e+05,\n",
       "        1.6842000e+03],\n",
       "       [2.8700001e+01, 6.6000000e+01, 1.3503000e+03, 1.2038484e+06,\n",
       "        2.5977000e+03],\n",
       "       [2.8200001e+01, 6.5000000e+01, 1.0317000e+03, 4.0930409e+05,\n",
       "        8.8171002e+02],\n",
       "       [2.8200001e+01, 6.5000000e+01, 1.0317000e+03, 8.6326575e+05,\n",
       "        1.8596200e+03],\n",
       "       [2.1200001e+01, 6.5000000e+01, 7.1559998e+02, 3.5645569e+05,\n",
       "        2.2428000e+02],\n",
       "       [2.1500000e+01, 6.4000000e+01, 3.9300000e+02, 2.3918370e+05,\n",
       "        2.6095001e+02],\n",
       "       [2.1799999e+01, 6.6000000e+01, 6.3920001e+02, 3.5713650e+05,\n",
       "        5.1600000e+02],\n",
       "       [2.0799999e+01, 5.9000000e+01, 4.7154092e+02, 3.1158719e+05,\n",
       "        6.1379999e+02],\n",
       "       [2.1299999e+01, 6.5000000e+01, 3.0789999e+02, 3.5136400e+05,\n",
       "        7.2229999e+02],\n",
       "       [2.0500000e+01, 6.3000000e+01, 4.5220001e+02, 4.3582520e+06,\n",
       "        8.1440098e+03],\n",
       "       [2.1900000e+01, 6.7000000e+01, 3.0129999e+02, 4.3307405e+06,\n",
       "        9.4670400e+03],\n",
       "       [2.1400000e+01, 6.4000000e+01, 4.2679999e+02, 4.8391520e+06,\n",
       "        1.0112850e+04],\n",
       "       [2.2000000e+01, 6.8000000e+01, 5.5470001e+02, 4.3161330e+06,\n",
       "        9.8574004e+03],\n",
       "       [2.1700001e+01, 6.5000000e+01, 4.1710001e+02, 4.7123365e+06,\n",
       "        1.1373780e+04],\n",
       "       [2.2200001e+01, 6.9000000e+01, 5.3320001e+02, 5.1920220e+06,\n",
       "        1.1203500e+04],\n",
       "       [2.1799999e+01, 6.6000000e+01, 3.5179999e+02, 4.0791282e+06,\n",
       "        8.7871299e+03],\n",
       "       [2.6500000e+01, 6.9000000e+01, 8.9500000e+02, 5.7424840e+06,\n",
       "        3.6131399e+03],\n",
       "       [2.6000000e+01, 6.8000000e+01, 9.0409998e+02, 7.9703490e+06,\n",
       "        8.6956699e+03],\n",
       "       [2.6700001e+01, 7.0000000e+01, 8.5629999e+02, 9.3425250e+06,\n",
       "        1.3498320e+04],\n",
       "       [2.6200001e+01, 6.9000000e+01, 8.5259998e+02, 1.1367405e+07,\n",
       "        2.2392811e+04],\n",
       "       [2.6900000e+01, 7.1000000e+01, 1.0495000e+03, 1.0980954e+07,\n",
       "        2.2573580e+04]], dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "40c6543e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    15.308889\n",
       "1    18.406316\n",
       "2    15.856111\n",
       "3    25.090526\n",
       "4    17.786250\n",
       "Name: Yield, dtype: float64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "701af3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=5, out_features=2, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "Generation = 1\n",
      "Fitness    = -3.8705549241112305\n",
      "Generation = 2\n",
      "Fitness    = -3.740781784157617\n",
      "Generation = 3\n",
      "Fitness    = -3.644047498803003\n",
      "Generation = 4\n",
      "Fitness    = -3.644047498803003\n",
      "Generation = 5\n",
      "Fitness    = -3.585162878136499\n",
      "Generation = 6\n",
      "Fitness    = -3.5695254803703858\n",
      "Generation = 7\n",
      "Fitness    = -3.5282092095421387\n",
      "Generation = 8\n",
      "Fitness    = -3.4420881272362305\n",
      "Generation = 9\n",
      "Fitness    = -3.381798029045923\n",
      "Generation = 10\n",
      "Fitness    = -3.3467299939201904\n",
      "Generation = 11\n",
      "Fitness    = -3.3467299939201904\n",
      "Generation = 12\n",
      "Fitness    = -3.3032686711357666\n",
      "Generation = 13\n",
      "Fitness    = -3.2690885067986084\n",
      "Generation = 14\n",
      "Fitness    = -3.245617389778955\n",
      "Generation = 15\n",
      "Fitness    = -3.2345151902245117\n",
      "Generation = 16\n",
      "Fitness    = -3.187817335228784\n",
      "Generation = 17\n",
      "Fitness    = -3.166836738686426\n",
      "Generation = 18\n",
      "Fitness    = -3.147132158379419\n",
      "Generation = 19\n",
      "Fitness    = -3.146181106667383\n",
      "Generation = 20\n",
      "Fitness    = -3.1387834549950195\n",
      "Generation = 21\n",
      "Fitness    = -3.1387834549950195\n",
      "Generation = 22\n",
      "Fitness    = -3.1387834549950195\n",
      "Generation = 23\n",
      "Fitness    = -3.1387834549950195\n",
      "Generation = 24\n",
      "Fitness    = -3.1385271550224854\n",
      "Generation = 25\n",
      "Fitness    = -3.1385271550224854\n",
      "Generation = 26\n",
      "Fitness    = -3.1385271550224854\n",
      "Generation = 27\n",
      "Fitness    = -3.1385271550224854\n",
      "Generation = 28\n",
      "Fitness    = -3.1385271550224854\n",
      "Generation = 29\n",
      "Fitness    = -3.1385271550224854\n",
      "Generation = 30\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 31\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 32\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 33\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 34\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 35\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 36\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 37\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 38\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 39\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 40\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 41\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 42\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 43\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 44\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 45\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 46\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 47\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 48\n",
      "Fitness    = -3.138474941353662\n",
      "Generation = 49\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 50\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 51\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 52\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 53\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 54\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 55\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 56\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 57\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 58\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 59\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 60\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 61\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 62\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 63\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 64\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 65\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 66\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 67\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 68\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 69\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 70\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 71\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 72\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 73\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 74\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 75\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 76\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 77\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 78\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 79\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 80\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 81\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 82\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 83\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 84\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 85\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 86\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 87\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 88\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 89\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 90\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 91\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 92\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 93\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 94\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 95\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 96\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 97\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 98\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 99\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 100\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 101\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 102\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 103\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 104\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 105\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 106\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 107\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 108\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 109\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 110\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 111\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 112\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 113\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 114\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 115\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 116\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 117\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 118\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 119\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 120\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 121\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 122\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 123\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 124\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 125\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 126\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 127\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 128\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 129\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 130\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 131\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 132\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 133\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 134\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 135\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 136\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 137\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 138\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 139\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 140\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 141\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 142\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 143\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 144\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 145\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 146\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 147\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 148\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 149\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 150\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 151\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 152\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 153\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 154\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 155\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 156\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 157\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 158\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 159\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 160\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 161\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 162\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 163\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 164\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 165\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 166\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 167\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 168\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 169\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 170\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 171\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitness    = -3.138472557167871\n",
      "Generation = 173\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 174\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 175\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 176\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 177\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 178\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 179\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 180\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 181\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 182\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 183\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 184\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 185\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 186\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 187\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 188\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 189\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 190\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 191\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 192\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 193\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 194\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 195\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 196\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 197\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 198\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 199\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 200\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 201\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 202\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 203\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 204\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 205\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 206\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 207\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 208\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 209\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 210\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 211\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 212\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 213\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 214\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 215\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 216\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 217\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 218\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 219\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 220\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 221\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 222\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 223\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 224\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 225\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 226\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 227\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 228\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 229\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 230\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 231\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 232\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 233\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 234\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 235\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 236\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 237\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 238\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 239\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 240\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 241\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 242\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 243\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 244\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 245\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 246\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 247\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 248\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 249\n",
      "Fitness    = -3.138472557167871\n",
      "Generation = 250\n",
      "Fitness    = -3.138472557167871\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitness value of the best solution = -3.138472557167871\n",
      "Index of the best solution : 0\n",
      "Predictions : n [[9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]\n",
      " [9.680811]]\n",
      "rmse :  9.850011\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pygad\n",
    "import pygad.torchga as torchga\n",
    "\n",
    "def fitness_func(solution, sol_idx):\n",
    "    global data_inputs, data_outputs, model, loss_function\n",
    "\n",
    "    model_weights_dict = torchga.model_weights_as_dict(model=model, weights_vector=solution)\n",
    "    model.load_state_dict(model_weights_dict)\n",
    "\n",
    "    predictions = model(data_inputs)\n",
    "    mse = loss_function(predictions, data_outputs)\n",
    "    rmse = torch.sqrt(mse).detach().numpy() + 1e-10  # Adding a small value to avoid division by zero\n",
    "\n",
    "    # Use RMSE as the fitness directly (minimize RMSE)\n",
    "    solution_fitness = -rmse\n",
    "\n",
    "    return solution_fitness\n",
    "\n",
    "def callback_generation(ga_instance):\n",
    "    print(\"Generation = {generation}\".format(generation=ga_instance.generations_completed))\n",
    "    print(\"Fitness    = {fitness}\".format(fitness=ga_instance.best_solution()[1]))\n",
    "\n",
    "# Create the PyTorch model.\n",
    "input_layer = torch.nn.Linear(5, 2)\n",
    "relu_layer = torch.nn.ReLU()\n",
    "output_layer = torch.nn.Linear(2, 1)\n",
    "\n",
    "model = torch.nn.Sequential(input_layer, relu_layer, output_layer)\n",
    "print(model)\n",
    "\n",
    "# Create an instance of the pygad.torchga.TorchGA class to build the initial population.\n",
    "torch_ga = pygad.torchga.TorchGA(model=model, num_solutions=20)\n",
    "\n",
    "loss_function = torch.nn.L1Loss()\n",
    "\n",
    "# Data inputs\n",
    "data_inputs = torch.tensor(a)\n",
    "\n",
    "# Data outputs\n",
    "data_outputs = torch.tensor(b)\n",
    "\n",
    "# Prepare the PyGAD parameters.\n",
    "num_generations = 250\n",
    "num_parents_mating = 5\n",
    "initial_population = torch_ga.population_weights\n",
    "parent_selection_type = \"sss\"\n",
    "crossover_type = \"single_point\"\n",
    "mutation_type = \"random\"\n",
    "mutation_percent_genes = 50\n",
    "keep_parents = -1\n",
    "\n",
    "ga_instance = pygad.GA(num_generations=num_generations,\n",
    "                       num_parents_mating=num_parents_mating,\n",
    "                       initial_population=initial_population,\n",
    "                       fitness_func=fitness_func,\n",
    "                       parent_selection_type=parent_selection_type,\n",
    "                       crossover_type=crossover_type,\n",
    "                       mutation_type=mutation_type,\n",
    "                       mutation_percent_genes=mutation_percent_genes,\n",
    "                       keep_parents=keep_parents,\n",
    "                       on_generation=callback_generation)\n",
    "\n",
    "ga_instance.run()\n",
    "\n",
    "# After the generations complete, some plots are shown that summarize how the outputs/fitness values evolve over generations.\n",
    "ga_instance.plot_result(title=\"PyGAD & PyTorch - Iteration vs. Fitness\", linewidth=4)\n",
    "\n",
    "# Returning the details of the best solution.\n",
    "solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "print(\"Fitness value of the best solution = {solution_fitness}\".format(solution_fitness=solution_fitness))\n",
    "print(\"Index of the best solution : {solution_idx}\".format(solution_idx=solution_idx))\n",
    "\n",
    "# Fetch the parameters of the best solution.\n",
    "best_solution_weights = torchga.model_weights_as_dict(model=model, weights_vector=solution)\n",
    "model.load_state_dict(best_solution_weights)\n",
    "\n",
    "predictions = model(data_inputs)\n",
    "print(\"Predictions : n\", predictions.detach().numpy())\n",
    "\n",
    "abs_error = loss_function(predictions, data_outputs)\n",
    "print(\"rmse : \", abs_error.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d749c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb06df9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
