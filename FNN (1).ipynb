{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sjsP1PAqWp09"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Onion.csv')\n",
        "dataset = df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Assuming 'df' is your DataFrame with the provided data\n",
        "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
        "df = pd.read_csv('Onion.csv')\n",
        "# Select relevant features for training\n",
        "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
        "\n",
        "# Extract features and target variable\n",
        "X = df[features]\n",
        "y = df['Yield']\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X[features] = scaler.fit_transform(X[features])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.1, random_state=42)\n",
        "\n",
        "# Build and train a simple feedforward neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(200, activation='relu', input_dim=len(features)))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Use RootMeanSquaredError as a custom metric\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=200, batch_size=8, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "# Evaluate the model\n",
        "results = model.evaluate(X_test, y_test)\n",
        "rmse_value = results[1]\n",
        "\n",
        "print(f\"Root Mean Squared Error on Test Data: {rmse_value:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5dAfi-aZq6mI",
        "outputId": "ab9f3a7d-f1e5-40ef-b62f-1f0daf61794f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-09aac343754c>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[features] = scaler.fit_transform(X[features])\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "32/32 [==============================] - 1s 10ms/step - loss: 1118.9744 - root_mean_squared_error: 33.4511 - val_loss: 115.7568 - val_root_mean_squared_error: 10.7590\n",
            "Epoch 2/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 972.1239 - root_mean_squared_error: 31.1789 - val_loss: 56.1377 - val_root_mean_squared_error: 7.4925\n",
            "Epoch 3/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 920.5003 - root_mean_squared_error: 30.3397 - val_loss: 94.4517 - val_root_mean_squared_error: 9.7186\n",
            "Epoch 4/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 906.3833 - root_mean_squared_error: 30.1062 - val_loss: 89.7035 - val_root_mean_squared_error: 9.4712\n",
            "Epoch 5/200\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 899.4782 - root_mean_squared_error: 29.9913 - val_loss: 82.8579 - val_root_mean_squared_error: 9.1026\n",
            "Epoch 6/200\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 893.3813 - root_mean_squared_error: 29.8895 - val_loss: 63.2072 - val_root_mean_squared_error: 7.9503\n",
            "Epoch 7/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 906.5599 - root_mean_squared_error: 30.1091 - val_loss: 127.5405 - val_root_mean_squared_error: 11.2934\n",
            "Epoch 8/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 891.5086 - root_mean_squared_error: 29.8581 - val_loss: 115.7663 - val_root_mean_squared_error: 10.7595\n",
            "Epoch 9/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 884.3676 - root_mean_squared_error: 29.7383 - val_loss: 69.9429 - val_root_mean_squared_error: 8.3632\n",
            "Epoch 10/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 877.2969 - root_mean_squared_error: 29.6192 - val_loss: 60.6052 - val_root_mean_squared_error: 7.7849\n",
            "Epoch 11/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 869.6285 - root_mean_squared_error: 29.4895 - val_loss: 95.4999 - val_root_mean_squared_error: 9.7724\n",
            "Epoch 12/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 944.1883 - root_mean_squared_error: 30.7276 - val_loss: 73.1064 - val_root_mean_squared_error: 8.5502\n",
            "Epoch 13/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 859.0923 - root_mean_squared_error: 29.3103 - val_loss: 115.2651 - val_root_mean_squared_error: 10.7362\n",
            "Epoch 14/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 854.2787 - root_mean_squared_error: 29.2280 - val_loss: 80.1974 - val_root_mean_squared_error: 8.9553\n",
            "Epoch 15/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 854.4978 - root_mean_squared_error: 29.2318 - val_loss: 40.4358 - val_root_mean_squared_error: 6.3589\n",
            "Epoch 16/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 858.1996 - root_mean_squared_error: 29.2950 - val_loss: 63.4350 - val_root_mean_squared_error: 7.9646\n",
            "Epoch 17/200\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 828.2728 - root_mean_squared_error: 28.7797 - val_loss: 112.5816 - val_root_mean_squared_error: 10.6104\n",
            "Epoch 18/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 829.0309 - root_mean_squared_error: 28.7929 - val_loss: 35.6776 - val_root_mean_squared_error: 5.9731\n",
            "Epoch 19/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 848.5463 - root_mean_squared_error: 29.1298 - val_loss: 77.9105 - val_root_mean_squared_error: 8.8267\n",
            "Epoch 20/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 795.9798 - root_mean_squared_error: 28.2131 - val_loss: 71.1568 - val_root_mean_squared_error: 8.4354\n",
            "Epoch 21/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 803.7161 - root_mean_squared_error: 28.3499 - val_loss: 37.8013 - val_root_mean_squared_error: 6.1483\n",
            "Epoch 22/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 780.8344 - root_mean_squared_error: 27.9434 - val_loss: 87.2285 - val_root_mean_squared_error: 9.3396\n",
            "Epoch 23/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 761.4897 - root_mean_squared_error: 27.5951 - val_loss: 76.6956 - val_root_mean_squared_error: 8.7576\n",
            "Epoch 24/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 753.3180 - root_mean_squared_error: 27.4466 - val_loss: 82.4341 - val_root_mean_squared_error: 9.0793\n",
            "Epoch 25/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 777.5306 - root_mean_squared_error: 27.8842 - val_loss: 39.3064 - val_root_mean_squared_error: 6.2695\n",
            "Epoch 26/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 719.3338 - root_mean_squared_error: 26.8204 - val_loss: 49.6093 - val_root_mean_squared_error: 7.0434\n",
            "Epoch 27/200\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 718.1193 - root_mean_squared_error: 26.7977 - val_loss: 133.5975 - val_root_mean_squared_error: 11.5584\n",
            "Epoch 28/200\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 723.8293 - root_mean_squared_error: 26.9041 - val_loss: 88.2628 - val_root_mean_squared_error: 9.3948\n",
            "Epoch 29/200\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 666.0515 - root_mean_squared_error: 25.8080 - val_loss: 26.9491 - val_root_mean_squared_error: 5.1913\n",
            "Epoch 30/200\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 604.9218 - root_mean_squared_error: 24.5952 - val_loss: 117.7760 - val_root_mean_squared_error: 10.8525\n",
            "Epoch 31/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 646.1609 - root_mean_squared_error: 25.4197 - val_loss: 28.5320 - val_root_mean_squared_error: 5.3415\n",
            "Epoch 32/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 590.3680 - root_mean_squared_error: 24.2975 - val_loss: 75.7447 - val_root_mean_squared_error: 8.7031\n",
            "Epoch 33/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 578.5471 - root_mean_squared_error: 24.0530 - val_loss: 35.2182 - val_root_mean_squared_error: 5.9345\n",
            "Epoch 34/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 490.3896 - root_mean_squared_error: 22.1447 - val_loss: 106.0458 - val_root_mean_squared_error: 10.2979\n",
            "Epoch 35/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 546.8331 - root_mean_squared_error: 23.3845 - val_loss: 23.9348 - val_root_mean_squared_error: 4.8923\n",
            "Epoch 36/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 431.8396 - root_mean_squared_error: 20.7808 - val_loss: 16.7942 - val_root_mean_squared_error: 4.0981\n",
            "Epoch 37/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 513.2405 - root_mean_squared_error: 22.6548 - val_loss: 80.2438 - val_root_mean_squared_error: 8.9579\n",
            "Epoch 38/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 474.6114 - root_mean_squared_error: 21.7856 - val_loss: 33.0648 - val_root_mean_squared_error: 5.7502\n",
            "Epoch 39/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 350.8892 - root_mean_squared_error: 18.7320 - val_loss: 33.5924 - val_root_mean_squared_error: 5.7959\n",
            "Epoch 40/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 327.0136 - root_mean_squared_error: 18.0835 - val_loss: 22.0764 - val_root_mean_squared_error: 4.6986\n",
            "Epoch 41/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 408.3334 - root_mean_squared_error: 20.2073 - val_loss: 34.4045 - val_root_mean_squared_error: 5.8655\n",
            "Epoch 42/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 229.1558 - root_mean_squared_error: 15.1379 - val_loss: 30.2702 - val_root_mean_squared_error: 5.5018\n",
            "Epoch 43/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 231.7365 - root_mean_squared_error: 15.2229 - val_loss: 40.7330 - val_root_mean_squared_error: 6.3822\n",
            "Epoch 44/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 252.4605 - root_mean_squared_error: 15.8890 - val_loss: 24.0800 - val_root_mean_squared_error: 4.9071\n",
            "Epoch 45/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 386.4405 - root_mean_squared_error: 19.6581 - val_loss: 40.5153 - val_root_mean_squared_error: 6.3652\n",
            "Epoch 46/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 130.4196 - root_mean_squared_error: 11.4201 - val_loss: 24.1567 - val_root_mean_squared_error: 4.9149\n",
            "Epoch 47/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 176.9230 - root_mean_squared_error: 13.3012 - val_loss: 27.6054 - val_root_mean_squared_error: 5.2541\n",
            "Epoch 48/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 136.4993 - root_mean_squared_error: 11.6833 - val_loss: 25.8241 - val_root_mean_squared_error: 5.0817\n",
            "Epoch 49/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 116.5832 - root_mean_squared_error: 10.7974 - val_loss: 30.9509 - val_root_mean_squared_error: 5.5634\n",
            "Epoch 50/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 133.6041 - root_mean_squared_error: 11.5587 - val_loss: 39.7863 - val_root_mean_squared_error: 6.3076\n",
            "Epoch 51/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 128.4608 - root_mean_squared_error: 11.3341 - val_loss: 38.4999 - val_root_mean_squared_error: 6.2048\n",
            "Epoch 52/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 169.1873 - root_mean_squared_error: 13.0072 - val_loss: 40.2910 - val_root_mean_squared_error: 6.3475\n",
            "Epoch 53/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 110.9759 - root_mean_squared_error: 10.5345 - val_loss: 31.0708 - val_root_mean_squared_error: 5.5741\n",
            "Epoch 54/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 159.4891 - root_mean_squared_error: 12.6289 - val_loss: 53.1556 - val_root_mean_squared_error: 7.2908\n",
            "Epoch 55/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 169.0006 - root_mean_squared_error: 13.0000 - val_loss: 31.1742 - val_root_mean_squared_error: 5.5834\n",
            "Epoch 56/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 511.4278 - root_mean_squared_error: 22.6148 - val_loss: 64.6415 - val_root_mean_squared_error: 8.0400\n",
            "Epoch 57/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 213.4497 - root_mean_squared_error: 14.6099 - val_loss: 31.1235 - val_root_mean_squared_error: 5.5788\n",
            "Epoch 58/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 119.2250 - root_mean_squared_error: 10.9190 - val_loss: 34.0773 - val_root_mean_squared_error: 5.8376\n",
            "Epoch 59/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 106.2383 - root_mean_squared_error: 10.3072 - val_loss: 35.6941 - val_root_mean_squared_error: 5.9745\n",
            "Epoch 60/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 91.6963 - root_mean_squared_error: 9.5758 - val_loss: 57.7900 - val_root_mean_squared_error: 7.6020\n",
            "Epoch 61/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 72.4333 - root_mean_squared_error: 8.5108 - val_loss: 37.4094 - val_root_mean_squared_error: 6.1163\n",
            "Epoch 62/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 68.9247 - root_mean_squared_error: 8.3021 - val_loss: 56.2973 - val_root_mean_squared_error: 7.5031\n",
            "Epoch 63/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 78.2442 - root_mean_squared_error: 8.8456 - val_loss: 27.8159 - val_root_mean_squared_error: 5.2741\n",
            "Epoch 64/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 78.2506 - root_mean_squared_error: 8.8459 - val_loss: 45.0242 - val_root_mean_squared_error: 6.7100\n",
            "Epoch 65/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 88.3123 - root_mean_squared_error: 9.3975 - val_loss: 21.1409 - val_root_mean_squared_error: 4.5979\n",
            "Epoch 66/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 200.5457 - root_mean_squared_error: 14.1614 - val_loss: 58.7390 - val_root_mean_squared_error: 7.6641\n",
            "Epoch 67/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 239.3698 - root_mean_squared_error: 15.4716 - val_loss: 37.0326 - val_root_mean_squared_error: 6.0854\n",
            "Epoch 68/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 190.2728 - root_mean_squared_error: 13.7939 - val_loss: 40.4855 - val_root_mean_squared_error: 6.3628\n",
            "Epoch 69/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 83.9742 - root_mean_squared_error: 9.1637 - val_loss: 33.8621 - val_root_mean_squared_error: 5.8191\n",
            "Epoch 70/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 32.7836 - root_mean_squared_error: 5.7257 - val_loss: 36.7347 - val_root_mean_squared_error: 6.0609\n",
            "Epoch 71/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 29.8178 - root_mean_squared_error: 5.4606 - val_loss: 39.9629 - val_root_mean_squared_error: 6.3216\n",
            "Epoch 72/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 27.8850 - root_mean_squared_error: 5.2806 - val_loss: 35.2312 - val_root_mean_squared_error: 5.9356\n",
            "Epoch 73/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 29.0930 - root_mean_squared_error: 5.3938 - val_loss: 29.9567 - val_root_mean_squared_error: 5.4733\n",
            "Epoch 74/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 30.8875 - root_mean_squared_error: 5.5577 - val_loss: 35.3648 - val_root_mean_squared_error: 5.9468\n",
            "Epoch 75/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 37.7743 - root_mean_squared_error: 6.1461 - val_loss: 32.0834 - val_root_mean_squared_error: 5.6642\n",
            "Epoch 76/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 26.2943 - root_mean_squared_error: 5.1278 - val_loss: 35.5169 - val_root_mean_squared_error: 5.9596\n",
            "Epoch 77/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 26.3043 - root_mean_squared_error: 5.1288 - val_loss: 31.3739 - val_root_mean_squared_error: 5.6012\n",
            "Epoch 78/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 23.5277 - root_mean_squared_error: 4.8505 - val_loss: 38.8105 - val_root_mean_squared_error: 6.2298\n",
            "Epoch 79/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 26.4775 - root_mean_squared_error: 5.1456 - val_loss: 33.5169 - val_root_mean_squared_error: 5.7894\n",
            "Epoch 80/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 25.0627 - root_mean_squared_error: 5.0063 - val_loss: 33.0783 - val_root_mean_squared_error: 5.7514\n",
            "Epoch 81/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 23.9222 - root_mean_squared_error: 4.8910 - val_loss: 37.2477 - val_root_mean_squared_error: 6.1031\n",
            "Epoch 82/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 22.3652 - root_mean_squared_error: 4.7292 - val_loss: 30.5871 - val_root_mean_squared_error: 5.5306\n",
            "Epoch 83/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 23.9483 - root_mean_squared_error: 4.8937 - val_loss: 36.4628 - val_root_mean_squared_error: 6.0384\n",
            "Epoch 84/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 29.3864 - root_mean_squared_error: 5.4209 - val_loss: 38.9502 - val_root_mean_squared_error: 6.2410\n",
            "Epoch 85/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 25.9217 - root_mean_squared_error: 5.0913 - val_loss: 41.3350 - val_root_mean_squared_error: 6.4292\n",
            "Epoch 86/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 24.7200 - root_mean_squared_error: 4.9719 - val_loss: 30.4540 - val_root_mean_squared_error: 5.5185\n",
            "Epoch 87/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 28.7746 - root_mean_squared_error: 5.3642 - val_loss: 39.0013 - val_root_mean_squared_error: 6.2451\n",
            "Epoch 88/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 33.4972 - root_mean_squared_error: 5.7877 - val_loss: 35.9489 - val_root_mean_squared_error: 5.9957\n",
            "Epoch 89/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 38.1267 - root_mean_squared_error: 6.1747 - val_loss: 40.7749 - val_root_mean_squared_error: 6.3855\n",
            "Epoch 90/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 32.2978 - root_mean_squared_error: 5.6831 - val_loss: 36.3519 - val_root_mean_squared_error: 6.0293\n",
            "Epoch 91/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 31.6889 - root_mean_squared_error: 5.6293 - val_loss: 41.1536 - val_root_mean_squared_error: 6.4151\n",
            "Epoch 92/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 26.1296 - root_mean_squared_error: 5.1117 - val_loss: 35.3700 - val_root_mean_squared_error: 5.9473\n",
            "Epoch 93/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 24.9946 - root_mean_squared_error: 4.9995 - val_loss: 41.5308 - val_root_mean_squared_error: 6.4444\n",
            "Epoch 94/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 23.7096 - root_mean_squared_error: 4.8693 - val_loss: 37.7486 - val_root_mean_squared_error: 6.1440\n",
            "Epoch 95/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 22.6642 - root_mean_squared_error: 4.7607 - val_loss: 35.1714 - val_root_mean_squared_error: 5.9306\n",
            "Epoch 96/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 18.9638 - root_mean_squared_error: 4.3547 - val_loss: 33.7065 - val_root_mean_squared_error: 5.8057\n",
            "Epoch 97/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 18.3099 - root_mean_squared_error: 4.2790 - val_loss: 35.1047 - val_root_mean_squared_error: 5.9249\n",
            "Epoch 98/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 17.2345 - root_mean_squared_error: 4.1515 - val_loss: 34.1099 - val_root_mean_squared_error: 5.8404\n",
            "Epoch 99/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 17.7679 - root_mean_squared_error: 4.2152 - val_loss: 36.5576 - val_root_mean_squared_error: 6.0463\n",
            "Epoch 100/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 17.6087 - root_mean_squared_error: 4.1963 - val_loss: 36.2066 - val_root_mean_squared_error: 6.0172\n",
            "Epoch 101/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 17.7011 - root_mean_squared_error: 4.2073 - val_loss: 39.6371 - val_root_mean_squared_error: 6.2958\n",
            "Epoch 102/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 18.8606 - root_mean_squared_error: 4.3429 - val_loss: 33.5383 - val_root_mean_squared_error: 5.7912\n",
            "Epoch 103/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 19.4966 - root_mean_squared_error: 4.4155 - val_loss: 34.5217 - val_root_mean_squared_error: 5.8755\n",
            "Epoch 104/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 22.7567 - root_mean_squared_error: 4.7704 - val_loss: 42.3512 - val_root_mean_squared_error: 6.5078\n",
            "Epoch 105/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 22.6475 - root_mean_squared_error: 4.7589 - val_loss: 32.1648 - val_root_mean_squared_error: 5.6714\n",
            "Epoch 106/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 39.5429 - root_mean_squared_error: 6.2883 - val_loss: 41.9239 - val_root_mean_squared_error: 6.4749\n",
            "Epoch 107/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 58.3676 - root_mean_squared_error: 7.6399 - val_loss: 39.3908 - val_root_mean_squared_error: 6.2762\n",
            "Epoch 108/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 54.9022 - root_mean_squared_error: 7.4096 - val_loss: 30.9955 - val_root_mean_squared_error: 5.5674\n",
            "Epoch 109/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 44.4042 - root_mean_squared_error: 6.6636 - val_loss: 29.6667 - val_root_mean_squared_error: 5.4467\n",
            "Epoch 110/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 56.8993 - root_mean_squared_error: 7.5432 - val_loss: 47.0761 - val_root_mean_squared_error: 6.8612\n",
            "Epoch 111/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 89.3246 - root_mean_squared_error: 9.4512 - val_loss: 29.8547 - val_root_mean_squared_error: 5.4639\n",
            "Epoch 112/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 246.5840 - root_mean_squared_error: 15.7030 - val_loss: 65.2687 - val_root_mean_squared_error: 8.0789\n",
            "Epoch 113/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 70.1076 - root_mean_squared_error: 8.3730 - val_loss: 34.2559 - val_root_mean_squared_error: 5.8529\n",
            "Epoch 114/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 141.0374 - root_mean_squared_error: 11.8759 - val_loss: 59.8088 - val_root_mean_squared_error: 7.7336\n",
            "Epoch 115/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 56.0399 - root_mean_squared_error: 7.4860 - val_loss: 33.0300 - val_root_mean_squared_error: 5.7472\n",
            "Epoch 116/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 126.1997 - root_mean_squared_error: 11.2339 - val_loss: 43.5435 - val_root_mean_squared_error: 6.5987\n",
            "Epoch 117/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 88.5963 - root_mean_squared_error: 9.4126 - val_loss: 21.9831 - val_root_mean_squared_error: 4.6886\n",
            "Epoch 118/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 315.9091 - root_mean_squared_error: 17.7738 - val_loss: 54.1128 - val_root_mean_squared_error: 7.3561\n",
            "Epoch 119/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 137.3297 - root_mean_squared_error: 11.7188 - val_loss: 43.7652 - val_root_mean_squared_error: 6.6155\n",
            "Epoch 120/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 135.9444 - root_mean_squared_error: 11.6595 - val_loss: 33.7179 - val_root_mean_squared_error: 5.8067\n",
            "Epoch 121/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 153.4425 - root_mean_squared_error: 12.3872 - val_loss: 43.8622 - val_root_mean_squared_error: 6.6229\n",
            "Epoch 122/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 130.2107 - root_mean_squared_error: 11.4110 - val_loss: 38.6339 - val_root_mean_squared_error: 6.2156\n",
            "Epoch 123/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 117.6346 - root_mean_squared_error: 10.8459 - val_loss: 47.9194 - val_root_mean_squared_error: 6.9224\n",
            "Epoch 124/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 122.5286 - root_mean_squared_error: 11.0693 - val_loss: 32.1746 - val_root_mean_squared_error: 5.6723\n",
            "Epoch 125/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 117.3789 - root_mean_squared_error: 10.8342 - val_loss: 58.1490 - val_root_mean_squared_error: 7.6255\n",
            "Epoch 126/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 84.0033 - root_mean_squared_error: 9.1653 - val_loss: 66.9185 - val_root_mean_squared_error: 8.1804\n",
            "Epoch 127/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 80.4499 - root_mean_squared_error: 8.9694 - val_loss: 55.1998 - val_root_mean_squared_error: 7.4297\n",
            "Epoch 128/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 76.0210 - root_mean_squared_error: 8.7190 - val_loss: 46.5682 - val_root_mean_squared_error: 6.8241\n",
            "Epoch 129/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 68.1012 - root_mean_squared_error: 8.2523 - val_loss: 54.5154 - val_root_mean_squared_error: 7.3835\n",
            "Epoch 130/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 60.1893 - root_mean_squared_error: 7.7582 - val_loss: 61.3568 - val_root_mean_squared_error: 7.8331\n",
            "Epoch 131/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 59.7239 - root_mean_squared_error: 7.7281 - val_loss: 47.9842 - val_root_mean_squared_error: 6.9271\n",
            "Epoch 132/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 63.3742 - root_mean_squared_error: 7.9608 - val_loss: 69.8247 - val_root_mean_squared_error: 8.3561\n",
            "Epoch 133/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 66.6832 - root_mean_squared_error: 8.1660 - val_loss: 49.2359 - val_root_mean_squared_error: 7.0168\n",
            "Epoch 134/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 78.4012 - root_mean_squared_error: 8.8544 - val_loss: 49.8981 - val_root_mean_squared_error: 7.0639\n",
            "Epoch 135/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 67.5220 - root_mean_squared_error: 8.2172 - val_loss: 41.1303 - val_root_mean_squared_error: 6.4133\n",
            "Epoch 136/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 73.7791 - root_mean_squared_error: 8.5895 - val_loss: 44.3321 - val_root_mean_squared_error: 6.6582\n",
            "Epoch 137/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 58.7501 - root_mean_squared_error: 7.6649 - val_loss: 50.5290 - val_root_mean_squared_error: 7.1084\n",
            "Epoch 138/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 45.8413 - root_mean_squared_error: 6.7706 - val_loss: 45.5178 - val_root_mean_squared_error: 6.7467\n",
            "Epoch 139/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 40.0984 - root_mean_squared_error: 6.3323 - val_loss: 37.6699 - val_root_mean_squared_error: 6.1376\n",
            "Epoch 140/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 48.8718 - root_mean_squared_error: 6.9908 - val_loss: 38.4694 - val_root_mean_squared_error: 6.2024\n",
            "Epoch 141/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 34.2227 - root_mean_squared_error: 5.8500 - val_loss: 41.6154 - val_root_mean_squared_error: 6.4510\n",
            "Epoch 142/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 40.6198 - root_mean_squared_error: 6.3734 - val_loss: 37.9399 - val_root_mean_squared_error: 6.1595\n",
            "Epoch 143/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 37.5020 - root_mean_squared_error: 6.1239 - val_loss: 33.9631 - val_root_mean_squared_error: 5.8278\n",
            "Epoch 144/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 24.2110 - root_mean_squared_error: 4.9205 - val_loss: 34.2476 - val_root_mean_squared_error: 5.8521\n",
            "Epoch 145/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 19.3838 - root_mean_squared_error: 4.4027 - val_loss: 39.4724 - val_root_mean_squared_error: 6.2827\n",
            "Epoch 146/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 23.3368 - root_mean_squared_error: 4.8308 - val_loss: 33.5758 - val_root_mean_squared_error: 5.7945\n",
            "Epoch 147/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 64.5497 - root_mean_squared_error: 8.0343 - val_loss: 28.0828 - val_root_mean_squared_error: 5.2993\n",
            "Epoch 148/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 73.4397 - root_mean_squared_error: 8.5697 - val_loss: 42.6571 - val_root_mean_squared_error: 6.5312\n",
            "Epoch 149/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 38.5409 - root_mean_squared_error: 6.2081 - val_loss: 32.6329 - val_root_mean_squared_error: 5.7125\n",
            "Epoch 150/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 26.0092 - root_mean_squared_error: 5.0999 - val_loss: 32.9642 - val_root_mean_squared_error: 5.7414\n",
            "Epoch 151/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 30.6051 - root_mean_squared_error: 5.5322 - val_loss: 31.6494 - val_root_mean_squared_error: 5.6258\n",
            "Epoch 152/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 19.0426 - root_mean_squared_error: 4.3638 - val_loss: 33.3628 - val_root_mean_squared_error: 5.7760\n",
            "Epoch 153/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 15.3596 - root_mean_squared_error: 3.9191 - val_loss: 30.8703 - val_root_mean_squared_error: 5.5561\n",
            "Epoch 154/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 14.3419 - root_mean_squared_error: 3.7871 - val_loss: 39.9066 - val_root_mean_squared_error: 6.3172\n",
            "Epoch 155/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 15.8288 - root_mean_squared_error: 3.9785 - val_loss: 34.6622 - val_root_mean_squared_error: 5.8875\n",
            "Epoch 156/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 19.3642 - root_mean_squared_error: 4.4005 - val_loss: 32.9235 - val_root_mean_squared_error: 5.7379\n",
            "Epoch 157/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 27.4708 - root_mean_squared_error: 5.2413 - val_loss: 34.7681 - val_root_mean_squared_error: 5.8964\n",
            "Epoch 158/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 58.2867 - root_mean_squared_error: 7.6346 - val_loss: 40.2822 - val_root_mean_squared_error: 6.3468\n",
            "Epoch 159/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 85.5483 - root_mean_squared_error: 9.2492 - val_loss: 42.7764 - val_root_mean_squared_error: 6.5404\n",
            "Epoch 160/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 33.7557 - root_mean_squared_error: 5.8100 - val_loss: 32.2728 - val_root_mean_squared_error: 5.6809\n",
            "Epoch 161/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 21.4592 - root_mean_squared_error: 4.6324 - val_loss: 39.4752 - val_root_mean_squared_error: 6.2829\n",
            "Epoch 162/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 22.0494 - root_mean_squared_error: 4.6957 - val_loss: 33.5766 - val_root_mean_squared_error: 5.7945\n",
            "Epoch 163/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 36.4718 - root_mean_squared_error: 6.0392 - val_loss: 48.8110 - val_root_mean_squared_error: 6.9865\n",
            "Epoch 164/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 50.0725 - root_mean_squared_error: 7.0762 - val_loss: 24.8302 - val_root_mean_squared_error: 4.9830\n",
            "Epoch 165/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 122.0080 - root_mean_squared_error: 11.0457 - val_loss: 52.2930 - val_root_mean_squared_error: 7.2314\n",
            "Epoch 166/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 38.5826 - root_mean_squared_error: 6.2115 - val_loss: 37.3877 - val_root_mean_squared_error: 6.1145\n",
            "Epoch 167/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 25.3008 - root_mean_squared_error: 5.0300 - val_loss: 43.0301 - val_root_mean_squared_error: 6.5597\n",
            "Epoch 168/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 34.1824 - root_mean_squared_error: 5.8466 - val_loss: 36.4915 - val_root_mean_squared_error: 6.0408\n",
            "Epoch 169/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 58.1437 - root_mean_squared_error: 7.6252 - val_loss: 33.3185 - val_root_mean_squared_error: 5.7722\n",
            "Epoch 170/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 38.5846 - root_mean_squared_error: 6.2116 - val_loss: 32.7637 - val_root_mean_squared_error: 5.7240\n",
            "Epoch 171/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 73.5478 - root_mean_squared_error: 8.5760 - val_loss: 33.5289 - val_root_mean_squared_error: 5.7904\n",
            "Epoch 172/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 119.5768 - root_mean_squared_error: 10.9351 - val_loss: 34.8099 - val_root_mean_squared_error: 5.9000\n",
            "Epoch 173/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 184.3921 - root_mean_squared_error: 13.5791 - val_loss: 37.4984 - val_root_mean_squared_error: 6.1236\n",
            "Epoch 174/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 248.8258 - root_mean_squared_error: 15.7742 - val_loss: 48.7063 - val_root_mean_squared_error: 6.9790\n",
            "Epoch 175/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 488.0668 - root_mean_squared_error: 22.0922 - val_loss: 29.8015 - val_root_mean_squared_error: 5.4591\n",
            "Epoch 176/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 456.2567 - root_mean_squared_error: 21.3602 - val_loss: 56.6626 - val_root_mean_squared_error: 7.5275\n",
            "Epoch 177/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 212.3525 - root_mean_squared_error: 14.5723 - val_loss: 37.1261 - val_root_mean_squared_error: 6.0931\n",
            "Epoch 178/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 110.2187 - root_mean_squared_error: 10.4985 - val_loss: 28.1620 - val_root_mean_squared_error: 5.3068\n",
            "Epoch 179/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 31.5718 - root_mean_squared_error: 5.6189 - val_loss: 37.4531 - val_root_mean_squared_error: 6.1199\n",
            "Epoch 180/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 43.4766 - root_mean_squared_error: 6.5937 - val_loss: 25.5428 - val_root_mean_squared_error: 5.0540\n",
            "Epoch 181/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 90.6173 - root_mean_squared_error: 9.5193 - val_loss: 46.4103 - val_root_mean_squared_error: 6.8125\n",
            "Epoch 182/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 33.6559 - root_mean_squared_error: 5.8014 - val_loss: 27.6576 - val_root_mean_squared_error: 5.2590\n",
            "Epoch 183/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 20.8731 - root_mean_squared_error: 4.5687 - val_loss: 36.3028 - val_root_mean_squared_error: 6.0252\n",
            "Epoch 184/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 13.8950 - root_mean_squared_error: 3.7276 - val_loss: 33.2701 - val_root_mean_squared_error: 5.7680\n",
            "Epoch 185/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 13.4773 - root_mean_squared_error: 3.6711 - val_loss: 31.6015 - val_root_mean_squared_error: 5.6215\n",
            "Epoch 186/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 13.6872 - root_mean_squared_error: 3.6996 - val_loss: 36.1943 - val_root_mean_squared_error: 6.0162\n",
            "Epoch 187/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 13.4758 - root_mean_squared_error: 3.6709 - val_loss: 30.3199 - val_root_mean_squared_error: 5.5064\n",
            "Epoch 188/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 12.2902 - root_mean_squared_error: 3.5057 - val_loss: 31.8196 - val_root_mean_squared_error: 5.6409\n",
            "Epoch 189/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 12.4516 - root_mean_squared_error: 3.5287 - val_loss: 32.3894 - val_root_mean_squared_error: 5.6912\n",
            "Epoch 190/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 12.4569 - root_mean_squared_error: 3.5294 - val_loss: 31.0834 - val_root_mean_squared_error: 5.5753\n",
            "Epoch 191/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 13.1243 - root_mean_squared_error: 3.6227 - val_loss: 34.8608 - val_root_mean_squared_error: 5.9043\n",
            "Epoch 192/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 12.7015 - root_mean_squared_error: 3.5639 - val_loss: 36.0482 - val_root_mean_squared_error: 6.0040\n",
            "Epoch 193/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 12.3151 - root_mean_squared_error: 3.5093 - val_loss: 32.1088 - val_root_mean_squared_error: 5.6665\n",
            "Epoch 194/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 12.5253 - root_mean_squared_error: 3.5391 - val_loss: 33.4949 - val_root_mean_squared_error: 5.7875\n",
            "Epoch 195/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 12.4684 - root_mean_squared_error: 3.5311 - val_loss: 39.7862 - val_root_mean_squared_error: 6.3076\n",
            "Epoch 196/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 12.3871 - root_mean_squared_error: 3.5195 - val_loss: 30.7686 - val_root_mean_squared_error: 5.5469\n",
            "Epoch 197/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 11.7407 - root_mean_squared_error: 3.4265 - val_loss: 33.2114 - val_root_mean_squared_error: 5.7629\n",
            "Epoch 198/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 12.2194 - root_mean_squared_error: 3.4956 - val_loss: 32.3982 - val_root_mean_squared_error: 5.6919\n",
            "Epoch 199/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 12.8342 - root_mean_squared_error: 3.5825 - val_loss: 27.1272 - val_root_mean_squared_error: 5.2084\n",
            "Epoch 200/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 12.2508 - root_mean_squared_error: 3.5001 - val_loss: 32.2438 - val_root_mean_squared_error: 5.6784\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 32.2438 - root_mean_squared_error: 5.6784\n",
            "Root Mean Squared Error on Test Data: 5.68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Assuming 'df' is your DataFrame with the provided data\n",
        "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
        "df = pd.read_csv('Potato.csv')\n",
        "# Select relevant features for training\n",
        "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
        "\n",
        "# Extract features and target variable\n",
        "X = df[features]\n",
        "y = df['Yield']\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X[features] = scaler.fit_transform(X[features])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.1, random_state=42)\n",
        "\n",
        "# Build and train a simple feedforward neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(200, activation='relu', input_dim=len(features)))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Use RootMeanSquaredError as a custom metric\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=200, batch_size=8, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "results = model.evaluate(X_test, y_test)\n",
        "rmse_value = results[1]\n",
        "\n",
        "print(f\"Root Mean Squared Error on Test Data: {rmse_value:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KlJf7l-g-5Lf",
        "outputId": "9aa6253e-f687-4bac-c986-5eba3e63f7cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-1c17f58d5e49>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[features] = scaler.fit_transform(X[features])\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "41/41 [==============================] - 1s 8ms/step - loss: 111.5517 - root_mean_squared_error: 10.5618 - val_loss: 33.1736 - val_root_mean_squared_error: 5.7597\n",
            "Epoch 2/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 27.6968 - root_mean_squared_error: 5.2628 - val_loss: 32.1367 - val_root_mean_squared_error: 5.6689\n",
            "Epoch 3/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 24.3671 - root_mean_squared_error: 4.9363 - val_loss: 42.8581 - val_root_mean_squared_error: 6.5466\n",
            "Epoch 4/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 22.0040 - root_mean_squared_error: 4.6908 - val_loss: 33.8075 - val_root_mean_squared_error: 5.8144\n",
            "Epoch 5/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 21.4620 - root_mean_squared_error: 4.6327 - val_loss: 29.8153 - val_root_mean_squared_error: 5.4603\n",
            "Epoch 6/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 23.1820 - root_mean_squared_error: 4.8148 - val_loss: 31.0696 - val_root_mean_squared_error: 5.5740\n",
            "Epoch 7/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 19.6734 - root_mean_squared_error: 4.4355 - val_loss: 23.9509 - val_root_mean_squared_error: 4.8940\n",
            "Epoch 8/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 18.4916 - root_mean_squared_error: 4.3002 - val_loss: 28.2943 - val_root_mean_squared_error: 5.3192\n",
            "Epoch 9/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 18.8682 - root_mean_squared_error: 4.3438 - val_loss: 25.6492 - val_root_mean_squared_error: 5.0645\n",
            "Epoch 10/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 17.1061 - root_mean_squared_error: 4.1359 - val_loss: 17.9872 - val_root_mean_squared_error: 4.2411\n",
            "Epoch 11/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 17.0332 - root_mean_squared_error: 4.1271 - val_loss: 22.0332 - val_root_mean_squared_error: 4.6940\n",
            "Epoch 12/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 17.1510 - root_mean_squared_error: 4.1414 - val_loss: 16.3974 - val_root_mean_squared_error: 4.0494\n",
            "Epoch 13/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 16.4605 - root_mean_squared_error: 4.0571 - val_loss: 21.4499 - val_root_mean_squared_error: 4.6314\n",
            "Epoch 14/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 14.7999 - root_mean_squared_error: 3.8471 - val_loss: 15.6452 - val_root_mean_squared_error: 3.9554\n",
            "Epoch 15/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 15.9679 - root_mean_squared_error: 3.9960 - val_loss: 21.1446 - val_root_mean_squared_error: 4.5983\n",
            "Epoch 16/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 15.5863 - root_mean_squared_error: 3.9479 - val_loss: 19.7691 - val_root_mean_squared_error: 4.4462\n",
            "Epoch 17/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 14.9592 - root_mean_squared_error: 3.8677 - val_loss: 18.7183 - val_root_mean_squared_error: 4.3265\n",
            "Epoch 18/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 15.8830 - root_mean_squared_error: 3.9853 - val_loss: 16.7143 - val_root_mean_squared_error: 4.0883\n",
            "Epoch 19/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 15.6081 - root_mean_squared_error: 3.9507 - val_loss: 16.5394 - val_root_mean_squared_error: 4.0669\n",
            "Epoch 20/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 14.6786 - root_mean_squared_error: 3.8313 - val_loss: 16.8893 - val_root_mean_squared_error: 4.1097\n",
            "Epoch 21/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 14.1739 - root_mean_squared_error: 3.7648 - val_loss: 14.0214 - val_root_mean_squared_error: 3.7445\n",
            "Epoch 22/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 14.7463 - root_mean_squared_error: 3.8401 - val_loss: 17.8032 - val_root_mean_squared_error: 4.2194\n",
            "Epoch 23/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 13.8730 - root_mean_squared_error: 3.7247 - val_loss: 14.6939 - val_root_mean_squared_error: 3.8333\n",
            "Epoch 24/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 14.3608 - root_mean_squared_error: 3.7896 - val_loss: 13.3570 - val_root_mean_squared_error: 3.6547\n",
            "Epoch 25/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 13.7926 - root_mean_squared_error: 3.7138 - val_loss: 17.7859 - val_root_mean_squared_error: 4.2173\n",
            "Epoch 26/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 14.0669 - root_mean_squared_error: 3.7506 - val_loss: 12.3951 - val_root_mean_squared_error: 3.5207\n",
            "Epoch 27/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 13.6799 - root_mean_squared_error: 3.6986 - val_loss: 22.9019 - val_root_mean_squared_error: 4.7856\n",
            "Epoch 28/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 13.7305 - root_mean_squared_error: 3.7055 - val_loss: 18.0799 - val_root_mean_squared_error: 4.2520\n",
            "Epoch 29/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 14.0630 - root_mean_squared_error: 3.7501 - val_loss: 13.1941 - val_root_mean_squared_error: 3.6324\n",
            "Epoch 30/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 12.9777 - root_mean_squared_error: 3.6025 - val_loss: 11.7800 - val_root_mean_squared_error: 3.4322\n",
            "Epoch 31/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 13.9415 - root_mean_squared_error: 3.7338 - val_loss: 12.7189 - val_root_mean_squared_error: 3.5664\n",
            "Epoch 32/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 12.7866 - root_mean_squared_error: 3.5758 - val_loss: 13.4707 - val_root_mean_squared_error: 3.6702\n",
            "Epoch 33/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 13.3278 - root_mean_squared_error: 3.6507 - val_loss: 15.6410 - val_root_mean_squared_error: 3.9549\n",
            "Epoch 34/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 12.9165 - root_mean_squared_error: 3.5940 - val_loss: 13.2086 - val_root_mean_squared_error: 3.6344\n",
            "Epoch 35/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 12.7174 - root_mean_squared_error: 3.5661 - val_loss: 12.0606 - val_root_mean_squared_error: 3.4728\n",
            "Epoch 36/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 13.1146 - root_mean_squared_error: 3.6214 - val_loss: 13.8125 - val_root_mean_squared_error: 3.7165\n",
            "Epoch 37/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 13.8157 - root_mean_squared_error: 3.7169 - val_loss: 13.1933 - val_root_mean_squared_error: 3.6323\n",
            "Epoch 38/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 12.0763 - root_mean_squared_error: 3.4751 - val_loss: 10.6142 - val_root_mean_squared_error: 3.2579\n",
            "Epoch 39/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 12.2361 - root_mean_squared_error: 3.4980 - val_loss: 12.9745 - val_root_mean_squared_error: 3.6020\n",
            "Epoch 40/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 12.5952 - root_mean_squared_error: 3.5490 - val_loss: 11.3156 - val_root_mean_squared_error: 3.3639\n",
            "Epoch 41/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 12.7423 - root_mean_squared_error: 3.5696 - val_loss: 10.3472 - val_root_mean_squared_error: 3.2167\n",
            "Epoch 42/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 11.5639 - root_mean_squared_error: 3.4006 - val_loss: 15.1199 - val_root_mean_squared_error: 3.8884\n",
            "Epoch 43/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 12.1180 - root_mean_squared_error: 3.4811 - val_loss: 10.6599 - val_root_mean_squared_error: 3.2650\n",
            "Epoch 44/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 11.3164 - root_mean_squared_error: 3.3640 - val_loss: 13.8532 - val_root_mean_squared_error: 3.7220\n",
            "Epoch 45/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 11.7067 - root_mean_squared_error: 3.4215 - val_loss: 11.7395 - val_root_mean_squared_error: 3.4263\n",
            "Epoch 46/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 11.4335 - root_mean_squared_error: 3.3813 - val_loss: 11.3438 - val_root_mean_squared_error: 3.3681\n",
            "Epoch 47/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 11.4070 - root_mean_squared_error: 3.3774 - val_loss: 9.1091 - val_root_mean_squared_error: 3.0181\n",
            "Epoch 48/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 11.7930 - root_mean_squared_error: 3.4341 - val_loss: 8.4219 - val_root_mean_squared_error: 2.9021\n",
            "Epoch 49/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 11.1906 - root_mean_squared_error: 3.3452 - val_loss: 10.4606 - val_root_mean_squared_error: 3.2343\n",
            "Epoch 50/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 11.2555 - root_mean_squared_error: 3.3549 - val_loss: 11.1918 - val_root_mean_squared_error: 3.3454\n",
            "Epoch 51/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 10.6953 - root_mean_squared_error: 3.2704 - val_loss: 10.0784 - val_root_mean_squared_error: 3.1746\n",
            "Epoch 52/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 11.6194 - root_mean_squared_error: 3.4087 - val_loss: 11.7923 - val_root_mean_squared_error: 3.4340\n",
            "Epoch 53/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 11.8545 - root_mean_squared_error: 3.4430 - val_loss: 12.1364 - val_root_mean_squared_error: 3.4837\n",
            "Epoch 54/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 10.5393 - root_mean_squared_error: 3.2464 - val_loss: 15.0417 - val_root_mean_squared_error: 3.8784\n",
            "Epoch 55/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 11.5302 - root_mean_squared_error: 3.3956 - val_loss: 15.3842 - val_root_mean_squared_error: 3.9223\n",
            "Epoch 56/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 10.6582 - root_mean_squared_error: 3.2647 - val_loss: 11.2076 - val_root_mean_squared_error: 3.3478\n",
            "Epoch 57/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 10.6743 - root_mean_squared_error: 3.2672 - val_loss: 10.3170 - val_root_mean_squared_error: 3.2120\n",
            "Epoch 58/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 11.0765 - root_mean_squared_error: 3.3281 - val_loss: 12.3372 - val_root_mean_squared_error: 3.5124\n",
            "Epoch 59/200\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 10.4771 - root_mean_squared_error: 3.2368 - val_loss: 11.9758 - val_root_mean_squared_error: 3.4606\n",
            "Epoch 60/200\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 10.0933 - root_mean_squared_error: 3.1770 - val_loss: 9.0494 - val_root_mean_squared_error: 3.0082\n",
            "Epoch 61/200\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 10.4727 - root_mean_squared_error: 3.2362 - val_loss: 12.2320 - val_root_mean_squared_error: 3.4974\n",
            "Epoch 62/200\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 10.7823 - root_mean_squared_error: 3.2836 - val_loss: 11.7453 - val_root_mean_squared_error: 3.4271\n",
            "Epoch 63/200\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 10.9578 - root_mean_squared_error: 3.3103 - val_loss: 12.4500 - val_root_mean_squared_error: 3.5285\n",
            "Epoch 64/200\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 9.7944 - root_mean_squared_error: 3.1296 - val_loss: 15.8158 - val_root_mean_squared_error: 3.9769\n",
            "Epoch 65/200\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 10.8500 - root_mean_squared_error: 3.2939 - val_loss: 18.6315 - val_root_mean_squared_error: 4.3164\n",
            "Epoch 66/200\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 11.1169 - root_mean_squared_error: 3.3342 - val_loss: 11.8103 - val_root_mean_squared_error: 3.4366\n",
            "Epoch 67/200\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 9.9619 - root_mean_squared_error: 3.1562 - val_loss: 10.5115 - val_root_mean_squared_error: 3.2421\n",
            "Epoch 68/200\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 9.7141 - root_mean_squared_error: 3.1167 - val_loss: 10.1857 - val_root_mean_squared_error: 3.1915\n",
            "Epoch 69/200\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 10.0698 - root_mean_squared_error: 3.1733 - val_loss: 14.2234 - val_root_mean_squared_error: 3.7714\n",
            "Epoch 70/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 9.9957 - root_mean_squared_error: 3.1616 - val_loss: 12.2080 - val_root_mean_squared_error: 3.4940\n",
            "Epoch 71/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 9.3407 - root_mean_squared_error: 3.0563 - val_loss: 13.9498 - val_root_mean_squared_error: 3.7349\n",
            "Epoch 72/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 10.6849 - root_mean_squared_error: 3.2688 - val_loss: 13.2097 - val_root_mean_squared_error: 3.6345\n",
            "Epoch 73/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 9.7261 - root_mean_squared_error: 3.1187 - val_loss: 13.0393 - val_root_mean_squared_error: 3.6110\n",
            "Epoch 74/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 8.5504 - root_mean_squared_error: 2.9241 - val_loss: 11.6004 - val_root_mean_squared_error: 3.4059\n",
            "Epoch 75/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 9.0932 - root_mean_squared_error: 3.0155 - val_loss: 13.9554 - val_root_mean_squared_error: 3.7357\n",
            "Epoch 76/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 8.5513 - root_mean_squared_error: 2.9243 - val_loss: 14.0102 - val_root_mean_squared_error: 3.7430\n",
            "Epoch 77/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 9.7666 - root_mean_squared_error: 3.1251 - val_loss: 13.1935 - val_root_mean_squared_error: 3.6323\n",
            "Epoch 78/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 8.8734 - root_mean_squared_error: 2.9788 - val_loss: 11.1938 - val_root_mean_squared_error: 3.3457\n",
            "Epoch 79/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 8.9979 - root_mean_squared_error: 2.9997 - val_loss: 12.6222 - val_root_mean_squared_error: 3.5528\n",
            "Epoch 80/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 9.1090 - root_mean_squared_error: 3.0181 - val_loss: 13.4104 - val_root_mean_squared_error: 3.6620\n",
            "Epoch 81/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 8.2475 - root_mean_squared_error: 2.8718 - val_loss: 12.4589 - val_root_mean_squared_error: 3.5297\n",
            "Epoch 82/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 7.8924 - root_mean_squared_error: 2.8093 - val_loss: 10.6490 - val_root_mean_squared_error: 3.2633\n",
            "Epoch 83/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 7.6897 - root_mean_squared_error: 2.7730 - val_loss: 13.3136 - val_root_mean_squared_error: 3.6488\n",
            "Epoch 84/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 9.5817 - root_mean_squared_error: 3.0954 - val_loss: 9.2985 - val_root_mean_squared_error: 3.0493\n",
            "Epoch 85/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 8.9791 - root_mean_squared_error: 2.9965 - val_loss: 12.1235 - val_root_mean_squared_error: 3.4819\n",
            "Epoch 86/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 8.2275 - root_mean_squared_error: 2.8684 - val_loss: 11.7305 - val_root_mean_squared_error: 3.4250\n",
            "Epoch 87/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 7.6268 - root_mean_squared_error: 2.7617 - val_loss: 12.9061 - val_root_mean_squared_error: 3.5925\n",
            "Epoch 88/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 7.9033 - root_mean_squared_error: 2.8113 - val_loss: 10.8259 - val_root_mean_squared_error: 3.2903\n",
            "Epoch 89/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 7.4748 - root_mean_squared_error: 2.7340 - val_loss: 12.5382 - val_root_mean_squared_error: 3.5409\n",
            "Epoch 90/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 7.4466 - root_mean_squared_error: 2.7288 - val_loss: 15.5137 - val_root_mean_squared_error: 3.9387\n",
            "Epoch 91/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 7.8600 - root_mean_squared_error: 2.8036 - val_loss: 10.6847 - val_root_mean_squared_error: 3.2688\n",
            "Epoch 92/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 7.3931 - root_mean_squared_error: 2.7190 - val_loss: 12.0931 - val_root_mean_squared_error: 3.4775\n",
            "Epoch 93/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 7.3340 - root_mean_squared_error: 2.7081 - val_loss: 12.8851 - val_root_mean_squared_error: 3.5896\n",
            "Epoch 94/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 8.1826 - root_mean_squared_error: 2.8605 - val_loss: 11.0442 - val_root_mean_squared_error: 3.3233\n",
            "Epoch 95/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 7.9168 - root_mean_squared_error: 2.8137 - val_loss: 11.5911 - val_root_mean_squared_error: 3.4046\n",
            "Epoch 96/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 7.0876 - root_mean_squared_error: 2.6623 - val_loss: 14.1131 - val_root_mean_squared_error: 3.7567\n",
            "Epoch 97/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.9876 - root_mean_squared_error: 2.6434 - val_loss: 13.4958 - val_root_mean_squared_error: 3.6737\n",
            "Epoch 98/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 7.4202 - root_mean_squared_error: 2.7240 - val_loss: 14.4204 - val_root_mean_squared_error: 3.7974\n",
            "Epoch 99/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 7.3307 - root_mean_squared_error: 2.7075 - val_loss: 21.5458 - val_root_mean_squared_error: 4.6417\n",
            "Epoch 100/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 9.5446 - root_mean_squared_error: 3.0894 - val_loss: 11.6133 - val_root_mean_squared_error: 3.4078\n",
            "Epoch 101/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 8.7975 - root_mean_squared_error: 2.9660 - val_loss: 13.4828 - val_root_mean_squared_error: 3.6719\n",
            "Epoch 102/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 7.5076 - root_mean_squared_error: 2.7400 - val_loss: 15.6756 - val_root_mean_squared_error: 3.9592\n",
            "Epoch 103/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 7.1701 - root_mean_squared_error: 2.6777 - val_loss: 16.2501 - val_root_mean_squared_error: 4.0311\n",
            "Epoch 104/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 7.8319 - root_mean_squared_error: 2.7986 - val_loss: 12.7452 - val_root_mean_squared_error: 3.5700\n",
            "Epoch 105/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.9208 - root_mean_squared_error: 2.6307 - val_loss: 12.0730 - val_root_mean_squared_error: 3.4746\n",
            "Epoch 106/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.1179 - root_mean_squared_error: 2.4734 - val_loss: 12.1813 - val_root_mean_squared_error: 3.4902\n",
            "Epoch 107/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 6.7774 - root_mean_squared_error: 2.6033 - val_loss: 13.1031 - val_root_mean_squared_error: 3.6198\n",
            "Epoch 108/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 6.2193 - root_mean_squared_error: 2.4939 - val_loss: 15.0310 - val_root_mean_squared_error: 3.8770\n",
            "Epoch 109/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.4903 - root_mean_squared_error: 2.5476 - val_loss: 12.0522 - val_root_mean_squared_error: 3.4716\n",
            "Epoch 110/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 7.1336 - root_mean_squared_error: 2.6709 - val_loss: 13.2589 - val_root_mean_squared_error: 3.6413\n",
            "Epoch 111/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 7.1877 - root_mean_squared_error: 2.6810 - val_loss: 15.6205 - val_root_mean_squared_error: 3.9523\n",
            "Epoch 112/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 7.2103 - root_mean_squared_error: 2.6852 - val_loss: 12.5433 - val_root_mean_squared_error: 3.5417\n",
            "Epoch 113/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 6.8397 - root_mean_squared_error: 2.6153 - val_loss: 12.0473 - val_root_mean_squared_error: 3.4709\n",
            "Epoch 114/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.6521 - root_mean_squared_error: 2.5792 - val_loss: 12.8169 - val_root_mean_squared_error: 3.5801\n",
            "Epoch 115/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.6711 - root_mean_squared_error: 2.5828 - val_loss: 17.7125 - val_root_mean_squared_error: 4.2086\n",
            "Epoch 116/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 6.2416 - root_mean_squared_error: 2.4983 - val_loss: 11.4174 - val_root_mean_squared_error: 3.3790\n",
            "Epoch 117/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.1316 - root_mean_squared_error: 2.4762 - val_loss: 14.3894 - val_root_mean_squared_error: 3.7933\n",
            "Epoch 118/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.3585 - root_mean_squared_error: 2.5216 - val_loss: 13.3800 - val_root_mean_squared_error: 3.6579\n",
            "Epoch 119/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 6.2951 - root_mean_squared_error: 2.5090 - val_loss: 13.6428 - val_root_mean_squared_error: 3.6936\n",
            "Epoch 120/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 6.1580 - root_mean_squared_error: 2.4815 - val_loss: 14.7001 - val_root_mean_squared_error: 3.8341\n",
            "Epoch 121/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.8107 - root_mean_squared_error: 2.4105 - val_loss: 12.5106 - val_root_mean_squared_error: 3.5370\n",
            "Epoch 122/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.8800 - root_mean_squared_error: 2.4249 - val_loss: 13.1652 - val_root_mean_squared_error: 3.6284\n",
            "Epoch 123/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.9514 - root_mean_squared_error: 2.4396 - val_loss: 13.5262 - val_root_mean_squared_error: 3.6778\n",
            "Epoch 124/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.5258 - root_mean_squared_error: 2.3507 - val_loss: 12.5130 - val_root_mean_squared_error: 3.5374\n",
            "Epoch 125/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 6.1839 - root_mean_squared_error: 2.4867 - val_loss: 12.1277 - val_root_mean_squared_error: 3.4825\n",
            "Epoch 126/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 6.6173 - root_mean_squared_error: 2.5724 - val_loss: 14.2556 - val_root_mean_squared_error: 3.7757\n",
            "Epoch 127/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.5628 - root_mean_squared_error: 2.3586 - val_loss: 12.8716 - val_root_mean_squared_error: 3.5877\n",
            "Epoch 128/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.9622 - root_mean_squared_error: 2.4418 - val_loss: 13.5852 - val_root_mean_squared_error: 3.6858\n",
            "Epoch 129/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.7247 - root_mean_squared_error: 2.3926 - val_loss: 13.2130 - val_root_mean_squared_error: 3.6350\n",
            "Epoch 130/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.4666 - root_mean_squared_error: 2.3381 - val_loss: 12.3778 - val_root_mean_squared_error: 3.5182\n",
            "Epoch 131/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.2536 - root_mean_squared_error: 2.2921 - val_loss: 13.1211 - val_root_mean_squared_error: 3.6223\n",
            "Epoch 132/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.5690 - root_mean_squared_error: 2.3599 - val_loss: 10.8296 - val_root_mean_squared_error: 3.2908\n",
            "Epoch 133/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.5664 - root_mean_squared_error: 2.3593 - val_loss: 14.3178 - val_root_mean_squared_error: 3.7839\n",
            "Epoch 134/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.7778 - root_mean_squared_error: 2.4037 - val_loss: 10.7901 - val_root_mean_squared_error: 3.2848\n",
            "Epoch 135/200\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 5.0870 - root_mean_squared_error: 2.2554 - val_loss: 16.8857 - val_root_mean_squared_error: 4.1092\n",
            "Epoch 136/200\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 5.5616 - root_mean_squared_error: 2.3583 - val_loss: 15.6783 - val_root_mean_squared_error: 3.9596\n",
            "Epoch 137/200\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 6.0835 - root_mean_squared_error: 2.4665 - val_loss: 14.0085 - val_root_mean_squared_error: 3.7428\n",
            "Epoch 138/200\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 5.5335 - root_mean_squared_error: 2.3523 - val_loss: 11.1054 - val_root_mean_squared_error: 3.3325\n",
            "Epoch 139/200\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 5.7932 - root_mean_squared_error: 2.4069 - val_loss: 13.7731 - val_root_mean_squared_error: 3.7112\n",
            "Epoch 140/200\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 5.6533 - root_mean_squared_error: 2.3777 - val_loss: 10.9365 - val_root_mean_squared_error: 3.3070\n",
            "Epoch 141/200\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 5.7966 - root_mean_squared_error: 2.4076 - val_loss: 14.2122 - val_root_mean_squared_error: 3.7699\n",
            "Epoch 142/200\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 5.1418 - root_mean_squared_error: 2.2676 - val_loss: 16.3398 - val_root_mean_squared_error: 4.0423\n",
            "Epoch 143/200\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 6.6465 - root_mean_squared_error: 2.5781 - val_loss: 13.4288 - val_root_mean_squared_error: 3.6645\n",
            "Epoch 144/200\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 6.0113 - root_mean_squared_error: 2.4518 - val_loss: 12.6039 - val_root_mean_squared_error: 3.5502\n",
            "Epoch 145/200\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 5.6583 - root_mean_squared_error: 2.3787 - val_loss: 11.7358 - val_root_mean_squared_error: 3.4258\n",
            "Epoch 146/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 6.2164 - root_mean_squared_error: 2.4933 - val_loss: 13.3919 - val_root_mean_squared_error: 3.6595\n",
            "Epoch 147/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.2842 - root_mean_squared_error: 2.2987 - val_loss: 13.1461 - val_root_mean_squared_error: 3.6258\n",
            "Epoch 148/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.1576 - root_mean_squared_error: 2.2710 - val_loss: 13.5699 - val_root_mean_squared_error: 3.6837\n",
            "Epoch 149/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.2717 - root_mean_squared_error: 2.2960 - val_loss: 13.1054 - val_root_mean_squared_error: 3.6201\n",
            "Epoch 150/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.1862 - root_mean_squared_error: 2.2773 - val_loss: 14.5655 - val_root_mean_squared_error: 3.8165\n",
            "Epoch 151/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.2283 - root_mean_squared_error: 2.2865 - val_loss: 12.5053 - val_root_mean_squared_error: 3.5363\n",
            "Epoch 152/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.0310 - root_mean_squared_error: 2.4558 - val_loss: 16.2484 - val_root_mean_squared_error: 4.0309\n",
            "Epoch 153/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.1141 - root_mean_squared_error: 2.2614 - val_loss: 15.7169 - val_root_mean_squared_error: 3.9645\n",
            "Epoch 154/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.0495 - root_mean_squared_error: 2.2471 - val_loss: 13.2817 - val_root_mean_squared_error: 3.6444\n",
            "Epoch 155/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.7907 - root_mean_squared_error: 2.4064 - val_loss: 17.9834 - val_root_mean_squared_error: 4.2407\n",
            "Epoch 156/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 6.5592 - root_mean_squared_error: 2.5611 - val_loss: 12.9657 - val_root_mean_squared_error: 3.6008\n",
            "Epoch 157/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.3411 - root_mean_squared_error: 2.3111 - val_loss: 13.8824 - val_root_mean_squared_error: 3.7259\n",
            "Epoch 158/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.9805 - root_mean_squared_error: 2.4455 - val_loss: 14.4651 - val_root_mean_squared_error: 3.8033\n",
            "Epoch 159/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 4.9646 - root_mean_squared_error: 2.2281 - val_loss: 13.3237 - val_root_mean_squared_error: 3.6502\n",
            "Epoch 160/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 4.9684 - root_mean_squared_error: 2.2290 - val_loss: 11.9726 - val_root_mean_squared_error: 3.4602\n",
            "Epoch 161/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 4.9478 - root_mean_squared_error: 2.2244 - val_loss: 12.5834 - val_root_mean_squared_error: 3.5473\n",
            "Epoch 162/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.2112 - root_mean_squared_error: 2.2828 - val_loss: 15.3524 - val_root_mean_squared_error: 3.9182\n",
            "Epoch 163/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 6.0665 - root_mean_squared_error: 2.4630 - val_loss: 11.8496 - val_root_mean_squared_error: 3.4423\n",
            "Epoch 164/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.1004 - root_mean_squared_error: 2.2584 - val_loss: 12.6218 - val_root_mean_squared_error: 3.5527\n",
            "Epoch 165/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.7290 - root_mean_squared_error: 2.3935 - val_loss: 13.0707 - val_root_mean_squared_error: 3.6153\n",
            "Epoch 166/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.5432 - root_mean_squared_error: 2.3544 - val_loss: 16.5656 - val_root_mean_squared_error: 4.0701\n",
            "Epoch 167/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.7995 - root_mean_squared_error: 2.4082 - val_loss: 13.6980 - val_root_mean_squared_error: 3.7011\n",
            "Epoch 168/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 4.9562 - root_mean_squared_error: 2.2262 - val_loss: 12.2828 - val_root_mean_squared_error: 3.5047\n",
            "Epoch 169/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 4.6583 - root_mean_squared_error: 2.1583 - val_loss: 13.9437 - val_root_mean_squared_error: 3.7341\n",
            "Epoch 170/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 4.7836 - root_mean_squared_error: 2.1871 - val_loss: 11.3263 - val_root_mean_squared_error: 3.3655\n",
            "Epoch 171/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 4.7389 - root_mean_squared_error: 2.1769 - val_loss: 15.1904 - val_root_mean_squared_error: 3.8975\n",
            "Epoch 172/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 4.5361 - root_mean_squared_error: 2.1298 - val_loss: 13.8750 - val_root_mean_squared_error: 3.7249\n",
            "Epoch 173/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 4.9908 - root_mean_squared_error: 2.2340 - val_loss: 12.6094 - val_root_mean_squared_error: 3.5510\n",
            "Epoch 174/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 4.9583 - root_mean_squared_error: 2.2267 - val_loss: 14.7192 - val_root_mean_squared_error: 3.8366\n",
            "Epoch 175/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 4.8924 - root_mean_squared_error: 2.2119 - val_loss: 17.7939 - val_root_mean_squared_error: 4.2183\n",
            "Epoch 176/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.3026 - root_mean_squared_error: 2.3027 - val_loss: 10.9629 - val_root_mean_squared_error: 3.3110\n",
            "Epoch 177/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.5928 - root_mean_squared_error: 2.3649 - val_loss: 15.3997 - val_root_mean_squared_error: 3.9242\n",
            "Epoch 178/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.3235 - root_mean_squared_error: 2.3073 - val_loss: 12.7888 - val_root_mean_squared_error: 3.5761\n",
            "Epoch 179/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.4694 - root_mean_squared_error: 2.3387 - val_loss: 14.0599 - val_root_mean_squared_error: 3.7497\n",
            "Epoch 180/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 4.7689 - root_mean_squared_error: 2.1838 - val_loss: 11.0455 - val_root_mean_squared_error: 3.3235\n",
            "Epoch 181/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 4.9465 - root_mean_squared_error: 2.2241 - val_loss: 13.4159 - val_root_mean_squared_error: 3.6628\n",
            "Epoch 182/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.0083 - root_mean_squared_error: 2.2379 - val_loss: 15.1635 - val_root_mean_squared_error: 3.8940\n",
            "Epoch 183/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 4.9836 - root_mean_squared_error: 2.2324 - val_loss: 13.4581 - val_root_mean_squared_error: 3.6685\n",
            "Epoch 184/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 4.8497 - root_mean_squared_error: 2.2022 - val_loss: 13.7827 - val_root_mean_squared_error: 3.7125\n",
            "Epoch 185/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.0338 - root_mean_squared_error: 2.2436 - val_loss: 12.3282 - val_root_mean_squared_error: 3.5112\n",
            "Epoch 186/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.5562 - root_mean_squared_error: 2.3572 - val_loss: 13.2253 - val_root_mean_squared_error: 3.6367\n",
            "Epoch 187/200\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.7301 - root_mean_squared_error: 2.3938 - val_loss: 12.3014 - val_root_mean_squared_error: 3.5073\n",
            "Epoch 188/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.0863 - root_mean_squared_error: 2.2553 - val_loss: 18.6428 - val_root_mean_squared_error: 4.3177\n",
            "Epoch 189/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.7027 - root_mean_squared_error: 2.3880 - val_loss: 12.6686 - val_root_mean_squared_error: 3.5593\n",
            "Epoch 190/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.2000 - root_mean_squared_error: 2.2803 - val_loss: 13.6160 - val_root_mean_squared_error: 3.6900\n",
            "Epoch 191/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 4.8544 - root_mean_squared_error: 2.2033 - val_loss: 16.0437 - val_root_mean_squared_error: 4.0055\n",
            "Epoch 192/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 4.8060 - root_mean_squared_error: 2.1923 - val_loss: 13.4531 - val_root_mean_squared_error: 3.6678\n",
            "Epoch 193/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.1279 - root_mean_squared_error: 2.2645 - val_loss: 14.2550 - val_root_mean_squared_error: 3.7756\n",
            "Epoch 194/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 4.7257 - root_mean_squared_error: 2.1739 - val_loss: 12.7055 - val_root_mean_squared_error: 3.5645\n",
            "Epoch 195/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 4.9285 - root_mean_squared_error: 2.2200 - val_loss: 14.4731 - val_root_mean_squared_error: 3.8043\n",
            "Epoch 196/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.5570 - root_mean_squared_error: 2.3573 - val_loss: 14.5072 - val_root_mean_squared_error: 3.8088\n",
            "Epoch 197/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 4.6255 - root_mean_squared_error: 2.1507 - val_loss: 14.7468 - val_root_mean_squared_error: 3.8402\n",
            "Epoch 198/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 4.8177 - root_mean_squared_error: 2.1949 - val_loss: 12.2347 - val_root_mean_squared_error: 3.4978\n",
            "Epoch 199/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.2527 - root_mean_squared_error: 2.2919 - val_loss: 10.1808 - val_root_mean_squared_error: 3.1907\n",
            "Epoch 200/200\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 5.2920 - root_mean_squared_error: 2.3004 - val_loss: 12.8492 - val_root_mean_squared_error: 3.5846\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 12.8492 - root_mean_squared_error: 3.5846\n",
            "Root Mean Squared Error on Test Data: 3.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Assuming 'df' is your DataFrame with the provided data\n",
        "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
        "df = pd.read_csv('rice.csv')\n",
        "# Select relevant features for training\n",
        "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
        "\n",
        "# Extract features and target variable\n",
        "X = df[features]\n",
        "y = df['Yield']\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X[features] = scaler.fit_transform(X[features])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.1, random_state=42)\n",
        "\n",
        "# Build and train a simple feedforward neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(200, activation='relu', input_dim=len(features)))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Use RootMeanSquaredError as a custom metric\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=200, batch_size=8, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "# Evaluate the model\n",
        "results = model.evaluate(X_test, y_test)\n",
        "rmse_value = results[1]\n",
        "\n",
        "print(f\"Root Mean Squared Error on Test Data: {rmse_value:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "B5I5HoBt-8Iy",
        "outputId": "96c6126e-e5e5-40cb-f365-a2940c7b2764"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-e2c552c39d62>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[features] = scaler.fit_transform(X[features])\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "65/65 [==============================] - 1s 6ms/step - loss: 1.5504 - root_mean_squared_error: 1.2452 - val_loss: 0.6041 - val_root_mean_squared_error: 0.7772\n",
            "Epoch 2/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.5726 - root_mean_squared_error: 0.7567 - val_loss: 0.5124 - val_root_mean_squared_error: 0.7158\n",
            "Epoch 3/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.5112 - root_mean_squared_error: 0.7150 - val_loss: 0.4732 - val_root_mean_squared_error: 0.6879\n",
            "Epoch 4/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.5230 - root_mean_squared_error: 0.7232 - val_loss: 0.5483 - val_root_mean_squared_error: 0.7405\n",
            "Epoch 5/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.4930 - root_mean_squared_error: 0.7021 - val_loss: 0.4959 - val_root_mean_squared_error: 0.7042\n",
            "Epoch 6/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.5101 - root_mean_squared_error: 0.7142 - val_loss: 0.4490 - val_root_mean_squared_error: 0.6701\n",
            "Epoch 7/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.4474 - root_mean_squared_error: 0.6689 - val_loss: 0.4863 - val_root_mean_squared_error: 0.6973\n",
            "Epoch 8/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.4265 - root_mean_squared_error: 0.6530 - val_loss: 0.4329 - val_root_mean_squared_error: 0.6579\n",
            "Epoch 9/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.4375 - root_mean_squared_error: 0.6615 - val_loss: 0.4923 - val_root_mean_squared_error: 0.7017\n",
            "Epoch 10/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4215 - root_mean_squared_error: 0.6492 - val_loss: 0.4521 - val_root_mean_squared_error: 0.6724\n",
            "Epoch 11/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.4072 - root_mean_squared_error: 0.6381 - val_loss: 0.5766 - val_root_mean_squared_error: 0.7594\n",
            "Epoch 12/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4300 - root_mean_squared_error: 0.6558 - val_loss: 0.4652 - val_root_mean_squared_error: 0.6820\n",
            "Epoch 13/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3977 - root_mean_squared_error: 0.6307 - val_loss: 0.3952 - val_root_mean_squared_error: 0.6287\n",
            "Epoch 14/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3991 - root_mean_squared_error: 0.6317 - val_loss: 0.4195 - val_root_mean_squared_error: 0.6477\n",
            "Epoch 15/200\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.3764 - root_mean_squared_error: 0.6135 - val_loss: 0.3971 - val_root_mean_squared_error: 0.6302\n",
            "Epoch 16/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.3897 - root_mean_squared_error: 0.6243 - val_loss: 0.3858 - val_root_mean_squared_error: 0.6211\n",
            "Epoch 17/200\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.3808 - root_mean_squared_error: 0.6171 - val_loss: 0.3918 - val_root_mean_squared_error: 0.6259\n",
            "Epoch 18/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.3940 - root_mean_squared_error: 0.6277 - val_loss: 0.4144 - val_root_mean_squared_error: 0.6438\n",
            "Epoch 19/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.3521 - root_mean_squared_error: 0.5934 - val_loss: 0.3869 - val_root_mean_squared_error: 0.6220\n",
            "Epoch 20/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.3732 - root_mean_squared_error: 0.6109 - val_loss: 0.3807 - val_root_mean_squared_error: 0.6170\n",
            "Epoch 21/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.3718 - root_mean_squared_error: 0.6097 - val_loss: 0.3660 - val_root_mean_squared_error: 0.6050\n",
            "Epoch 22/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3773 - root_mean_squared_error: 0.6142 - val_loss: 0.3727 - val_root_mean_squared_error: 0.6105\n",
            "Epoch 23/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3410 - root_mean_squared_error: 0.5839 - val_loss: 0.3776 - val_root_mean_squared_error: 0.6145\n",
            "Epoch 24/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3237 - root_mean_squared_error: 0.5689 - val_loss: 0.3746 - val_root_mean_squared_error: 0.6121\n",
            "Epoch 25/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3330 - root_mean_squared_error: 0.5771 - val_loss: 0.4208 - val_root_mean_squared_error: 0.6487\n",
            "Epoch 26/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3376 - root_mean_squared_error: 0.5811 - val_loss: 0.3656 - val_root_mean_squared_error: 0.6046\n",
            "Epoch 27/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3052 - root_mean_squared_error: 0.5525 - val_loss: 0.3529 - val_root_mean_squared_error: 0.5941\n",
            "Epoch 28/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3683 - root_mean_squared_error: 0.6069 - val_loss: 0.3835 - val_root_mean_squared_error: 0.6193\n",
            "Epoch 29/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3432 - root_mean_squared_error: 0.5859 - val_loss: 0.3891 - val_root_mean_squared_error: 0.6238\n",
            "Epoch 30/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3014 - root_mean_squared_error: 0.5490 - val_loss: 0.3273 - val_root_mean_squared_error: 0.5721\n",
            "Epoch 31/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2942 - root_mean_squared_error: 0.5424 - val_loss: 0.3812 - val_root_mean_squared_error: 0.6174\n",
            "Epoch 32/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2928 - root_mean_squared_error: 0.5411 - val_loss: 0.3698 - val_root_mean_squared_error: 0.6081\n",
            "Epoch 33/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3059 - root_mean_squared_error: 0.5531 - val_loss: 0.3350 - val_root_mean_squared_error: 0.5788\n",
            "Epoch 34/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3020 - root_mean_squared_error: 0.5495 - val_loss: 0.3387 - val_root_mean_squared_error: 0.5820\n",
            "Epoch 35/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2984 - root_mean_squared_error: 0.5462 - val_loss: 0.3479 - val_root_mean_squared_error: 0.5898\n",
            "Epoch 36/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2750 - root_mean_squared_error: 0.5244 - val_loss: 0.3464 - val_root_mean_squared_error: 0.5886\n",
            "Epoch 37/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2959 - root_mean_squared_error: 0.5440 - val_loss: 0.3668 - val_root_mean_squared_error: 0.6057\n",
            "Epoch 38/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2642 - root_mean_squared_error: 0.5141 - val_loss: 0.3616 - val_root_mean_squared_error: 0.6013\n",
            "Epoch 39/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2716 - root_mean_squared_error: 0.5211 - val_loss: 0.3480 - val_root_mean_squared_error: 0.5899\n",
            "Epoch 40/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2786 - root_mean_squared_error: 0.5279 - val_loss: 0.3908 - val_root_mean_squared_error: 0.6251\n",
            "Epoch 41/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2772 - root_mean_squared_error: 0.5265 - val_loss: 0.3625 - val_root_mean_squared_error: 0.6021\n",
            "Epoch 42/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2645 - root_mean_squared_error: 0.5142 - val_loss: 0.3801 - val_root_mean_squared_error: 0.6165\n",
            "Epoch 43/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2535 - root_mean_squared_error: 0.5035 - val_loss: 0.3738 - val_root_mean_squared_error: 0.6114\n",
            "Epoch 44/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.2486 - root_mean_squared_error: 0.4986 - val_loss: 0.3408 - val_root_mean_squared_error: 0.5837\n",
            "Epoch 45/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2608 - root_mean_squared_error: 0.5107 - val_loss: 0.3460 - val_root_mean_squared_error: 0.5882\n",
            "Epoch 46/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2357 - root_mean_squared_error: 0.4855 - val_loss: 0.3538 - val_root_mean_squared_error: 0.5948\n",
            "Epoch 47/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2245 - root_mean_squared_error: 0.4738 - val_loss: 0.3876 - val_root_mean_squared_error: 0.6226\n",
            "Epoch 48/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2261 - root_mean_squared_error: 0.4755 - val_loss: 0.3890 - val_root_mean_squared_error: 0.6237\n",
            "Epoch 49/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2269 - root_mean_squared_error: 0.4763 - val_loss: 0.4094 - val_root_mean_squared_error: 0.6399\n",
            "Epoch 50/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2211 - root_mean_squared_error: 0.4702 - val_loss: 0.3253 - val_root_mean_squared_error: 0.5703\n",
            "Epoch 51/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2241 - root_mean_squared_error: 0.4733 - val_loss: 0.3808 - val_root_mean_squared_error: 0.6171\n",
            "Epoch 52/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2085 - root_mean_squared_error: 0.4566 - val_loss: 0.3775 - val_root_mean_squared_error: 0.6144\n",
            "Epoch 53/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2306 - root_mean_squared_error: 0.4802 - val_loss: 0.4151 - val_root_mean_squared_error: 0.6443\n",
            "Epoch 54/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2253 - root_mean_squared_error: 0.4747 - val_loss: 0.4199 - val_root_mean_squared_error: 0.6480\n",
            "Epoch 55/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2331 - root_mean_squared_error: 0.4828 - val_loss: 0.3826 - val_root_mean_squared_error: 0.6185\n",
            "Epoch 56/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2099 - root_mean_squared_error: 0.4581 - val_loss: 0.4022 - val_root_mean_squared_error: 0.6342\n",
            "Epoch 57/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2232 - root_mean_squared_error: 0.4725 - val_loss: 0.3733 - val_root_mean_squared_error: 0.6110\n",
            "Epoch 58/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2074 - root_mean_squared_error: 0.4554 - val_loss: 0.3829 - val_root_mean_squared_error: 0.6188\n",
            "Epoch 59/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1913 - root_mean_squared_error: 0.4374 - val_loss: 0.3233 - val_root_mean_squared_error: 0.5686\n",
            "Epoch 60/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.1963 - root_mean_squared_error: 0.4430 - val_loss: 0.3478 - val_root_mean_squared_error: 0.5898\n",
            "Epoch 61/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1694 - root_mean_squared_error: 0.4116 - val_loss: 0.4077 - val_root_mean_squared_error: 0.6385\n",
            "Epoch 62/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2119 - root_mean_squared_error: 0.4603 - val_loss: 0.3258 - val_root_mean_squared_error: 0.5708\n",
            "Epoch 63/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1780 - root_mean_squared_error: 0.4220 - val_loss: 0.3511 - val_root_mean_squared_error: 0.5925\n",
            "Epoch 64/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1770 - root_mean_squared_error: 0.4207 - val_loss: 0.3754 - val_root_mean_squared_error: 0.6127\n",
            "Epoch 65/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1729 - root_mean_squared_error: 0.4159 - val_loss: 0.3609 - val_root_mean_squared_error: 0.6008\n",
            "Epoch 66/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1843 - root_mean_squared_error: 0.4292 - val_loss: 0.3475 - val_root_mean_squared_error: 0.5895\n",
            "Epoch 67/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2030 - root_mean_squared_error: 0.4505 - val_loss: 0.3485 - val_root_mean_squared_error: 0.5903\n",
            "Epoch 68/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1670 - root_mean_squared_error: 0.4087 - val_loss: 0.3369 - val_root_mean_squared_error: 0.5804\n",
            "Epoch 69/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.1702 - root_mean_squared_error: 0.4126 - val_loss: 0.3572 - val_root_mean_squared_error: 0.5977\n",
            "Epoch 70/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.1795 - root_mean_squared_error: 0.4237 - val_loss: 0.3623 - val_root_mean_squared_error: 0.6019\n",
            "Epoch 71/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.1955 - root_mean_squared_error: 0.4422 - val_loss: 0.3366 - val_root_mean_squared_error: 0.5801\n",
            "Epoch 72/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.1474 - root_mean_squared_error: 0.3839 - val_loss: 0.3192 - val_root_mean_squared_error: 0.5650\n",
            "Epoch 73/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.1508 - root_mean_squared_error: 0.3884 - val_loss: 0.3733 - val_root_mean_squared_error: 0.6110\n",
            "Epoch 74/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.1796 - root_mean_squared_error: 0.4238 - val_loss: 0.3490 - val_root_mean_squared_error: 0.5907\n",
            "Epoch 75/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.1545 - root_mean_squared_error: 0.3930 - val_loss: 0.3488 - val_root_mean_squared_error: 0.5906\n",
            "Epoch 76/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.1371 - root_mean_squared_error: 0.3702 - val_loss: 0.3758 - val_root_mean_squared_error: 0.6131\n",
            "Epoch 77/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.1483 - root_mean_squared_error: 0.3850 - val_loss: 0.4014 - val_root_mean_squared_error: 0.6335\n",
            "Epoch 78/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1450 - root_mean_squared_error: 0.3808 - val_loss: 0.3574 - val_root_mean_squared_error: 0.5978\n",
            "Epoch 79/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1429 - root_mean_squared_error: 0.3781 - val_loss: 0.3066 - val_root_mean_squared_error: 0.5537\n",
            "Epoch 80/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1333 - root_mean_squared_error: 0.3652 - val_loss: 0.3137 - val_root_mean_squared_error: 0.5601\n",
            "Epoch 81/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1327 - root_mean_squared_error: 0.3642 - val_loss: 0.3193 - val_root_mean_squared_error: 0.5650\n",
            "Epoch 82/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1570 - root_mean_squared_error: 0.3962 - val_loss: 0.2925 - val_root_mean_squared_error: 0.5409\n",
            "Epoch 83/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1406 - root_mean_squared_error: 0.3749 - val_loss: 0.3440 - val_root_mean_squared_error: 0.5865\n",
            "Epoch 84/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1312 - root_mean_squared_error: 0.3622 - val_loss: 0.3885 - val_root_mean_squared_error: 0.6233\n",
            "Epoch 85/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1286 - root_mean_squared_error: 0.3586 - val_loss: 0.3335 - val_root_mean_squared_error: 0.5775\n",
            "Epoch 86/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1317 - root_mean_squared_error: 0.3628 - val_loss: 0.2916 - val_root_mean_squared_error: 0.5400\n",
            "Epoch 87/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1167 - root_mean_squared_error: 0.3417 - val_loss: 0.2874 - val_root_mean_squared_error: 0.5361\n",
            "Epoch 88/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0985 - root_mean_squared_error: 0.3138 - val_loss: 0.3571 - val_root_mean_squared_error: 0.5976\n",
            "Epoch 89/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1213 - root_mean_squared_error: 0.3483 - val_loss: 0.3125 - val_root_mean_squared_error: 0.5590\n",
            "Epoch 90/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1071 - root_mean_squared_error: 0.3272 - val_loss: 0.3907 - val_root_mean_squared_error: 0.6250\n",
            "Epoch 91/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1193 - root_mean_squared_error: 0.3454 - val_loss: 0.3034 - val_root_mean_squared_error: 0.5508\n",
            "Epoch 92/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1155 - root_mean_squared_error: 0.3399 - val_loss: 0.3586 - val_root_mean_squared_error: 0.5989\n",
            "Epoch 93/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1206 - root_mean_squared_error: 0.3473 - val_loss: 0.2831 - val_root_mean_squared_error: 0.5321\n",
            "Epoch 94/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1136 - root_mean_squared_error: 0.3370 - val_loss: 0.3287 - val_root_mean_squared_error: 0.5733\n",
            "Epoch 95/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1147 - root_mean_squared_error: 0.3386 - val_loss: 0.2754 - val_root_mean_squared_error: 0.5248\n",
            "Epoch 96/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1141 - root_mean_squared_error: 0.3378 - val_loss: 0.3329 - val_root_mean_squared_error: 0.5770\n",
            "Epoch 97/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1138 - root_mean_squared_error: 0.3374 - val_loss: 0.3127 - val_root_mean_squared_error: 0.5592\n",
            "Epoch 98/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1157 - root_mean_squared_error: 0.3402 - val_loss: 0.3280 - val_root_mean_squared_error: 0.5727\n",
            "Epoch 99/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1076 - root_mean_squared_error: 0.3280 - val_loss: 0.2799 - val_root_mean_squared_error: 0.5291\n",
            "Epoch 100/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0959 - root_mean_squared_error: 0.3096 - val_loss: 0.3062 - val_root_mean_squared_error: 0.5533\n",
            "Epoch 101/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1110 - root_mean_squared_error: 0.3332 - val_loss: 0.2747 - val_root_mean_squared_error: 0.5241\n",
            "Epoch 102/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1033 - root_mean_squared_error: 0.3213 - val_loss: 0.3137 - val_root_mean_squared_error: 0.5601\n",
            "Epoch 103/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1046 - root_mean_squared_error: 0.3234 - val_loss: 0.3501 - val_root_mean_squared_error: 0.5917\n",
            "Epoch 104/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1085 - root_mean_squared_error: 0.3294 - val_loss: 0.2890 - val_root_mean_squared_error: 0.5376\n",
            "Epoch 105/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0966 - root_mean_squared_error: 0.3107 - val_loss: 0.2674 - val_root_mean_squared_error: 0.5171\n",
            "Epoch 106/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1294 - root_mean_squared_error: 0.3597 - val_loss: 0.2897 - val_root_mean_squared_error: 0.5383\n",
            "Epoch 107/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0912 - root_mean_squared_error: 0.3020 - val_loss: 0.2647 - val_root_mean_squared_error: 0.5145\n",
            "Epoch 108/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1000 - root_mean_squared_error: 0.3162 - val_loss: 0.2771 - val_root_mean_squared_error: 0.5264\n",
            "Epoch 109/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1276 - root_mean_squared_error: 0.3572 - val_loss: 0.2863 - val_root_mean_squared_error: 0.5351\n",
            "Epoch 110/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1373 - root_mean_squared_error: 0.3706 - val_loss: 0.3236 - val_root_mean_squared_error: 0.5689\n",
            "Epoch 111/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1174 - root_mean_squared_error: 0.3427 - val_loss: 0.3041 - val_root_mean_squared_error: 0.5514\n",
            "Epoch 112/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0928 - root_mean_squared_error: 0.3047 - val_loss: 0.2721 - val_root_mean_squared_error: 0.5216\n",
            "Epoch 113/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0920 - root_mean_squared_error: 0.3033 - val_loss: 0.3012 - val_root_mean_squared_error: 0.5488\n",
            "Epoch 114/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1027 - root_mean_squared_error: 0.3204 - val_loss: 0.2842 - val_root_mean_squared_error: 0.5332\n",
            "Epoch 115/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0961 - root_mean_squared_error: 0.3100 - val_loss: 0.2825 - val_root_mean_squared_error: 0.5315\n",
            "Epoch 116/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0824 - root_mean_squared_error: 0.2871 - val_loss: 0.2551 - val_root_mean_squared_error: 0.5050\n",
            "Epoch 117/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0794 - root_mean_squared_error: 0.2818 - val_loss: 0.2458 - val_root_mean_squared_error: 0.4958\n",
            "Epoch 118/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0794 - root_mean_squared_error: 0.2818 - val_loss: 0.3915 - val_root_mean_squared_error: 0.6257\n",
            "Epoch 119/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1198 - root_mean_squared_error: 0.3461 - val_loss: 0.3104 - val_root_mean_squared_error: 0.5572\n",
            "Epoch 120/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0831 - root_mean_squared_error: 0.2883 - val_loss: 0.3302 - val_root_mean_squared_error: 0.5746\n",
            "Epoch 121/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0895 - root_mean_squared_error: 0.2992 - val_loss: 0.2978 - val_root_mean_squared_error: 0.5457\n",
            "Epoch 122/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0831 - root_mean_squared_error: 0.2883 - val_loss: 0.2491 - val_root_mean_squared_error: 0.4991\n",
            "Epoch 123/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0917 - root_mean_squared_error: 0.3028 - val_loss: 0.2465 - val_root_mean_squared_error: 0.4964\n",
            "Epoch 124/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0974 - root_mean_squared_error: 0.3121 - val_loss: 0.3211 - val_root_mean_squared_error: 0.5666\n",
            "Epoch 125/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1008 - root_mean_squared_error: 0.3175 - val_loss: 0.2974 - val_root_mean_squared_error: 0.5454\n",
            "Epoch 126/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.1281 - root_mean_squared_error: 0.3579 - val_loss: 0.2432 - val_root_mean_squared_error: 0.4931\n",
            "Epoch 127/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.0971 - root_mean_squared_error: 0.3115 - val_loss: 0.2519 - val_root_mean_squared_error: 0.5019\n",
            "Epoch 128/200\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.0728 - root_mean_squared_error: 0.2699 - val_loss: 0.2773 - val_root_mean_squared_error: 0.5266\n",
            "Epoch 129/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.0781 - root_mean_squared_error: 0.2794 - val_loss: 0.3413 - val_root_mean_squared_error: 0.5842\n",
            "Epoch 130/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.0923 - root_mean_squared_error: 0.3038 - val_loss: 0.2864 - val_root_mean_squared_error: 0.5351\n",
            "Epoch 131/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.0929 - root_mean_squared_error: 0.3049 - val_loss: 0.2724 - val_root_mean_squared_error: 0.5219\n",
            "Epoch 132/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.0730 - root_mean_squared_error: 0.2703 - val_loss: 0.2714 - val_root_mean_squared_error: 0.5210\n",
            "Epoch 133/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0728 - root_mean_squared_error: 0.2698 - val_loss: 0.2867 - val_root_mean_squared_error: 0.5354\n",
            "Epoch 134/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0783 - root_mean_squared_error: 0.2798 - val_loss: 0.2849 - val_root_mean_squared_error: 0.5337\n",
            "Epoch 135/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0820 - root_mean_squared_error: 0.2864 - val_loss: 0.3045 - val_root_mean_squared_error: 0.5518\n",
            "Epoch 136/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0738 - root_mean_squared_error: 0.2716 - val_loss: 0.2730 - val_root_mean_squared_error: 0.5225\n",
            "Epoch 137/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1110 - root_mean_squared_error: 0.3332 - val_loss: 0.2658 - val_root_mean_squared_error: 0.5156\n",
            "Epoch 138/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0937 - root_mean_squared_error: 0.3062 - val_loss: 0.3145 - val_root_mean_squared_error: 0.5608\n",
            "Epoch 139/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0696 - root_mean_squared_error: 0.2639 - val_loss: 0.2781 - val_root_mean_squared_error: 0.5274\n",
            "Epoch 140/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0772 - root_mean_squared_error: 0.2778 - val_loss: 0.3020 - val_root_mean_squared_error: 0.5495\n",
            "Epoch 141/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0774 - root_mean_squared_error: 0.2782 - val_loss: 0.2714 - val_root_mean_squared_error: 0.5210\n",
            "Epoch 142/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0954 - root_mean_squared_error: 0.3089 - val_loss: 0.3470 - val_root_mean_squared_error: 0.5891\n",
            "Epoch 143/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0756 - root_mean_squared_error: 0.2749 - val_loss: 0.2818 - val_root_mean_squared_error: 0.5308\n",
            "Epoch 144/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0731 - root_mean_squared_error: 0.2704 - val_loss: 0.2955 - val_root_mean_squared_error: 0.5436\n",
            "Epoch 145/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0707 - root_mean_squared_error: 0.2659 - val_loss: 0.3116 - val_root_mean_squared_error: 0.5582\n",
            "Epoch 146/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0846 - root_mean_squared_error: 0.2909 - val_loss: 0.2647 - val_root_mean_squared_error: 0.5145\n",
            "Epoch 147/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0808 - root_mean_squared_error: 0.2842 - val_loss: 0.2841 - val_root_mean_squared_error: 0.5330\n",
            "Epoch 148/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0880 - root_mean_squared_error: 0.2967 - val_loss: 0.2584 - val_root_mean_squared_error: 0.5083\n",
            "Epoch 149/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1581 - root_mean_squared_error: 0.3976 - val_loss: 0.2498 - val_root_mean_squared_error: 0.4998\n",
            "Epoch 150/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1098 - root_mean_squared_error: 0.3313 - val_loss: 0.3047 - val_root_mean_squared_error: 0.5520\n",
            "Epoch 151/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1086 - root_mean_squared_error: 0.3296 - val_loss: 0.2495 - val_root_mean_squared_error: 0.4995\n",
            "Epoch 152/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0936 - root_mean_squared_error: 0.3059 - val_loss: 0.3058 - val_root_mean_squared_error: 0.5530\n",
            "Epoch 153/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0814 - root_mean_squared_error: 0.2852 - val_loss: 0.2492 - val_root_mean_squared_error: 0.4992\n",
            "Epoch 154/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0803 - root_mean_squared_error: 0.2834 - val_loss: 0.3027 - val_root_mean_squared_error: 0.5502\n",
            "Epoch 155/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1069 - root_mean_squared_error: 0.3270 - val_loss: 0.2861 - val_root_mean_squared_error: 0.5349\n",
            "Epoch 156/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1145 - root_mean_squared_error: 0.3384 - val_loss: 0.2938 - val_root_mean_squared_error: 0.5420\n",
            "Epoch 157/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0814 - root_mean_squared_error: 0.2853 - val_loss: 0.2733 - val_root_mean_squared_error: 0.5228\n",
            "Epoch 158/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0682 - root_mean_squared_error: 0.2612 - val_loss: 0.2946 - val_root_mean_squared_error: 0.5428\n",
            "Epoch 159/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0612 - root_mean_squared_error: 0.2474 - val_loss: 0.2542 - val_root_mean_squared_error: 0.5042\n",
            "Epoch 160/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0613 - root_mean_squared_error: 0.2477 - val_loss: 0.2842 - val_root_mean_squared_error: 0.5331\n",
            "Epoch 161/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0655 - root_mean_squared_error: 0.2559 - val_loss: 0.2430 - val_root_mean_squared_error: 0.4929\n",
            "Epoch 162/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0606 - root_mean_squared_error: 0.2461 - val_loss: 0.2889 - val_root_mean_squared_error: 0.5375\n",
            "Epoch 163/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0613 - root_mean_squared_error: 0.2475 - val_loss: 0.2737 - val_root_mean_squared_error: 0.5232\n",
            "Epoch 164/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0603 - root_mean_squared_error: 0.2456 - val_loss: 0.2746 - val_root_mean_squared_error: 0.5240\n",
            "Epoch 165/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0590 - root_mean_squared_error: 0.2429 - val_loss: 0.2857 - val_root_mean_squared_error: 0.5345\n",
            "Epoch 166/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0595 - root_mean_squared_error: 0.2440 - val_loss: 0.2674 - val_root_mean_squared_error: 0.5171\n",
            "Epoch 167/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0627 - root_mean_squared_error: 0.2505 - val_loss: 0.2662 - val_root_mean_squared_error: 0.5159\n",
            "Epoch 168/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0605 - root_mean_squared_error: 0.2459 - val_loss: 0.2948 - val_root_mean_squared_error: 0.5430\n",
            "Epoch 169/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0564 - root_mean_squared_error: 0.2375 - val_loss: 0.2720 - val_root_mean_squared_error: 0.5215\n",
            "Epoch 170/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0639 - root_mean_squared_error: 0.2528 - val_loss: 0.2900 - val_root_mean_squared_error: 0.5386\n",
            "Epoch 171/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0760 - root_mean_squared_error: 0.2757 - val_loss: 0.2745 - val_root_mean_squared_error: 0.5239\n",
            "Epoch 172/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0728 - root_mean_squared_error: 0.2697 - val_loss: 0.2949 - val_root_mean_squared_error: 0.5431\n",
            "Epoch 173/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0903 - root_mean_squared_error: 0.3005 - val_loss: 0.2576 - val_root_mean_squared_error: 0.5076\n",
            "Epoch 174/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0727 - root_mean_squared_error: 0.2696 - val_loss: 0.2354 - val_root_mean_squared_error: 0.4851\n",
            "Epoch 175/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0621 - root_mean_squared_error: 0.2492 - val_loss: 0.2914 - val_root_mean_squared_error: 0.5398\n",
            "Epoch 176/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0641 - root_mean_squared_error: 0.2533 - val_loss: 0.2068 - val_root_mean_squared_error: 0.4548\n",
            "Epoch 177/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1099 - root_mean_squared_error: 0.3315 - val_loss: 0.3115 - val_root_mean_squared_error: 0.5581\n",
            "Epoch 178/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0684 - root_mean_squared_error: 0.2616 - val_loss: 0.2618 - val_root_mean_squared_error: 0.5116\n",
            "Epoch 179/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0778 - root_mean_squared_error: 0.2789 - val_loss: 0.2731 - val_root_mean_squared_error: 0.5226\n",
            "Epoch 180/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0711 - root_mean_squared_error: 0.2666 - val_loss: 0.2679 - val_root_mean_squared_error: 0.5176\n",
            "Epoch 181/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.0796 - root_mean_squared_error: 0.2822 - val_loss: 0.2902 - val_root_mean_squared_error: 0.5387\n",
            "Epoch 182/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.0660 - root_mean_squared_error: 0.2568 - val_loss: 0.2043 - val_root_mean_squared_error: 0.4520\n",
            "Epoch 183/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.1002 - root_mean_squared_error: 0.3165 - val_loss: 0.3822 - val_root_mean_squared_error: 0.6182\n",
            "Epoch 184/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.1424 - root_mean_squared_error: 0.3774 - val_loss: 0.3392 - val_root_mean_squared_error: 0.5824\n",
            "Epoch 185/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.0988 - root_mean_squared_error: 0.3143 - val_loss: 0.2754 - val_root_mean_squared_error: 0.5247\n",
            "Epoch 186/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.0760 - root_mean_squared_error: 0.2756 - val_loss: 0.4255 - val_root_mean_squared_error: 0.6523\n",
            "Epoch 187/200\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.0661 - root_mean_squared_error: 0.2570 - val_loss: 0.2415 - val_root_mean_squared_error: 0.4914\n",
            "Epoch 188/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0620 - root_mean_squared_error: 0.2490 - val_loss: 0.2596 - val_root_mean_squared_error: 0.5095\n",
            "Epoch 189/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0556 - root_mean_squared_error: 0.2358 - val_loss: 0.2584 - val_root_mean_squared_error: 0.5083\n",
            "Epoch 190/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0647 - root_mean_squared_error: 0.2545 - val_loss: 0.2162 - val_root_mean_squared_error: 0.4649\n",
            "Epoch 191/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0611 - root_mean_squared_error: 0.2471 - val_loss: 0.2515 - val_root_mean_squared_error: 0.5015\n",
            "Epoch 192/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0545 - root_mean_squared_error: 0.2335 - val_loss: 0.2590 - val_root_mean_squared_error: 0.5089\n",
            "Epoch 193/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0548 - root_mean_squared_error: 0.2341 - val_loss: 0.2416 - val_root_mean_squared_error: 0.4916\n",
            "Epoch 194/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0580 - root_mean_squared_error: 0.2409 - val_loss: 0.2672 - val_root_mean_squared_error: 0.5169\n",
            "Epoch 195/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0702 - root_mean_squared_error: 0.2649 - val_loss: 0.2124 - val_root_mean_squared_error: 0.4608\n",
            "Epoch 196/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0671 - root_mean_squared_error: 0.2590 - val_loss: 0.2233 - val_root_mean_squared_error: 0.4725\n",
            "Epoch 197/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0605 - root_mean_squared_error: 0.2460 - val_loss: 0.2550 - val_root_mean_squared_error: 0.5050\n",
            "Epoch 198/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0729 - root_mean_squared_error: 0.2701 - val_loss: 0.2301 - val_root_mean_squared_error: 0.4797\n",
            "Epoch 199/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0750 - root_mean_squared_error: 0.2738 - val_loss: 0.2825 - val_root_mean_squared_error: 0.5315\n",
            "Epoch 200/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0586 - root_mean_squared_error: 0.2422 - val_loss: 0.2689 - val_root_mean_squared_error: 0.5185\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2689 - root_mean_squared_error: 0.5185\n",
            "Root Mean Squared Error on Test Data: 0.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "\n",
        "# Assuming 'df' is your DataFrame with the provided data\n",
        "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
        "\n",
        "# Select relevant features for training\n",
        "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
        "\n",
        "# Extract features and target variable\n",
        "X = df[features]\n",
        "y = df['Yield']\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X[features] = scaler.fit_transform(X[features])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build and train a simple feedforward neural network model\n",
        "def build_model(activation='relu', neurons_per_layer=100):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(neurons_per_layer, activation=activation, input_dim=len(features)))\n",
        "    model.add(Dense(neurons_per_layer, activation=activation))\n",
        "    model.add(Dense(neurons_per_layer, activation=activation))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Create the KerasRegressor object\n",
        "keras_regressor = KerasRegressor(build_fn=build_model, epochs=200, batch_size=16, verbose=0,activation='relu',neurons_per_layer=50)\n",
        "\n",
        "# Define the parameters to tune\n",
        "param_grid = {\n",
        "    'activation': ['relu', 'sigmoid', 'tanh'],\n",
        "    'neurons_per_layer': [50, 100, 150],\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(estimator=keras_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Best Activation: {grid_search.best_params_['activation']}\")\n",
        "print(f\"Best Neurons per Layer: {grid_search.best_params_['neurons_per_layer']}\")\n",
        "print(f\"Mean Squared Error on Test Data: {mse:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "S-t3uG6bppMd",
        "outputId": "a1913c6c-9428-4c06-e5f4-2a15985a25c7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-d4d9f5ab3040>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[features] = scaler.fit_transform(X[features])\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Activation: sigmoid\n",
            "Best Neurons per Layer: 50\n",
            "Mean Squared Error on Test Data: 71.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gParqrmEqb0M",
        "outputId": "b0cd6f97-ec92-4fc9-fae7-7109df2fe146"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.12.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (23.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.2.0)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "import tensorflow as tf\n",
        "\n",
        "# Assuming 'df' is your DataFrame with the provided data\n",
        "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
        "\n",
        "# Select relevant features for training\n",
        "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
        "\n",
        "# Extract features and target variable\n",
        "X = df[features]\n",
        "y = df['Yield']\n",
        "\n",
        "# Normalize numerical features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X.loc[:, features] = scaler.fit_transform(X.loc[:, features])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a function to create the Keras model\n",
        "def create_model(dropout_rate=0.0):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(200, activation='relu', input_dim=len(features)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Create a KerasRegressor with the defined model\n",
        "model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=10, dropout_rate=0.2)\n",
        "\n",
        "# Perform Grid Search\n",
        "param_grid = {'dropout_rate': [0.0, 0.2, 0.5]}\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_dropout_rate = grid_result.best_params_['dropout_rate']\n",
        "print(f\"Best Dropout Rate: {best_dropout_rate}\")\n",
        "\n",
        "# Predict using the best model\n",
        "best_model = grid_result.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the best model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error on Test Data: {mse:.2f}\")\n",
        "\n",
        "# Display some predictions and actual values\n",
        "for i in range(10):  # Displaying first 10 predictions for illustration\n",
        "    print(f\"Prediction: {y_pred[i]:.2f}, Actual: {y_test[i]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YTXq-tonqCeA",
        "outputId": "224f2fd5-2977-4669-8dbd-016d54176a42"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-c1bbf753ee51>:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X.loc[:, features] = scaler.fit_transform(X.loc[:, features])\n",
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 1s 3ms/step - loss: 860.6007\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 794.5109\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 705.3574\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 646.0703\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 634.2474\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 623.5331\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 622.1644\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 613.6783\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 619.0889\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 613.4775\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 602.7654\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 604.2783\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 600.1053\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 590.2908\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 586.9760\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 585.5665\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 573.7950\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 571.7435\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 565.4012\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 557.3942\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 551.4961\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 547.8047\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 541.5800\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 535.3988\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 523.4062\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 517.8260\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 511.0360\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 507.4303\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 491.9868\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 496.2197\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 475.3648\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 470.8948\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 467.6385\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 454.7716\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 454.8865\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 448.6428\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 435.7332\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 425.4616\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 414.4933\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 408.3310\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 394.1448\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 395.9918\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 395.7467\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 387.4348\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 368.3691\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 351.1289\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 356.7018\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 341.3026\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 345.9233\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 328.8136\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 309.6128\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 298.1270\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 285.0254\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 300.1182\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 269.5114\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 270.7900\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 255.4889\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 240.1969\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 241.8929\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 221.1680\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 221.9617\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 214.5140\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 201.1967\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 188.2592\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 187.1795\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 173.2074\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 175.1729\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 171.3585\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 155.2532\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 148.5543\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 148.0158\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 134.9078\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 138.3521\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 142.5081\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 131.0322\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 124.4958\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 119.3185\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 100.1069\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 109.3588\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 92.6632\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 92.2298\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 107.0793\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 91.1340\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 85.1687\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 77.1920\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 73.5548\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 72.3297\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 74.9098\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 64.6199\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 60.5647\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 72.5396\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 48.3348\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 68.6367\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 58.5936\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 64.4042\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 62.3589\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 55.4720\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 51.6141\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 55.3210\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 46.7546\n",
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 1s 3ms/step - loss: 1873.1605\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1794.4407\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1680.6615\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1567.9703\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1516.8708\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1486.8517\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1476.7311\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1468.9812\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1467.4480\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1456.3337\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1453.6848\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1455.5054\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1446.8844\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1445.1663\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1439.1852\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1425.8096\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1424.7672\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1418.8903\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1414.2473\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1407.8578\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1396.7791\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1394.7629\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1392.8630\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1385.3160\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1367.0533\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1371.0223\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1353.9404\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1357.3623\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1342.9613\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1327.1520\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1316.1268\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1308.7882\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1309.2792\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1291.4008\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1280.3438\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1269.3997\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1256.7515\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1259.6885\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1272.6605\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1257.9762\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1221.6049\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1215.8904\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1203.7614\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1213.3607\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1154.6749\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1162.0004\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1143.4218\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1129.2325\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1105.0973\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1110.6675\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1115.5769\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1095.6136\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1058.5375\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1033.6628\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1017.5228\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1009.1426\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 979.0272\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 985.8621\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 950.3254\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 937.8346\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 921.4326\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 908.0799\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 913.2946\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 863.5580\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 888.4170\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 877.1530\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 844.0791\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 818.1932\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 816.5014\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 831.7513\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 830.0367\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 754.0477\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 746.9600\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 737.6442\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 714.7249\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 723.4759\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 678.9568\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 741.6823\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 648.7866\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 700.2125\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 640.9276\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 624.8259\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 613.6645\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 593.3611\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 639.8099\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 599.6119\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 565.0264\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 566.2869\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 530.9257\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 533.7347\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 482.1396\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 513.1620\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 503.9891\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 451.0201\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 453.5956\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 414.6037\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 403.8990\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 406.7739\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 391.4011\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 381.3057\n",
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 1s 3ms/step - loss: 1149.9832\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1101.1880\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1013.3171\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 958.7178\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 927.6738\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 916.5134\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 911.1880\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 905.7482\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 908.6902\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 916.0750\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 900.1812\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 893.7755\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 894.5544\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 885.4572\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 888.1435\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 881.2689\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 879.1096\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 879.0780\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 876.0951\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 873.1325\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 865.6498\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 866.9496\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 864.3131\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 852.7201\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 855.6947\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 847.8655\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 839.8811\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 836.4982\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 825.6315\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 820.5186\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 817.2079\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 830.9230\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 797.9067\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 804.2087\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 779.0326\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 792.6238\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 763.6697\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 759.0824\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 747.0081\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 742.5009\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 745.7369\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 722.4373\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 711.0567\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 706.8539\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 683.9229\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 672.7810\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 656.7611\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 657.0640\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 649.6860\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 618.3536\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 614.4944\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 599.3785\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 599.8302\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 555.0400\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 547.4247\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 551.7612\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 502.4718\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 489.9265\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 480.7704\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 448.6427\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 424.8928\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 409.3755\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 457.2050\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 318.5707\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 391.5997\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 334.8123\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 339.5652\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 303.5246\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 278.1519\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 241.8001\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 234.9023\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 196.3911\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 176.1582\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 158.6603\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 159.8453\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 145.5413\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 136.9914\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 113.0635\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 169.5579\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 74.8325\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 137.0706\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 75.0616\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 69.3428\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 59.0607\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 70.6377\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 50.0476\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 41.1479\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 41.3102\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 35.7800\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 33.1096\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 31.5998\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 31.3701\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 29.2830\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 28.6908\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 27.8999\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 28.0474\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 28.7147\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 26.4586\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 26.0816\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 25.7663\n",
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 1s 3ms/step - loss: 865.7783\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 802.7183\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 728.8000\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 657.8668\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 638.5908\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 627.1617\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 616.8627\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 617.9850\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 611.5765\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 612.2122\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 621.8281\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 602.6986\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 602.4468\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 601.0433\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 601.5961\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 596.7615\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 589.6397\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 584.8502\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 581.4252\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 572.5549\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 566.4213\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 573.0081\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 560.6582\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 560.6635\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 552.3688\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 542.6910\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 552.8354\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 550.6420\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 538.4943\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 539.5000\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 529.2211\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 524.0824\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 506.7300\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 523.2754\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 499.7029\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 516.7634\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 489.0929\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 500.8680\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 490.9070\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 472.7768\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 456.2237\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 476.8000\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 450.5807\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 476.6718\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 483.4574\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 434.7389\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 416.9296\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 400.8042\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 401.0657\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 438.9184\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 398.6269\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 424.1073\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 397.2667\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 379.9157\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 390.6840\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 420.4896\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 339.4814\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 365.1576\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 350.7369\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 330.8651\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 359.3924\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 375.0327\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 363.2158\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 307.5756\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 321.0001\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 341.1289\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 304.8200\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 280.1116\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 290.5383\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 297.6655\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 231.9782\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 304.5222\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 271.2813\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 242.6239\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 237.1984\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 220.6772\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 215.1167\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 197.1527\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 252.5429\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 193.2578\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 196.5259\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 219.9194\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 220.9688\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 222.5436\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 185.5274\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 170.8635\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 219.5626\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 135.2593\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 165.5450\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 130.4992\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 201.1014\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 122.5651\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 196.4885\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 125.3075\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 133.3423\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 128.6090\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 134.3415\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 115.7396\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 109.6447\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 161.2217\n",
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 1s 4ms/step - loss: 1878.8440\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1826.9841\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1728.8922\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1623.8505\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1563.2988\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1506.7202\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1487.0450\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1483.8430\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1475.9523\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1467.2290\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1468.2428\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1452.4373\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1450.9142\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1457.9874\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1434.3739\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1423.6157\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1429.8101\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1446.1605\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1402.7488\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1411.3945\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1418.7303\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1417.8687\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1406.6959\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1404.1749\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1393.8090\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1380.4470\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1375.3224\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1385.4604\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1396.5840\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1368.9934\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1347.8270\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1387.3180\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1377.5315\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1362.1882\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1330.4348\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1353.2561\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1339.9304\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1346.7358\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1334.5411\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1324.4502\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1300.4130\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1313.5663\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1304.7671\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1270.1403\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1271.1312\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1275.5789\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1270.9834\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1285.3365\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1238.4359\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1274.9458\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1236.7629\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1184.9724\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1216.8243\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1257.7839\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1142.5160\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1188.5863\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1143.8074\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1133.0345\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1164.8889\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1153.4650\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1077.6154\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1154.5940\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1088.7562\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1073.0249\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1104.2933\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1085.6490\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1119.3285\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1042.8127\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1006.3799\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1016.9323\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 987.9298\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1019.2070\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 970.7330\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 978.9818\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 965.5231\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 973.8802\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 967.0201\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1014.1404\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 905.7616\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 879.4811\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1008.6238\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 894.7275\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 894.1146\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 913.2426\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 861.5685\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 765.5468\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 919.5419\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 929.2538\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 803.5456\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 830.2889\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 848.0035\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 797.5655\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 783.2142\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 699.1608\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 753.2260\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 730.2014\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 741.5006\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 725.2219\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 658.1279\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 677.5951\n",
            "8/8 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 1s 3ms/step - loss: 1163.0424\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1123.8381\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1064.1719\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 971.1306\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 962.2925\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 922.6039\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 907.0562\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 904.2538\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 898.3693\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 889.2618\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 892.9319\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 914.4445\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 888.7951\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 890.9145\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 888.2404\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 873.6463\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 878.7137\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 881.2808\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 866.9517\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 856.4734\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 862.3889\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 869.2743\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 859.3134\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 866.0074\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 854.1440\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 839.7898\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 848.7321\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 829.1882\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 824.0134\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 843.4874\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 823.5547\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 808.2802\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 824.2332\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 799.5618\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 786.5135\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 800.3930\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 781.4636\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 771.8926\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 758.7799\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 782.5791\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 717.2326\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 749.5140\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 720.9705\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 708.9167\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 703.6155\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 649.5295\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 671.2128\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 650.8442\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 655.9809\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 647.4367\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 608.4286\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 568.7141\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 633.4481\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 612.4675\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 561.8625\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 544.8786\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 595.6873\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 515.2153\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 479.3229\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 479.7088\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 440.8521\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 450.3022\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 468.2141\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 454.4330\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 423.2902\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 420.7999\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 367.1620\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 383.6212\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 334.5673\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 331.6195\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 304.3712\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 295.7354\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 278.9447\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 243.1260\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 178.1272\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 282.7195\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 190.0914\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 167.6664\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 187.2688\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 194.7661\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 110.9752\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 127.2307\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 137.7817\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 78.9680\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 154.9686\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 123.3456\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 117.6093\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 63.7382\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 54.7842\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 105.5408\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 60.4564\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 64.4999\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 74.8010\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 123.3795\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 59.4852\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 71.6545\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 45.5498\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 31.5939\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 52.2504\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 56.9428\n",
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 1s 3ms/step - loss: 866.1733\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 819.4229\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 744.8335\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 688.0776\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 635.2924\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 630.6184\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 612.8768\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 618.5776\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 610.9131\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 632.7944\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 606.3408\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 626.4541\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 608.1951\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 595.1296\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 617.3239\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 583.0344\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 606.5307\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 605.6334\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 567.3698\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 596.6215\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 575.0292\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 573.6520\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 569.4041\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 592.7346\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 584.3757\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 551.7540\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 557.7674\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 572.5891\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 530.9991\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 582.4912\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 531.4100\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 564.8870\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 555.0447\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 554.3854\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 527.0361\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 544.5349\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 535.4803\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 509.1808\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 518.5074\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 506.0380\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 525.1688\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 545.7189\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 493.4152\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 519.0052\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 490.4322\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 514.1300\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 461.4603\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 533.9621\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 486.8941\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 468.8888\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 514.0272\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 487.3925\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 473.4013\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 471.6540\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 448.7057\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 445.2122\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 484.5799\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 434.8493\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 447.1628\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 408.2952\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 405.9348\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 433.0565\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 412.3237\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 441.2913\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 390.8104\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 391.4655\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 447.0067\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 447.7373\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 394.5162\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 411.6436\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 373.4370\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 328.5558\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 306.2825\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 315.5653\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 340.2455\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 301.9102\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 407.1290\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 365.7350\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 293.0365\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 293.3171\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 307.8706\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 274.4535\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 267.9802\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 311.2221\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 277.4550\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 291.1243\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 271.2955\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 191.7719\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 343.9746\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 184.9984\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 330.2999\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 318.7343\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 222.8128\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 193.1074\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 311.6503\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 232.0923\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 247.4074\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 241.4115\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 116.1448\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 494.1633\n",
            "8/8 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 1s 3ms/step - loss: 1874.1710\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1796.5182\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1697.3262\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1591.2898\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1560.8660\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1484.1042\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1482.7784\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1491.7362\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1464.2791\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1456.1355\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1434.0646\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1444.1458\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1441.7211\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1476.0847\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1445.1182\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1441.2067\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1460.1255\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1426.6871\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1427.7412\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1442.9812\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1434.1488\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1417.1508\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1434.1617\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1464.7334\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1400.5884\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1420.9526\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1391.8420\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1429.5402\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1407.2209\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1380.7855\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1396.9233\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1377.6910\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1376.1926\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1369.2083\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1423.5316\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1427.3326\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1402.3071\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1372.7032\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1357.4020\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1321.0884\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1366.5670\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1344.0952\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1342.7603\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1377.0175\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1350.7996\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1363.2715\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1351.8782\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1372.0377\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1378.4850\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1303.5127\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1271.1149\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1322.9612\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1302.4574\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1275.9008\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1208.3822\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1202.2574\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1283.6654\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1204.8231\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1227.3363\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1316.3314\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1243.5413\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1195.5266\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1223.9774\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1310.4850\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1190.8188\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1273.1721\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1198.5129\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1174.4736\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1121.0272\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1174.6533\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1104.8878\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1202.6017\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1113.7031\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1195.8657\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1157.1248\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1119.6680\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1035.2058\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1105.9364\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1139.8374\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 953.0726\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1082.9651\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1024.8910\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 920.7537\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 897.2412\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1033.7708\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1114.4114\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1015.3920\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 975.3192\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 989.4553\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 881.7916\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 872.4170\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1041.1549\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 833.9346\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 946.6025\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1088.4292\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1040.7168\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 799.4080\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 756.2764\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 836.1864\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 896.8842\n",
            "8/8 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 1s 3ms/step - loss: 1157.4531\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1117.2802\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1036.4647\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 986.6718\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 945.0473\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 949.6551\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 933.6068\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 918.7070\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 924.0489\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 901.0939\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 938.6075\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 923.2807\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 883.1282\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 929.5309\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 899.0497\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 891.3339\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 882.3861\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 871.5596\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 884.0939\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 897.8828\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 928.0698\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 892.6323\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 888.2106\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 909.1185\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 873.9783\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 872.3765\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 848.7271\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 893.3475\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 847.3284\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 901.7020\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 883.3130\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 880.5761\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 862.7041\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 823.0653\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 809.9345\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 836.1788\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 872.8580\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 868.8935\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 804.4002\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 813.0701\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 883.0770\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 817.5016\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 855.4875\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 836.6170\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 806.2460\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 793.3116\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 801.2032\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 811.4761\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 796.8713\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 796.8470\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 847.4370\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 824.8436\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 777.9116\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 730.3859\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 708.2689\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 791.2806\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 765.2814\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 795.7430\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 776.5251\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 796.8434\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 720.9031\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 762.0546\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 746.6757\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 701.1196\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 757.8507\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 783.8201\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 634.8017\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 668.4142\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 643.1876\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 677.2462\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 732.9411\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 672.1341\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 586.7991\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 605.6498\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 564.0301\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 617.3021\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 532.9280\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 695.1501\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 557.7480\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 467.1562\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 466.4041\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 548.9118\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 610.9918\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 553.5076\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 552.9667\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 587.7232\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 580.3652\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 383.7849\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 462.2268\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 414.3374\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 353.1767\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 446.5516\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 323.5944\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 306.8386\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 388.0201\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 249.8385\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 412.1762\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 351.5767\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 365.0886\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 384.0473\n",
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "23/23 [==============================] - 1s 3ms/step - loss: 1292.5875\n",
            "Epoch 2/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1195.5820\n",
            "Epoch 3/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1060.1400\n",
            "Epoch 4/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1020.8372\n",
            "Epoch 5/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1018.5196\n",
            "Epoch 6/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1002.0115\n",
            "Epoch 7/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1001.3535\n",
            "Epoch 8/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 998.8874\n",
            "Epoch 9/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1002.1343\n",
            "Epoch 10/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1002.6359\n",
            "Epoch 11/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 999.4821\n",
            "Epoch 12/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 983.9821\n",
            "Epoch 13/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 992.9233\n",
            "Epoch 14/100\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 996.2972\n",
            "Epoch 15/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 979.9767\n",
            "Epoch 16/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 993.0999\n",
            "Epoch 17/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 977.6276\n",
            "Epoch 18/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 995.5281\n",
            "Epoch 19/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 974.3986\n",
            "Epoch 20/100\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 980.6614\n",
            "Epoch 21/100\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 965.2223\n",
            "Epoch 22/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 974.3378\n",
            "Epoch 23/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 957.6702\n",
            "Epoch 24/100\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 945.5881\n",
            "Epoch 25/100\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 967.9930\n",
            "Epoch 26/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 959.5687\n",
            "Epoch 27/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 945.0129\n",
            "Epoch 28/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 947.4780\n",
            "Epoch 29/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 926.0626\n",
            "Epoch 30/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1001.9774\n",
            "Epoch 31/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 986.2796\n",
            "Epoch 32/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 945.5211\n",
            "Epoch 33/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 930.5452\n",
            "Epoch 34/100\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 937.9344\n",
            "Epoch 35/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 922.8093\n",
            "Epoch 36/100\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 922.6489\n",
            "Epoch 37/100\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 902.4435\n",
            "Epoch 38/100\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 891.7625\n",
            "Epoch 39/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 870.9379\n",
            "Epoch 40/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 902.3341\n",
            "Epoch 41/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 897.8735\n",
            "Epoch 42/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 884.7168\n",
            "Epoch 43/100\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 875.7310\n",
            "Epoch 44/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 838.8301\n",
            "Epoch 45/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 862.1050\n",
            "Epoch 46/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 838.9254\n",
            "Epoch 47/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 837.1592\n",
            "Epoch 48/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 840.2101\n",
            "Epoch 49/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 834.0538\n",
            "Epoch 50/100\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 830.2715\n",
            "Epoch 51/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 814.4342\n",
            "Epoch 52/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 810.0741\n",
            "Epoch 53/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 845.0968\n",
            "Epoch 54/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 782.3006\n",
            "Epoch 55/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 777.4194\n",
            "Epoch 56/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 749.6575\n",
            "Epoch 57/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 747.6819\n",
            "Epoch 58/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 731.5773\n",
            "Epoch 59/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 756.5714\n",
            "Epoch 60/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 745.7504\n",
            "Epoch 61/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 723.7261\n",
            "Epoch 62/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 745.3264\n",
            "Epoch 63/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 736.8920\n",
            "Epoch 64/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 717.0051\n",
            "Epoch 65/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 705.1714\n",
            "Epoch 66/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 676.4254\n",
            "Epoch 67/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 700.6170\n",
            "Epoch 68/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 651.2536\n",
            "Epoch 69/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 696.1067\n",
            "Epoch 70/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 650.2769\n",
            "Epoch 71/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 621.8210\n",
            "Epoch 72/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 624.9108\n",
            "Epoch 73/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 627.1088\n",
            "Epoch 74/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 652.3727\n",
            "Epoch 75/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 616.7930\n",
            "Epoch 76/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 630.7444\n",
            "Epoch 77/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 586.5526\n",
            "Epoch 78/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 583.5812\n",
            "Epoch 79/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 540.9521\n",
            "Epoch 80/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 558.3393\n",
            "Epoch 81/100\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 626.5016\n",
            "Epoch 82/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 648.1994\n",
            "Epoch 83/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 567.4243\n",
            "Epoch 84/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 443.8030\n",
            "Epoch 85/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 533.3418\n",
            "Epoch 86/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 577.1562\n",
            "Epoch 87/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 482.5369\n",
            "Epoch 88/100\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 420.4872\n",
            "Epoch 89/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 510.9631\n",
            "Epoch 90/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 442.0227\n",
            "Epoch 91/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 423.2198\n",
            "Epoch 92/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 352.6454\n",
            "Epoch 93/100\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 463.7551\n",
            "Epoch 94/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 657.4707\n",
            "Epoch 95/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 515.6281\n",
            "Epoch 96/100\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 408.0309\n",
            "Epoch 97/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 600.4321\n",
            "Epoch 98/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 638.5472\n",
            "Epoch 99/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 531.6021\n",
            "Epoch 100/100\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 428.1566\n",
            "Best Dropout Rate: 0.2\n",
            "6/6 [==============================] - 0s 2ms/step\n",
            "Mean Squared Error on Test Data: 60.12\n",
            "Prediction: 10.80, Actual: 8.55\n",
            "Prediction: 15.04, Actual: 14.46\n",
            "Prediction: 15.67, Actual: 8.50\n",
            "Prediction: 11.98, Actual: 5.00\n",
            "Prediction: 9.03, Actual: 2.79\n",
            "Prediction: 7.30, Actual: 8.01\n",
            "Prediction: 12.72, Actual: 14.75\n",
            "Prediction: 14.78, Actual: 8.03\n",
            "Prediction: 0.57, Actual: 2.21\n",
            "Prediction: 8.25, Actual: 5.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# Assuming 'df' is your DataFrame with the provided data\n",
        "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
        "\n",
        "# Select relevant features for training\n",
        "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall','Fertilizer','Pesticide']\n",
        "\n",
        "# Extract features and target variable\n",
        "X = df[features]\n",
        "y = df['Yield']\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X[features] = scaler.fit_transform(X[features])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build and train a simple feedforward neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(200, activation='relu', input_dim=len(features)))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
        "\n",
        "model.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_test, y_test))\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error on Test Data: {mse:.2f}\")\n",
        "\n",
        "# Display some predictions and actual values\n",
        "for i in range(10):  # Displaying first 10 predictions for illustration\n",
        "    print(f\"Prediction: {y_pred[i][0]:.2f}, Actual: {y_test[i]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc0GJAD587yV",
        "outputId": "b2ea5a84-d135-4fc1-a120-d153978c5b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-a98f792b2160>:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[features] = scaler.fit_transform(X[features])\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "14/14 [==============================] - 1s 16ms/step - loss: 1303.2655 - val_loss: 190.9877\n",
            "Epoch 2/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1253.1028 - val_loss: 141.8612\n",
            "Epoch 3/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1158.1248 - val_loss: 94.6541\n",
            "Epoch 4/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1078.5468 - val_loss: 79.4581\n",
            "Epoch 5/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 1029.3947 - val_loss: 70.1851\n",
            "Epoch 6/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 1013.0273 - val_loss: 79.7723\n",
            "Epoch 7/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 1022.9358 - val_loss: 87.6269\n",
            "Epoch 8/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 1014.2403 - val_loss: 60.0829\n",
            "Epoch 9/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1001.5395 - val_loss: 70.3170\n",
            "Epoch 10/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 998.0538 - val_loss: 75.3206\n",
            "Epoch 11/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 999.2281 - val_loss: 92.3593\n",
            "Epoch 12/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 995.0279 - val_loss: 78.2544\n",
            "Epoch 13/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 996.5621 - val_loss: 63.5275\n",
            "Epoch 14/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 991.8740 - val_loss: 71.8741\n",
            "Epoch 15/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 992.6302 - val_loss: 64.8703\n",
            "Epoch 16/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 998.1931 - val_loss: 87.7226\n",
            "Epoch 17/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 986.9263 - val_loss: 70.0605\n",
            "Epoch 18/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 982.7723 - val_loss: 72.0591\n",
            "Epoch 19/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 985.0615 - val_loss: 59.7545\n",
            "Epoch 20/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 977.2451 - val_loss: 82.9356\n",
            "Epoch 21/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 975.5212 - val_loss: 76.2921\n",
            "Epoch 22/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 973.6339 - val_loss: 65.3411\n",
            "Epoch 23/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 970.7582 - val_loss: 74.1589\n",
            "Epoch 24/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 968.2862 - val_loss: 78.1635\n",
            "Epoch 25/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 968.8069 - val_loss: 61.8841\n",
            "Epoch 26/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 963.1978 - val_loss: 77.6003\n",
            "Epoch 27/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 958.3305 - val_loss: 77.5793\n",
            "Epoch 28/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 954.7703 - val_loss: 73.5762\n",
            "Epoch 29/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 955.5813 - val_loss: 66.0478\n",
            "Epoch 30/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 949.4450 - val_loss: 65.5498\n",
            "Epoch 31/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 948.6205 - val_loss: 78.3786\n",
            "Epoch 32/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 947.8811 - val_loss: 73.8784\n",
            "Epoch 33/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 945.2809 - val_loss: 66.9788\n",
            "Epoch 34/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 937.3636 - val_loss: 74.5245\n",
            "Epoch 35/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 933.9412 - val_loss: 69.9755\n",
            "Epoch 36/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 926.8400 - val_loss: 75.1678\n",
            "Epoch 37/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 939.2739 - val_loss: 79.9933\n",
            "Epoch 38/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 919.6622 - val_loss: 68.0609\n",
            "Epoch 39/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 920.6329 - val_loss: 71.9743\n",
            "Epoch 40/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 918.9604 - val_loss: 87.5027\n",
            "Epoch 41/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 908.8303 - val_loss: 71.4113\n",
            "Epoch 42/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 914.1755 - val_loss: 63.4883\n",
            "Epoch 43/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 918.4991 - val_loss: 96.4437\n",
            "Epoch 44/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 896.5766 - val_loss: 77.2185\n",
            "Epoch 45/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 895.4480 - val_loss: 84.6117\n",
            "Epoch 46/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 890.6232 - val_loss: 78.1547\n",
            "Epoch 47/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 880.9452 - val_loss: 81.8250\n",
            "Epoch 48/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 882.5057 - val_loss: 91.8317\n",
            "Epoch 49/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 876.5303 - val_loss: 97.8635\n",
            "Epoch 50/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 877.3035 - val_loss: 78.8380\n",
            "Epoch 51/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 871.8420 - val_loss: 105.3424\n",
            "Epoch 52/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 860.4288 - val_loss: 92.1052\n",
            "Epoch 53/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 853.2673 - val_loss: 85.3133\n",
            "Epoch 54/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 852.9233 - val_loss: 96.6931\n",
            "Epoch 55/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 850.3427 - val_loss: 85.0475\n",
            "Epoch 56/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 848.2642 - val_loss: 108.1186\n",
            "Epoch 57/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 833.9630 - val_loss: 93.6290\n",
            "Epoch 58/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 828.6682 - val_loss: 101.1892\n",
            "Epoch 59/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 822.1168 - val_loss: 101.5522\n",
            "Epoch 60/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 817.0828 - val_loss: 98.6863\n",
            "Epoch 61/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 810.5515 - val_loss: 105.6367\n",
            "Epoch 62/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 805.5161 - val_loss: 107.0115\n",
            "Epoch 63/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 803.2908 - val_loss: 98.0992\n",
            "Epoch 64/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 795.0475 - val_loss: 93.8677\n",
            "Epoch 65/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 788.9513 - val_loss: 120.8808\n",
            "Epoch 66/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 784.5576 - val_loss: 105.7020\n",
            "Epoch 67/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 771.1747 - val_loss: 107.4256\n",
            "Epoch 68/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 773.1141 - val_loss: 115.1490\n",
            "Epoch 69/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 770.1451 - val_loss: 96.2745\n",
            "Epoch 70/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 753.6414 - val_loss: 108.6174\n",
            "Epoch 71/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 753.6711 - val_loss: 98.6506\n",
            "Epoch 72/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 755.1426 - val_loss: 112.9469\n",
            "Epoch 73/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 734.2421 - val_loss: 103.9986\n",
            "Epoch 74/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 729.3289 - val_loss: 107.5791\n",
            "Epoch 75/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 716.4816 - val_loss: 92.1689\n",
            "Epoch 76/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 709.6996 - val_loss: 123.9588\n",
            "Epoch 77/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 712.0311 - val_loss: 81.5681\n",
            "Epoch 78/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 684.6940 - val_loss: 137.9987\n",
            "Epoch 79/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 694.6949 - val_loss: 106.4752\n",
            "Epoch 80/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 676.0530 - val_loss: 132.3179\n",
            "Epoch 81/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 683.8818 - val_loss: 94.8909\n",
            "Epoch 82/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 660.5173 - val_loss: 109.2695\n",
            "Epoch 83/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 680.5629 - val_loss: 115.6011\n",
            "Epoch 84/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 660.2487 - val_loss: 90.7552\n",
            "Epoch 85/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 638.0826 - val_loss: 118.2620\n",
            "Epoch 86/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 636.8362 - val_loss: 110.8713\n",
            "Epoch 87/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 609.5919 - val_loss: 80.2260\n",
            "Epoch 88/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 624.4077 - val_loss: 91.4513\n",
            "Epoch 89/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 644.7620 - val_loss: 109.3551\n",
            "Epoch 90/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 608.7421 - val_loss: 83.4365\n",
            "Epoch 91/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 590.2062 - val_loss: 75.0118\n",
            "Epoch 92/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 574.0549 - val_loss: 126.8924\n",
            "Epoch 93/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 569.5158 - val_loss: 89.7678\n",
            "Epoch 94/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 575.3021 - val_loss: 72.3602\n",
            "Epoch 95/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 535.6091 - val_loss: 99.6739\n",
            "Epoch 96/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 532.3142 - val_loss: 92.5887\n",
            "Epoch 97/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 520.2204 - val_loss: 82.0601\n",
            "Epoch 98/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 500.3578 - val_loss: 96.3736\n",
            "Epoch 99/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 520.4410 - val_loss: 73.5266\n",
            "Epoch 100/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 512.5763 - val_loss: 93.2408\n",
            "Epoch 101/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 484.9832 - val_loss: 67.8101\n",
            "Epoch 102/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 473.6571 - val_loss: 91.4337\n",
            "Epoch 103/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 466.8284 - val_loss: 60.2954\n",
            "Epoch 104/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 468.7263 - val_loss: 70.5282\n",
            "Epoch 105/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 439.6968 - val_loss: 78.9336\n",
            "Epoch 106/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 420.5688 - val_loss: 71.4123\n",
            "Epoch 107/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 433.6334 - val_loss: 65.9150\n",
            "Epoch 108/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 416.0714 - val_loss: 77.5753\n",
            "Epoch 109/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 408.1517 - val_loss: 67.1391\n",
            "Epoch 110/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 383.9920 - val_loss: 86.5042\n",
            "Epoch 111/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 364.8727 - val_loss: 53.2430\n",
            "Epoch 112/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 384.8687 - val_loss: 67.0607\n",
            "Epoch 113/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 354.3054 - val_loss: 68.5339\n",
            "Epoch 114/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 341.1478 - val_loss: 61.5541\n",
            "Epoch 115/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 336.8180 - val_loss: 67.4065\n",
            "Epoch 116/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 322.2816 - val_loss: 63.4399\n",
            "Epoch 117/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 326.0184 - val_loss: 62.3158\n",
            "Epoch 118/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 310.7685 - val_loss: 58.4001\n",
            "Epoch 119/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 295.6631 - val_loss: 60.7035\n",
            "Epoch 120/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 319.6073 - val_loss: 70.5725\n",
            "Epoch 121/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 347.9932 - val_loss: 50.3719\n",
            "Epoch 122/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 265.5232 - val_loss: 80.7306\n",
            "Epoch 123/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 274.6535 - val_loss: 64.8968\n",
            "Epoch 124/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 256.6850 - val_loss: 57.5285\n",
            "Epoch 125/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 252.1062 - val_loss: 54.8656\n",
            "Epoch 126/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 267.2455 - val_loss: 56.2107\n",
            "Epoch 127/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 233.4409 - val_loss: 55.0389\n",
            "Epoch 128/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 221.2639 - val_loss: 54.4498\n",
            "Epoch 129/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 227.0712 - val_loss: 57.3170\n",
            "Epoch 130/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 222.8065 - val_loss: 59.2859\n",
            "Epoch 131/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 199.7428 - val_loss: 50.2278\n",
            "Epoch 132/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 203.3193 - val_loss: 56.3930\n",
            "Epoch 133/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 213.4674 - val_loss: 44.8776\n",
            "Epoch 134/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 186.4714 - val_loss: 49.7293\n",
            "Epoch 135/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 170.7278 - val_loss: 56.2900\n",
            "Epoch 136/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 171.9687 - val_loss: 51.9804\n",
            "Epoch 137/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 182.5219 - val_loss: 46.4423\n",
            "Epoch 138/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 152.5998 - val_loss: 56.4101\n",
            "Epoch 139/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 150.5975 - val_loss: 54.1420\n",
            "Epoch 140/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 154.6714 - val_loss: 46.6382\n",
            "Epoch 141/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 132.9342 - val_loss: 48.0358\n",
            "Epoch 142/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 142.8816 - val_loss: 51.3088\n",
            "Epoch 143/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 153.8165 - val_loss: 53.5344\n",
            "Epoch 144/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 151.1388 - val_loss: 53.4557\n",
            "Epoch 145/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 119.6741 - val_loss: 45.2669\n",
            "Epoch 146/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 123.5422 - val_loss: 45.7584\n",
            "Epoch 147/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 107.3494 - val_loss: 46.0417\n",
            "Epoch 148/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 108.5383 - val_loss: 45.5523\n",
            "Epoch 149/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 95.3544 - val_loss: 44.6461\n",
            "Epoch 150/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 100.6720 - val_loss: 45.5072\n",
            "Epoch 151/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 99.4285 - val_loss: 42.1407\n",
            "Epoch 152/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 82.8512 - val_loss: 50.2208\n",
            "Epoch 153/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 90.6393 - val_loss: 45.6488\n",
            "Epoch 154/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 85.4764 - val_loss: 44.1766\n",
            "Epoch 155/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 75.0547 - val_loss: 45.3828\n",
            "Epoch 156/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 81.8169 - val_loss: 48.3343\n",
            "Epoch 157/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 67.2059 - val_loss: 49.3299\n",
            "Epoch 158/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 68.9445 - val_loss: 49.3925\n",
            "Epoch 159/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 56.3244 - val_loss: 43.3751\n",
            "Epoch 160/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 68.9318 - val_loss: 44.4595\n",
            "Epoch 161/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 52.9784 - val_loss: 48.1679\n",
            "Epoch 162/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 58.1791 - val_loss: 49.1036\n",
            "Epoch 163/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 52.1482 - val_loss: 46.2717\n",
            "Epoch 164/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 64.1447 - val_loss: 45.5744\n",
            "Epoch 165/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 49.3779 - val_loss: 45.2489\n",
            "Epoch 166/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 48.4925 - val_loss: 47.3004\n",
            "Epoch 167/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 49.9603 - val_loss: 43.5302\n",
            "Epoch 168/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 48.8645 - val_loss: 49.8776\n",
            "Epoch 169/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 48.0089 - val_loss: 43.3546\n",
            "Epoch 170/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 47.0624 - val_loss: 45.4028\n",
            "Epoch 171/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 38.0356 - val_loss: 44.7666\n",
            "Epoch 172/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 43.5371 - val_loss: 42.0069\n",
            "Epoch 173/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 42.3557 - val_loss: 46.7431\n",
            "Epoch 174/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 28.5257 - val_loss: 41.8202\n",
            "Epoch 175/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 41.9066 - val_loss: 45.8524\n",
            "Epoch 176/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 32.6520 - val_loss: 44.8120\n",
            "Epoch 177/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 32.2285 - val_loss: 42.6238\n",
            "Epoch 178/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 30.4962 - val_loss: 43.2157\n",
            "Epoch 179/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 30.4454 - val_loss: 43.8587\n",
            "Epoch 180/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 29.8325 - val_loss: 42.3753\n",
            "Epoch 181/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 26.8573 - val_loss: 42.5781\n",
            "Epoch 182/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 27.9755 - val_loss: 42.3053\n",
            "Epoch 183/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25.8754 - val_loss: 42.0024\n",
            "Epoch 184/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 27.8353 - val_loss: 42.3670\n",
            "Epoch 185/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 24.2397 - val_loss: 43.6356\n",
            "Epoch 186/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25.6888 - val_loss: 44.5724\n",
            "Epoch 187/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 28.9068 - val_loss: 42.8158\n",
            "Epoch 188/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 27.3537 - val_loss: 41.8159\n",
            "Epoch 189/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 28.7661 - val_loss: 42.0355\n",
            "Epoch 190/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25.2011 - val_loss: 41.3876\n",
            "Epoch 191/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25.9319 - val_loss: 49.1734\n",
            "Epoch 192/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 26.1415 - val_loss: 41.4224\n",
            "Epoch 193/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 28.6678 - val_loss: 45.6709\n",
            "Epoch 194/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 25.2455 - val_loss: 41.5416\n",
            "Epoch 195/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 22.5156 - val_loss: 42.1221\n",
            "Epoch 196/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 24.1677 - val_loss: 41.9523\n",
            "Epoch 197/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 21.4680 - val_loss: 41.8142\n",
            "Epoch 198/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 21.0593 - val_loss: 41.3712\n",
            "Epoch 199/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 20.7036 - val_loss: 42.0573\n",
            "Epoch 200/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 20.7270 - val_loss: 41.3585\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Mean Squared Error on Test Data: 41.36\n",
            "Prediction: 10.29, Actual: 8.55\n",
            "Prediction: 14.82, Actual: 14.46\n",
            "Prediction: 15.28, Actual: 8.50\n",
            "Prediction: 10.13, Actual: 5.00\n",
            "Prediction: 8.15, Actual: 2.79\n",
            "Prediction: 7.57, Actual: 8.01\n",
            "Prediction: 13.13, Actual: 14.75\n",
            "Prediction: 13.84, Actual: 8.03\n",
            "Prediction: 1.49, Actual: 2.21\n",
            "Prediction: 9.03, Actual: 5.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, Dense, Flatten\n",
        "\n",
        "# Assuming 'df' is your DataFrame with the provided data\n",
        "# Columns: AvgTemp, AvgHumidity, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
        "\n",
        "# Select relevant features for training\n",
        "all_features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall']\n",
        "\n",
        "# Extract features and target variable\n",
        "X = df[all_features].values\n",
        "y = df['Yield'].values\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define CNN model\n",
        "input_layer = Input(shape=(X_train.shape[1], 1))\n",
        "conv1d_layer = Conv1D(32, kernel_size=3, activation='relu')(input_layer)\n",
        "flatten_layer = Flatten()(conv1d_layer)\n",
        "fc_layer = Dense(64, activation='relu')(flatten_layer)\n",
        "\n",
        "# Output layer for yield prediction\n",
        "output_layer = Dense(1, activation='linear')(fc_layer)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=16, validation_split=0.2)\n",
        "\n",
        "# Test the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error on Test Data: {mse:.2f}\")\n",
        "\n",
        "# Display some predictions and actual values\n",
        "for i in range(10):  # Displaying first 10 predictions for illustration\n",
        "    print(f\"Prediction: {y_pred[i][0]:.2f}, Actual: {y_test[i]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyYNhvoLWzoB",
        "outputId": "d11e1506-47c5-4b7d-f5e2-fecafa1e7392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 3, 1)]            0         \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 1, 32)             128       \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 64)                2112      \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2305 (9.00 KB)\n",
            "Trainable params: 2305 (9.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/150\n",
            "12/12 [==============================] - 1s 17ms/step - loss: 1213.6273 - val_loss: 1714.0308\n",
            "Epoch 2/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1205.2329 - val_loss: 1705.4347\n",
            "Epoch 3/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1197.2473 - val_loss: 1694.5920\n",
            "Epoch 4/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1184.9143 - val_loss: 1680.7111\n",
            "Epoch 5/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1168.1301 - val_loss: 1661.7345\n",
            "Epoch 6/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1148.6630 - val_loss: 1635.0704\n",
            "Epoch 7/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1119.8480 - val_loss: 1603.4836\n",
            "Epoch 8/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1094.2205 - val_loss: 1576.1024\n",
            "Epoch 9/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1069.3582 - val_loss: 1550.4827\n",
            "Epoch 10/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1048.0972 - val_loss: 1525.2192\n",
            "Epoch 11/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1030.6318 - val_loss: 1497.6160\n",
            "Epoch 12/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1008.2324 - val_loss: 1473.0452\n",
            "Epoch 13/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 990.1135 - val_loss: 1446.0452\n",
            "Epoch 14/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 969.0748 - val_loss: 1424.9764\n",
            "Epoch 15/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 953.6582 - val_loss: 1404.0924\n",
            "Epoch 16/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 941.0820 - val_loss: 1389.2144\n",
            "Epoch 17/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 930.9739 - val_loss: 1377.0663\n",
            "Epoch 18/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 922.4922 - val_loss: 1373.2729\n",
            "Epoch 19/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 919.9381 - val_loss: 1370.1567\n",
            "Epoch 20/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 915.6604 - val_loss: 1371.9944\n",
            "Epoch 21/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 913.9742 - val_loss: 1369.3241\n",
            "Epoch 22/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 913.4432 - val_loss: 1369.0048\n",
            "Epoch 23/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 910.6214 - val_loss: 1371.5782\n",
            "Epoch 24/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 910.5645 - val_loss: 1375.4213\n",
            "Epoch 25/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 909.7336 - val_loss: 1375.3364\n",
            "Epoch 26/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 910.4248 - val_loss: 1375.4185\n",
            "Epoch 27/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 907.7905 - val_loss: 1376.9849\n",
            "Epoch 28/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 906.6667 - val_loss: 1378.6108\n",
            "Epoch 29/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 906.0235 - val_loss: 1381.2356\n",
            "Epoch 30/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 906.6994 - val_loss: 1384.3947\n",
            "Epoch 31/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 906.1984 - val_loss: 1385.0710\n",
            "Epoch 32/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 905.8785 - val_loss: 1385.3075\n",
            "Epoch 33/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 905.0012 - val_loss: 1385.0659\n",
            "Epoch 34/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 905.4632 - val_loss: 1385.8665\n",
            "Epoch 35/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 904.3736 - val_loss: 1386.0671\n",
            "Epoch 36/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 903.7255 - val_loss: 1387.1400\n",
            "Epoch 37/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 903.8018 - val_loss: 1387.4631\n",
            "Epoch 38/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 903.4053 - val_loss: 1387.9359\n",
            "Epoch 39/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 903.1517 - val_loss: 1389.0618\n",
            "Epoch 40/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 902.8151 - val_loss: 1389.5490\n",
            "Epoch 41/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 902.5951 - val_loss: 1389.6732\n",
            "Epoch 42/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 902.0328 - val_loss: 1390.5349\n",
            "Epoch 43/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 901.1906 - val_loss: 1390.3828\n",
            "Epoch 44/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 901.3076 - val_loss: 1390.8765\n",
            "Epoch 45/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 900.9805 - val_loss: 1391.2753\n",
            "Epoch 46/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 900.7468 - val_loss: 1393.6099\n",
            "Epoch 47/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 900.9919 - val_loss: 1393.6171\n",
            "Epoch 48/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 900.8474 - val_loss: 1401.8785\n",
            "Epoch 49/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 904.1957 - val_loss: 1403.7498\n",
            "Epoch 50/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 904.7833 - val_loss: 1403.8218\n",
            "Epoch 51/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 902.8041 - val_loss: 1403.1825\n",
            "Epoch 52/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 901.3411 - val_loss: 1403.9536\n",
            "Epoch 53/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 899.5408 - val_loss: 1403.7313\n",
            "Epoch 54/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 898.9901 - val_loss: 1403.5566\n",
            "Epoch 55/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 898.6595 - val_loss: 1403.7832\n",
            "Epoch 56/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 898.2014 - val_loss: 1403.2333\n",
            "Epoch 57/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 898.5214 - val_loss: 1403.8905\n",
            "Epoch 58/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 897.3437 - val_loss: 1403.7172\n",
            "Epoch 59/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 897.5903 - val_loss: 1401.8500\n",
            "Epoch 60/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 897.4998 - val_loss: 1401.7067\n",
            "Epoch 61/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 898.2092 - val_loss: 1402.4485\n",
            "Epoch 62/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 898.5860 - val_loss: 1402.6252\n",
            "Epoch 63/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 897.1847 - val_loss: 1401.0978\n",
            "Epoch 64/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 897.1058 - val_loss: 1401.9519\n",
            "Epoch 65/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 896.8505 - val_loss: 1402.9159\n",
            "Epoch 66/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 897.8234 - val_loss: 1401.9459\n",
            "Epoch 67/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 896.1941 - val_loss: 1400.7222\n",
            "Epoch 68/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 896.3580 - val_loss: 1399.1106\n",
            "Epoch 69/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 895.8943 - val_loss: 1399.1222\n",
            "Epoch 70/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 896.5974 - val_loss: 1400.5579\n",
            "Epoch 71/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 896.3145 - val_loss: 1400.2529\n",
            "Epoch 72/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 897.0437 - val_loss: 1400.4675\n",
            "Epoch 73/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 896.3513 - val_loss: 1400.7140\n",
            "Epoch 74/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 895.7878 - val_loss: 1400.8229\n",
            "Epoch 75/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 896.0401 - val_loss: 1402.4532\n",
            "Epoch 76/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 896.1682 - val_loss: 1403.3516\n",
            "Epoch 77/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 895.5881 - val_loss: 1403.3505\n",
            "Epoch 78/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 895.3356 - val_loss: 1401.8708\n",
            "Epoch 79/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 895.2864 - val_loss: 1401.6090\n",
            "Epoch 80/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 894.8959 - val_loss: 1401.0476\n",
            "Epoch 81/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 894.7938 - val_loss: 1402.4369\n",
            "Epoch 82/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 894.6657 - val_loss: 1400.8503\n",
            "Epoch 83/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 894.3602 - val_loss: 1400.6250\n",
            "Epoch 84/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 894.2265 - val_loss: 1401.5944\n",
            "Epoch 85/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 894.2870 - val_loss: 1403.9080\n",
            "Epoch 86/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 894.3586 - val_loss: 1403.8489\n",
            "Epoch 87/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 894.5172 - val_loss: 1403.6862\n",
            "Epoch 88/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 894.1750 - val_loss: 1402.9448\n",
            "Epoch 89/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 895.1693 - val_loss: 1403.0259\n",
            "Epoch 90/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 893.3123 - val_loss: 1402.8567\n",
            "Epoch 91/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 894.3085 - val_loss: 1400.0654\n",
            "Epoch 92/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 894.0604 - val_loss: 1400.3146\n",
            "Epoch 93/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 893.7860 - val_loss: 1399.3365\n",
            "Epoch 94/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 893.2178 - val_loss: 1399.9911\n",
            "Epoch 95/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 894.0811 - val_loss: 1401.1052\n",
            "Epoch 96/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 893.0029 - val_loss: 1401.2631\n",
            "Epoch 97/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 893.0449 - val_loss: 1401.3347\n",
            "Epoch 98/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 892.2915 - val_loss: 1401.8995\n",
            "Epoch 99/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 892.2870 - val_loss: 1403.5154\n",
            "Epoch 100/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 893.2408 - val_loss: 1404.3683\n",
            "Epoch 101/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 892.9191 - val_loss: 1403.9983\n",
            "Epoch 102/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 892.9759 - val_loss: 1402.5841\n",
            "Epoch 103/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 891.8809 - val_loss: 1401.9215\n",
            "Epoch 104/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 891.6213 - val_loss: 1401.8568\n",
            "Epoch 105/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 894.1181 - val_loss: 1403.2170\n",
            "Epoch 106/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 892.4521 - val_loss: 1402.0876\n",
            "Epoch 107/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 892.5419 - val_loss: 1403.1548\n",
            "Epoch 108/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 891.7223 - val_loss: 1402.2610\n",
            "Epoch 109/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 891.5203 - val_loss: 1399.4941\n",
            "Epoch 110/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 891.2513 - val_loss: 1399.1499\n",
            "Epoch 111/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 891.8774 - val_loss: 1399.1283\n",
            "Epoch 112/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 890.4666 - val_loss: 1400.5070\n",
            "Epoch 113/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 890.2508 - val_loss: 1400.2338\n",
            "Epoch 114/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 890.9013 - val_loss: 1401.3767\n",
            "Epoch 115/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 890.1080 - val_loss: 1402.8239\n",
            "Epoch 116/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 891.7393 - val_loss: 1403.7771\n",
            "Epoch 117/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 890.5637 - val_loss: 1400.7849\n",
            "Epoch 118/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 891.0981 - val_loss: 1396.9187\n",
            "Epoch 119/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 891.0226 - val_loss: 1394.7134\n",
            "Epoch 120/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 890.4713 - val_loss: 1394.9581\n",
            "Epoch 121/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 890.1430 - val_loss: 1394.5804\n",
            "Epoch 122/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 889.7422 - val_loss: 1395.3312\n",
            "Epoch 123/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 890.2601 - val_loss: 1395.9294\n",
            "Epoch 124/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 889.8011 - val_loss: 1394.4921\n",
            "Epoch 125/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 889.6483 - val_loss: 1395.6913\n",
            "Epoch 126/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 890.4436 - val_loss: 1396.1545\n",
            "Epoch 127/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 890.0181 - val_loss: 1395.2932\n",
            "Epoch 128/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 889.1391 - val_loss: 1395.9296\n",
            "Epoch 129/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 889.3867 - val_loss: 1395.7039\n",
            "Epoch 130/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 889.1384 - val_loss: 1395.7382\n",
            "Epoch 131/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 889.3824 - val_loss: 1395.2589\n",
            "Epoch 132/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 889.3101 - val_loss: 1395.9935\n",
            "Epoch 133/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 889.0805 - val_loss: 1396.8038\n",
            "Epoch 134/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 888.3693 - val_loss: 1397.3448\n",
            "Epoch 135/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 887.8746 - val_loss: 1397.0364\n",
            "Epoch 136/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 888.1295 - val_loss: 1398.8939\n",
            "Epoch 137/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 888.0626 - val_loss: 1399.0347\n",
            "Epoch 138/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 887.9733 - val_loss: 1398.9504\n",
            "Epoch 139/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 888.1202 - val_loss: 1400.9374\n",
            "Epoch 140/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 887.9081 - val_loss: 1399.3749\n",
            "Epoch 141/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 887.1068 - val_loss: 1397.8656\n",
            "Epoch 142/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 887.1765 - val_loss: 1395.9766\n",
            "Epoch 143/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 888.5931 - val_loss: 1394.1853\n",
            "Epoch 144/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 888.6027 - val_loss: 1393.0916\n",
            "Epoch 145/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 888.5018 - val_loss: 1393.5060\n",
            "Epoch 146/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 887.8605 - val_loss: 1393.5880\n",
            "Epoch 147/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 887.3712 - val_loss: 1394.5061\n",
            "Epoch 148/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 886.9758 - val_loss: 1394.5245\n",
            "Epoch 149/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 886.7838 - val_loss: 1394.7903\n",
            "Epoch 150/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 886.9352 - val_loss: 1394.5966\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Mean Squared Error on Test Data: 62.25\n",
            "Prediction: 9.70, Actual: 8.55\n",
            "Prediction: 13.60, Actual: 14.46\n",
            "Prediction: 21.07, Actual: 8.50\n",
            "Prediction: 7.04, Actual: 5.00\n",
            "Prediction: 7.13, Actual: 2.79\n",
            "Prediction: 6.08, Actual: 8.01\n",
            "Prediction: 16.50, Actual: 14.75\n",
            "Prediction: 21.11, Actual: 8.03\n",
            "Prediction: 5.49, Actual: 2.21\n",
            "Prediction: 7.92, Actual: 5.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# Assuming 'df' is your DataFrame with the provided data\n",
        "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
        "\n",
        "# Select relevant features for training\n",
        "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall','Fertilizer','Pesticide']\n",
        "\n",
        "# Extract features and target variable\n",
        "X = df[features]\n",
        "y = df['Yield']\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X[features] = scaler.fit_transform(X[features])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.1, random_state=42)\n",
        "\n",
        "# Build and train a simple feedforward neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(200, activation='relu', input_dim=len(features)))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
        "\n",
        "model.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_test, y_test))\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error on Test Data: {mse:.2f}\")\n",
        "\n",
        "# Display some predictions and actual values\n",
        "for i in range(10):  # Displaying first 10 predictions for illustration\n",
        "    print(f\"Prediction: {y_pred[i][0]:.2f}, Actual: {y_test[i]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmt-0CJ1W4Vw",
        "outputId": "b4b70777-4e06-4b06-d603-4d4c3c5de79b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-ba593d2cd7e2>:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[features] = scaler.fit_transform(X[features])\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "16/16 [==============================] - 2s 20ms/step - loss: 1184.1788 - val_loss: 87.9264\n",
            "Epoch 2/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1110.5712 - val_loss: 60.1818\n",
            "Epoch 3/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1013.1373 - val_loss: 80.3822\n",
            "Epoch 4/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 924.8690 - val_loss: 100.6371\n",
            "Epoch 5/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 909.0250 - val_loss: 100.7250\n",
            "Epoch 6/200\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 900.2833 - val_loss: 115.2312\n",
            "Epoch 7/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 899.8365 - val_loss: 90.1251\n",
            "Epoch 8/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 893.7475 - val_loss: 103.9805\n",
            "Epoch 9/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 889.6456 - val_loss: 112.7675\n",
            "Epoch 10/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 889.7438 - val_loss: 106.4167\n",
            "Epoch 11/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 887.4766 - val_loss: 90.3326\n",
            "Epoch 12/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 891.2073 - val_loss: 96.8764\n",
            "Epoch 13/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 883.2330 - val_loss: 81.7975\n",
            "Epoch 14/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 879.0603 - val_loss: 92.8986\n",
            "Epoch 15/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 873.6739 - val_loss: 77.9076\n",
            "Epoch 16/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 877.8887 - val_loss: 84.3240\n",
            "Epoch 17/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 872.8965 - val_loss: 82.3593\n",
            "Epoch 18/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 868.0632 - val_loss: 87.1523\n",
            "Epoch 19/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 869.4772 - val_loss: 76.8794\n",
            "Epoch 20/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 861.8252 - val_loss: 73.9942\n",
            "Epoch 21/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 857.7507 - val_loss: 79.7656\n",
            "Epoch 22/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 853.8288 - val_loss: 102.0917\n",
            "Epoch 23/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 855.6609 - val_loss: 65.7888\n",
            "Epoch 24/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 859.5466 - val_loss: 96.6940\n",
            "Epoch 25/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 847.7542 - val_loss: 72.4058\n",
            "Epoch 26/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 842.9160 - val_loss: 98.1949\n",
            "Epoch 27/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 837.1595 - val_loss: 75.2450\n",
            "Epoch 28/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 833.8851 - val_loss: 79.3165\n",
            "Epoch 29/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 826.3726 - val_loss: 81.0389\n",
            "Epoch 30/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 822.5800 - val_loss: 83.4761\n",
            "Epoch 31/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 821.0432 - val_loss: 71.2010\n",
            "Epoch 32/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 816.7036 - val_loss: 94.8965\n",
            "Epoch 33/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 812.4130 - val_loss: 89.7454\n",
            "Epoch 34/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 803.9059 - val_loss: 77.8269\n",
            "Epoch 35/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 798.4940 - val_loss: 89.4682\n",
            "Epoch 36/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 793.3293 - val_loss: 82.0617\n",
            "Epoch 37/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 792.9852 - val_loss: 90.9200\n",
            "Epoch 38/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 791.5146 - val_loss: 98.2921\n",
            "Epoch 39/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 796.1984 - val_loss: 85.0866\n",
            "Epoch 40/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 776.6983 - val_loss: 92.0490\n",
            "Epoch 41/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 777.4875 - val_loss: 77.0762\n",
            "Epoch 42/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 780.0743 - val_loss: 100.9219\n",
            "Epoch 43/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 773.5491 - val_loss: 62.9406\n",
            "Epoch 44/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 759.0538 - val_loss: 124.6012\n",
            "Epoch 45/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 756.4954 - val_loss: 66.6196\n",
            "Epoch 46/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 743.7448 - val_loss: 80.7213\n",
            "Epoch 47/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 746.8925 - val_loss: 101.2234\n",
            "Epoch 48/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 737.1727 - val_loss: 97.7296\n",
            "Epoch 49/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 744.4548 - val_loss: 56.6242\n",
            "Epoch 50/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 712.7103 - val_loss: 125.9852\n",
            "Epoch 51/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 720.3655 - val_loss: 70.0190\n",
            "Epoch 52/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 707.6434 - val_loss: 99.1299\n",
            "Epoch 53/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 696.4698 - val_loss: 64.4135\n",
            "Epoch 54/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 693.3718 - val_loss: 96.8123\n",
            "Epoch 55/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 684.6698 - val_loss: 92.7200\n",
            "Epoch 56/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 692.6125 - val_loss: 59.6734\n",
            "Epoch 57/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 667.8427 - val_loss: 156.6391\n",
            "Epoch 58/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 672.5630 - val_loss: 88.5503\n",
            "Epoch 59/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 654.7183 - val_loss: 76.0972\n",
            "Epoch 60/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 657.4474 - val_loss: 79.4566\n",
            "Epoch 61/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 662.7068 - val_loss: 34.6852\n",
            "Epoch 62/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 625.0870 - val_loss: 100.7181\n",
            "Epoch 63/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 622.7910 - val_loss: 90.3036\n",
            "Epoch 64/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 611.3235 - val_loss: 36.2327\n",
            "Epoch 65/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 600.1125 - val_loss: 51.6250\n",
            "Epoch 66/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 604.1507 - val_loss: 79.8792\n",
            "Epoch 67/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 574.2125 - val_loss: 29.1949\n",
            "Epoch 68/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 582.5046 - val_loss: 51.6508\n",
            "Epoch 69/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 574.1168 - val_loss: 75.3461\n",
            "Epoch 70/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 560.8118 - val_loss: 30.9234\n",
            "Epoch 71/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 564.0823 - val_loss: 42.3264\n",
            "Epoch 72/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 572.1147 - val_loss: 27.3479\n",
            "Epoch 73/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 516.4131 - val_loss: 77.9462\n",
            "Epoch 74/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 529.9094 - val_loss: 42.2227\n",
            "Epoch 75/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 519.7836 - val_loss: 47.2517\n",
            "Epoch 76/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 532.3280 - val_loss: 25.3734\n",
            "Epoch 77/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 482.4502 - val_loss: 47.4453\n",
            "Epoch 78/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 495.2225 - val_loss: 31.2105\n",
            "Epoch 79/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 468.8378 - val_loss: 52.4678\n",
            "Epoch 80/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 498.0909 - val_loss: 30.2765\n",
            "Epoch 81/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 465.7269 - val_loss: 39.6055\n",
            "Epoch 82/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 457.3059 - val_loss: 44.0957\n",
            "Epoch 83/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 450.1851 - val_loss: 23.7779\n",
            "Epoch 84/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 466.0524 - val_loss: 30.0402\n",
            "Epoch 85/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 423.4352 - val_loss: 40.4629\n",
            "Epoch 86/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 435.4851 - val_loss: 25.6825\n",
            "Epoch 87/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 405.4708 - val_loss: 19.8112\n",
            "Epoch 88/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 418.0626 - val_loss: 32.2270\n",
            "Epoch 89/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 375.9450 - val_loss: 37.8384\n",
            "Epoch 90/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 375.1010 - val_loss: 23.6272\n",
            "Epoch 91/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 373.7652 - val_loss: 26.9597\n",
            "Epoch 92/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 352.6335 - val_loss: 31.2034\n",
            "Epoch 93/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 359.2099 - val_loss: 25.4321\n",
            "Epoch 94/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 336.4592 - val_loss: 20.9367\n",
            "Epoch 95/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 338.5540 - val_loss: 31.6966\n",
            "Epoch 96/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 336.5227 - val_loss: 19.3465\n",
            "Epoch 97/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 341.0415 - val_loss: 24.1750\n",
            "Epoch 98/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 293.0277 - val_loss: 35.1221\n",
            "Epoch 99/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 298.0948 - val_loss: 23.2750\n",
            "Epoch 100/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 268.1697 - val_loss: 25.6241\n",
            "Epoch 101/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 281.3228 - val_loss: 34.1857\n",
            "Epoch 102/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 258.5019 - val_loss: 19.7883\n",
            "Epoch 103/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 296.9344 - val_loss: 22.0186\n",
            "Epoch 104/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 262.4588 - val_loss: 29.4505\n",
            "Epoch 105/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 252.7402 - val_loss: 18.2027\n",
            "Epoch 106/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 234.0328 - val_loss: 31.9042\n",
            "Epoch 107/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 204.5803 - val_loss: 19.7156\n",
            "Epoch 108/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 288.9934 - val_loss: 34.6953\n",
            "Epoch 109/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 234.3305 - val_loss: 27.4609\n",
            "Epoch 110/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 215.2964 - val_loss: 24.7364\n",
            "Epoch 111/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 201.0570 - val_loss: 27.4726\n",
            "Epoch 112/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 178.1035 - val_loss: 18.8903\n",
            "Epoch 113/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 182.6361 - val_loss: 32.4395\n",
            "Epoch 114/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 209.9498 - val_loss: 20.6449\n",
            "Epoch 115/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 167.0188 - val_loss: 27.0489\n",
            "Epoch 116/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 157.7997 - val_loss: 25.9238\n",
            "Epoch 117/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 155.7820 - val_loss: 23.6666\n",
            "Epoch 118/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 191.4392 - val_loss: 22.5139\n",
            "Epoch 119/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 178.0814 - val_loss: 19.3255\n",
            "Epoch 120/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 163.1144 - val_loss: 27.0451\n",
            "Epoch 121/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 130.6405 - val_loss: 26.6538\n",
            "Epoch 122/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 134.0231 - val_loss: 23.8256\n",
            "Epoch 123/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 124.0348 - val_loss: 24.0659\n",
            "Epoch 124/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 131.5184 - val_loss: 24.0697\n",
            "Epoch 125/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 116.8277 - val_loss: 27.5052\n",
            "Epoch 126/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 111.4112 - val_loss: 20.1685\n",
            "Epoch 127/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 102.6774 - val_loss: 28.6181\n",
            "Epoch 128/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 95.9017 - val_loss: 21.5084\n",
            "Epoch 129/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 127.2051 - val_loss: 26.0668\n",
            "Epoch 130/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 86.2304 - val_loss: 34.0027\n",
            "Epoch 131/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 93.4730 - val_loss: 22.9241\n",
            "Epoch 132/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 76.9937 - val_loss: 30.4133\n",
            "Epoch 133/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 72.2458 - val_loss: 22.8948\n",
            "Epoch 134/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 80.7074 - val_loss: 29.3481\n",
            "Epoch 135/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 66.0860 - val_loss: 27.4933\n",
            "Epoch 136/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 64.1804 - val_loss: 26.2511\n",
            "Epoch 137/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 60.2570 - val_loss: 29.9257\n",
            "Epoch 138/200\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 58.5312 - val_loss: 27.5255\n",
            "Epoch 139/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 56.3691 - val_loss: 27.6681\n",
            "Epoch 140/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 57.1441 - val_loss: 21.8214\n",
            "Epoch 141/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 80.2740 - val_loss: 33.1684\n",
            "Epoch 142/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 72.3489 - val_loss: 24.9578\n",
            "Epoch 143/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 61.2909 - val_loss: 33.0850\n",
            "Epoch 144/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 68.3860 - val_loss: 24.8744\n",
            "Epoch 145/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 50.2587 - val_loss: 32.1086\n",
            "Epoch 146/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 43.7898 - val_loss: 24.4011\n",
            "Epoch 147/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 37.7984 - val_loss: 29.3293\n",
            "Epoch 148/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 37.3509 - val_loss: 26.6234\n",
            "Epoch 149/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 34.0090 - val_loss: 26.5085\n",
            "Epoch 150/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 34.7408 - val_loss: 26.2055\n",
            "Epoch 151/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 32.6004 - val_loss: 25.3391\n",
            "Epoch 152/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 36.5913 - val_loss: 31.2015\n",
            "Epoch 153/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 37.8842 - val_loss: 26.4430\n",
            "Epoch 154/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 32.8424 - val_loss: 31.9274\n",
            "Epoch 155/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 30.2224 - val_loss: 28.3198\n",
            "Epoch 156/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 28.7197 - val_loss: 26.7940\n",
            "Epoch 157/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 27.1257 - val_loss: 26.6548\n",
            "Epoch 158/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 29.7265 - val_loss: 30.9487\n",
            "Epoch 159/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.1927 - val_loss: 26.3293\n",
            "Epoch 160/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 29.9723 - val_loss: 34.0477\n",
            "Epoch 161/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 32.0028 - val_loss: 23.5578\n",
            "Epoch 162/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 28.0060 - val_loss: 31.4535\n",
            "Epoch 163/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 30.5932 - val_loss: 27.8774\n",
            "Epoch 164/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 31.3776 - val_loss: 32.6882\n",
            "Epoch 165/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 35.9075 - val_loss: 26.1360\n",
            "Epoch 166/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 28.6727 - val_loss: 32.1234\n",
            "Epoch 167/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 26.4436 - val_loss: 24.9460\n",
            "Epoch 168/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 28.2242 - val_loss: 31.4119\n",
            "Epoch 169/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 28.5777 - val_loss: 28.2217\n",
            "Epoch 170/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 30.1859 - val_loss: 28.1946\n",
            "Epoch 171/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 31.3533 - val_loss: 26.4347\n",
            "Epoch 172/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 27.7175 - val_loss: 32.9419\n",
            "Epoch 173/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 23.1260 - val_loss: 25.7486\n",
            "Epoch 174/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 23.3248 - val_loss: 28.2694\n",
            "Epoch 175/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 23.8328 - val_loss: 27.4505\n",
            "Epoch 176/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 24.9517 - val_loss: 25.8238\n",
            "Epoch 177/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 24.4678 - val_loss: 28.2670\n",
            "Epoch 178/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 24.3965 - val_loss: 27.8451\n",
            "Epoch 179/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 24.4519 - val_loss: 27.0343\n",
            "Epoch 180/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 21.8420 - val_loss: 25.6445\n",
            "Epoch 181/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 24.2786 - val_loss: 31.2238\n",
            "Epoch 182/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 25.3959 - val_loss: 26.9310\n",
            "Epoch 183/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 22.3623 - val_loss: 34.5932\n",
            "Epoch 184/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 23.8305 - val_loss: 24.2759\n",
            "Epoch 185/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 22.7605 - val_loss: 26.0680\n",
            "Epoch 186/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 24.2927 - val_loss: 27.1835\n",
            "Epoch 187/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 23.3070 - val_loss: 26.9052\n",
            "Epoch 188/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 27.7745 - val_loss: 26.4335\n",
            "Epoch 189/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.3332 - val_loss: 28.3538\n",
            "Epoch 190/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 31.3234 - val_loss: 26.2206\n",
            "Epoch 191/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 34.7142 - val_loss: 31.1932\n",
            "Epoch 192/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 40.8037 - val_loss: 24.5179\n",
            "Epoch 193/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 28.6722 - val_loss: 24.9992\n",
            "Epoch 194/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 21.8184 - val_loss: 28.0220\n",
            "Epoch 195/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 28.6487 - val_loss: 26.9469\n",
            "Epoch 196/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 35.4377 - val_loss: 22.2644\n",
            "Epoch 197/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 44.8953 - val_loss: 33.4246\n",
            "Epoch 198/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 37.5511 - val_loss: 23.3354\n",
            "Epoch 199/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 31.5062 - val_loss: 34.5241\n",
            "Epoch 200/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 36.0281 - val_loss: 22.2356\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "Mean Squared Error on Test Data: 22.24\n",
            "Prediction: 9.83, Actual: 8.55\n",
            "Prediction: 14.89, Actual: 14.46\n",
            "Prediction: 12.83, Actual: 8.50\n",
            "Prediction: 9.96, Actual: 5.00\n",
            "Prediction: 6.64, Actual: 2.79\n",
            "Prediction: 11.59, Actual: 8.01\n",
            "Prediction: 15.36, Actual: 14.75\n",
            "Prediction: 12.85, Actual: 8.03\n",
            "Prediction: 1.19, Actual: 2.21\n",
            "Prediction: 7.82, Actual: 5.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# Assuming 'df' is your DataFrame with the provided data\n",
        "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
        "\n",
        "# Select relevant features for training\n",
        "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall','Fertilizer','Pesticide']\n",
        "\n",
        "# Extract features and target variable\n",
        "X = df[features]\n",
        "y = df['Yield']\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X[features] = scaler.fit_transform(X[features])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.1, random_state=42)\n",
        "\n",
        "# Build and train a simple feedforward neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(200, activation='relu', input_dim=len(features)))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
        "\n",
        "model.fit(X_train, y_train, epochs=200, batch_size=8, validation_data=(X_test, y_test))\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error on Test Data: {mse:.2f}\")\n",
        "\n",
        "# Display some predictions and actual values\n",
        "for i in range(10):  # Displaying first 10 predictions for illustration\n",
        "    print(f\"Prediction: {y_pred[i][0]:.2f}, Actual: {y_test[i]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf90-rLAZjiA",
        "outputId": "c7883886-e384-4e3a-99b6-72d533bf8077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-7c2432b1deff>:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[features] = scaler.fit_transform(X[features])\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "32/32 [==============================] - 1s 7ms/step - loss: 1167.6385 - val_loss: 57.4935\n",
            "Epoch 2/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 943.1470 - val_loss: 127.7404\n",
            "Epoch 3/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 906.7662 - val_loss: 69.8040\n",
            "Epoch 4/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 911.2368 - val_loss: 68.7367\n",
            "Epoch 5/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 900.9366 - val_loss: 108.2498\n",
            "Epoch 6/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 895.4323 - val_loss: 101.4860\n",
            "Epoch 7/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 896.6046 - val_loss: 129.4990\n",
            "Epoch 8/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 898.2089 - val_loss: 65.5446\n",
            "Epoch 9/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 886.5383 - val_loss: 90.0188\n",
            "Epoch 10/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 919.4233 - val_loss: 59.8454\n",
            "Epoch 11/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 907.7125 - val_loss: 97.1025\n",
            "Epoch 12/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 870.5392 - val_loss: 62.7391\n",
            "Epoch 13/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 879.1084 - val_loss: 92.5051\n",
            "Epoch 14/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 853.6710 - val_loss: 52.4843\n",
            "Epoch 15/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 863.8277 - val_loss: 79.2876\n",
            "Epoch 16/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 850.4724 - val_loss: 79.5727\n",
            "Epoch 17/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 845.4101 - val_loss: 39.0411\n",
            "Epoch 18/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 844.3786 - val_loss: 98.2208\n",
            "Epoch 19/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 830.0959 - val_loss: 155.2047\n",
            "Epoch 20/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 821.0219 - val_loss: 58.3265\n",
            "Epoch 21/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 794.9228 - val_loss: 127.7644\n",
            "Epoch 22/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 827.4178 - val_loss: 64.3587\n",
            "Epoch 23/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 799.4660 - val_loss: 147.5798\n",
            "Epoch 24/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 821.3784 - val_loss: 94.6674\n",
            "Epoch 25/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 774.8802 - val_loss: 83.1337\n",
            "Epoch 26/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 740.2567 - val_loss: 76.2923\n",
            "Epoch 27/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 706.7446 - val_loss: 25.8069\n",
            "Epoch 28/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 718.0687 - val_loss: 105.7853\n",
            "Epoch 29/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 824.5363 - val_loss: 51.8262\n",
            "Epoch 30/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 713.7684 - val_loss: 51.4463\n",
            "Epoch 31/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 659.8004 - val_loss: 33.5325\n",
            "Epoch 32/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 620.9107 - val_loss: 103.3674\n",
            "Epoch 33/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 615.9263 - val_loss: 147.5081\n",
            "Epoch 34/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 592.0599 - val_loss: 108.0860\n",
            "Epoch 35/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 530.8948 - val_loss: 26.7316\n",
            "Epoch 36/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 567.2856 - val_loss: 199.8547\n",
            "Epoch 37/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 584.8003 - val_loss: 37.6536\n",
            "Epoch 38/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 491.7006 - val_loss: 33.0433\n",
            "Epoch 39/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 583.0886 - val_loss: 57.1041\n",
            "Epoch 40/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 704.3818 - val_loss: 43.8379\n",
            "Epoch 41/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 577.0522 - val_loss: 36.8868\n",
            "Epoch 42/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 463.9333 - val_loss: 34.3707\n",
            "Epoch 43/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 442.5357 - val_loss: 28.0567\n",
            "Epoch 44/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 432.5078 - val_loss: 36.1279\n",
            "Epoch 45/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 458.8023 - val_loss: 44.4298\n",
            "Epoch 46/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 338.9269 - val_loss: 30.4058\n",
            "Epoch 47/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 417.5804 - val_loss: 108.6988\n",
            "Epoch 48/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 285.1373 - val_loss: 33.1122\n",
            "Epoch 49/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 572.7988 - val_loss: 116.3426\n",
            "Epoch 50/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 444.7639 - val_loss: 31.7987\n",
            "Epoch 51/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 409.7387 - val_loss: 94.6781\n",
            "Epoch 52/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 272.5828 - val_loss: 31.9796\n",
            "Epoch 53/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 300.7488 - val_loss: 60.0107\n",
            "Epoch 54/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 179.4176 - val_loss: 50.2180\n",
            "Epoch 55/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 180.5425 - val_loss: 88.6710\n",
            "Epoch 56/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 159.5364 - val_loss: 34.8345\n",
            "Epoch 57/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 186.8192 - val_loss: 59.4949\n",
            "Epoch 58/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 169.9452 - val_loss: 78.0237\n",
            "Epoch 59/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 74.9855 - val_loss: 40.7311\n",
            "Epoch 60/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 64.5450 - val_loss: 31.9369\n",
            "Epoch 61/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 71.7607 - val_loss: 31.4581\n",
            "Epoch 62/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 79.9184 - val_loss: 48.5020\n",
            "Epoch 63/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 101.5114 - val_loss: 33.7158\n",
            "Epoch 64/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 93.1276 - val_loss: 43.6184\n",
            "Epoch 65/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 44.4761 - val_loss: 36.9761\n",
            "Epoch 66/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 65.3854 - val_loss: 37.1719\n",
            "Epoch 67/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 30.7788 - val_loss: 34.6447\n",
            "Epoch 68/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 41.1051 - val_loss: 32.1012\n",
            "Epoch 69/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 32.2277 - val_loss: 35.7519\n",
            "Epoch 70/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 34.2921 - val_loss: 40.4772\n",
            "Epoch 71/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 31.0053 - val_loss: 38.3926\n",
            "Epoch 72/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 45.7640 - val_loss: 31.8580\n",
            "Epoch 73/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 98.7731 - val_loss: 55.6883\n",
            "Epoch 74/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 61.2923 - val_loss: 29.4775\n",
            "Epoch 75/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 135.6698 - val_loss: 35.8931\n",
            "Epoch 76/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 231.5237 - val_loss: 45.2198\n",
            "Epoch 77/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 159.9804 - val_loss: 38.9552\n",
            "Epoch 78/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 88.6668 - val_loss: 39.0892\n",
            "Epoch 79/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 53.3772 - val_loss: 45.7291\n",
            "Epoch 80/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 42.5015 - val_loss: 38.9645\n",
            "Epoch 81/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 43.1167 - val_loss: 33.3639\n",
            "Epoch 82/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 36.5459 - val_loss: 39.7909\n",
            "Epoch 83/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 42.0218 - val_loss: 39.3113\n",
            "Epoch 84/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 58.0479 - val_loss: 33.5458\n",
            "Epoch 85/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 83.6862 - val_loss: 34.1606\n",
            "Epoch 86/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 49.5157 - val_loss: 36.7825\n",
            "Epoch 87/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 40.4327 - val_loss: 41.6314\n",
            "Epoch 88/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 43.2882 - val_loss: 30.0289\n",
            "Epoch 89/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 40.7747 - val_loss: 34.7352\n",
            "Epoch 90/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 27.4707 - val_loss: 34.9643\n",
            "Epoch 91/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 20.7717 - val_loss: 33.6023\n",
            "Epoch 92/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 20.6728 - val_loss: 36.7564\n",
            "Epoch 93/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 21.5697 - val_loss: 46.9230\n",
            "Epoch 94/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 21.0032 - val_loss: 33.0864\n",
            "Epoch 95/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 20.1829 - val_loss: 40.1250\n",
            "Epoch 96/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 19.3809 - val_loss: 37.0362\n",
            "Epoch 97/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 19.5804 - val_loss: 39.0806\n",
            "Epoch 98/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 19.2177 - val_loss: 31.7030\n",
            "Epoch 99/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 18.3742 - val_loss: 34.5431\n",
            "Epoch 100/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 18.3611 - val_loss: 37.4634\n",
            "Epoch 101/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 17.8276 - val_loss: 33.5068\n",
            "Epoch 102/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 18.9538 - val_loss: 37.6320\n",
            "Epoch 103/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 18.1125 - val_loss: 38.8370\n",
            "Epoch 104/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 17.9531 - val_loss: 39.4623\n",
            "Epoch 105/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 18.8982 - val_loss: 36.8322\n",
            "Epoch 106/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 18.4598 - val_loss: 37.2189\n",
            "Epoch 107/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 18.3670 - val_loss: 36.5877\n",
            "Epoch 108/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 18.8175 - val_loss: 35.0854\n",
            "Epoch 109/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 18.1186 - val_loss: 32.5706\n",
            "Epoch 110/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 17.6875 - val_loss: 30.8868\n",
            "Epoch 111/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 19.3697 - val_loss: 37.7402\n",
            "Epoch 112/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 17.1888 - val_loss: 35.1738\n",
            "Epoch 113/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 16.9531 - val_loss: 34.2631\n",
            "Epoch 114/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 17.8106 - val_loss: 32.7339\n",
            "Epoch 115/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 20.4992 - val_loss: 32.3558\n",
            "Epoch 116/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 45.3875 - val_loss: 35.4270\n",
            "Epoch 117/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 130.3611 - val_loss: 57.7977\n",
            "Epoch 118/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 118.1578 - val_loss: 83.6269\n",
            "Epoch 119/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 146.6004 - val_loss: 51.3760\n",
            "Epoch 120/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 132.7391 - val_loss: 89.2451\n",
            "Epoch 121/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 343.6782 - val_loss: 73.1581\n",
            "Epoch 122/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 107.7270 - val_loss: 42.8643\n",
            "Epoch 123/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 285.7230 - val_loss: 46.1178\n",
            "Epoch 124/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 152.6276 - val_loss: 81.7307\n",
            "Epoch 125/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 181.6902 - val_loss: 36.3276\n",
            "Epoch 126/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 406.0331 - val_loss: 49.1172\n",
            "Epoch 127/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 86.5299 - val_loss: 52.4913\n",
            "Epoch 128/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 56.7617 - val_loss: 49.3977\n",
            "Epoch 129/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 32.8688 - val_loss: 49.1983\n",
            "Epoch 130/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 26.8210 - val_loss: 45.0082\n",
            "Epoch 131/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 21.4661 - val_loss: 36.0925\n",
            "Epoch 132/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 19.8657 - val_loss: 36.3346\n",
            "Epoch 133/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 20.2139 - val_loss: 38.9369\n",
            "Epoch 134/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 20.7191 - val_loss: 34.4511\n",
            "Epoch 135/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 24.2920 - val_loss: 46.7769\n",
            "Epoch 136/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 25.1500 - val_loss: 38.3106\n",
            "Epoch 137/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 31.8735 - val_loss: 45.2481\n",
            "Epoch 138/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 26.5509 - val_loss: 36.7585\n",
            "Epoch 139/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 25.7734 - val_loss: 39.1807\n",
            "Epoch 140/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 22.5379 - val_loss: 37.3599\n",
            "Epoch 141/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 26.6948 - val_loss: 37.0000\n",
            "Epoch 142/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 32.8099 - val_loss: 25.9108\n",
            "Epoch 143/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 82.2334 - val_loss: 49.7679\n",
            "Epoch 144/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 52.3295 - val_loss: 31.8459\n",
            "Epoch 145/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 49.1778 - val_loss: 39.2689\n",
            "Epoch 146/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 26.1273 - val_loss: 34.1586\n",
            "Epoch 147/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 18.3795 - val_loss: 36.8359\n",
            "Epoch 148/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 16.4401 - val_loss: 34.4991\n",
            "Epoch 149/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 17.0586 - val_loss: 33.2359\n",
            "Epoch 150/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 18.0960 - val_loss: 34.8931\n",
            "Epoch 151/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 21.8316 - val_loss: 40.4729\n",
            "Epoch 152/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 23.8840 - val_loss: 35.1326\n",
            "Epoch 153/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 19.8379 - val_loss: 34.1324\n",
            "Epoch 154/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 16.2999 - val_loss: 32.7953\n",
            "Epoch 155/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 15.7100 - val_loss: 34.6422\n",
            "Epoch 156/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 14.2156 - val_loss: 35.9981\n",
            "Epoch 157/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 14.1448 - val_loss: 35.5266\n",
            "Epoch 158/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 14.0502 - val_loss: 33.9280\n",
            "Epoch 159/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 13.9592 - val_loss: 35.4563\n",
            "Epoch 160/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 14.6496 - val_loss: 32.4375\n",
            "Epoch 161/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 14.1881 - val_loss: 33.7440\n",
            "Epoch 162/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 14.5546 - val_loss: 38.8044\n",
            "Epoch 163/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 15.0994 - val_loss: 28.0963\n",
            "Epoch 164/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 18.2769 - val_loss: 34.0041\n",
            "Epoch 165/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 26.6296 - val_loss: 36.3160\n",
            "Epoch 166/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 84.1377 - val_loss: 50.3528\n",
            "Epoch 167/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 113.0403 - val_loss: 39.8832\n",
            "Epoch 168/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 147.1898 - val_loss: 24.5483\n",
            "Epoch 169/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 252.6272 - val_loss: 39.6492\n",
            "Epoch 170/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 45.7461 - val_loss: 25.9395\n",
            "Epoch 171/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 396.4630 - val_loss: 50.3534\n",
            "Epoch 172/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 106.5782 - val_loss: 33.8231\n",
            "Epoch 173/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 21.1174 - val_loss: 39.7061\n",
            "Epoch 174/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 18.1445 - val_loss: 38.1707\n",
            "Epoch 175/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 25.1340 - val_loss: 35.9759\n",
            "Epoch 176/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 43.1866 - val_loss: 34.1855\n",
            "Epoch 177/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 31.1682 - val_loss: 33.2020\n",
            "Epoch 178/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 31.6867 - val_loss: 34.8506\n",
            "Epoch 179/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 31.0745 - val_loss: 36.4843\n",
            "Epoch 180/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 51.6888 - val_loss: 34.7800\n",
            "Epoch 181/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 49.1682 - val_loss: 35.8456\n",
            "Epoch 182/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 53.3239 - val_loss: 33.0445\n",
            "Epoch 183/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 26.3670 - val_loss: 38.5585\n",
            "Epoch 184/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 18.9076 - val_loss: 33.6985\n",
            "Epoch 185/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 15.3505 - val_loss: 32.2931\n",
            "Epoch 186/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 17.1800 - val_loss: 32.9783\n",
            "Epoch 187/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 17.0592 - val_loss: 33.6409\n",
            "Epoch 188/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 18.4238 - val_loss: 32.6711\n",
            "Epoch 189/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 17.3786 - val_loss: 30.7180\n",
            "Epoch 190/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 19.5038 - val_loss: 38.2974\n",
            "Epoch 191/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 14.7694 - val_loss: 30.1570\n",
            "Epoch 192/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 15.1468 - val_loss: 33.1311\n",
            "Epoch 193/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 14.5687 - val_loss: 34.6236\n",
            "Epoch 194/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 15.0612 - val_loss: 33.6365\n",
            "Epoch 195/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 15.6030 - val_loss: 26.1554\n",
            "Epoch 196/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 18.5476 - val_loss: 30.0548\n",
            "Epoch 197/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 21.6341 - val_loss: 32.8459\n",
            "Epoch 198/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 34.1609 - val_loss: 41.7741\n",
            "Epoch 199/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 27.0993 - val_loss: 36.6528\n",
            "Epoch 200/200\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 27.7829 - val_loss: 36.0708\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "Mean Squared Error on Test Data: 36.07\n",
            "Prediction: 5.78, Actual: 8.55\n",
            "Prediction: 15.18, Actual: 14.46\n",
            "Prediction: 14.94, Actual: 8.50\n",
            "Prediction: 7.65, Actual: 5.00\n",
            "Prediction: 5.58, Actual: 2.79\n",
            "Prediction: 7.16, Actual: 8.01\n",
            "Prediction: 11.64, Actual: 14.75\n",
            "Prediction: 13.53, Actual: 8.03\n",
            "Prediction: 1.43, Actual: 2.21\n",
            "Prediction: 9.84, Actual: 5.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "\n",
        "# Assuming 'df' is your DataFrame with the provided data\n",
        "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
        "\n",
        "# Select relevant features for training\n",
        "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall','Fertilizer','Pesticide']\n",
        "\n",
        "# Extract features and target variable\n",
        "X = df[features]\n",
        "y = df['Yield']\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X[features] = scaler.fit_transform(X[features])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.1, random_state=42)\n",
        "\n",
        "# Build and train a simple feedforward neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(200, activation='sigmoid', input_dim=len(features)))\n",
        "model.add(Dense(100, activation='sigmoid'))\n",
        "model.add(Dense(50, activation='sigmoid'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
        "\n",
        "model.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_test, y_test))\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error on Test Data: {mse:.2f}\")\n",
        "\n",
        "# Display some predictions and actual values\n",
        "for i in range(10):  # Displaying first 10 predictions for illustration\n",
        "    print(f\"Prediction: {y_pred[i][0]:.2f}, Actual: {y_test[i]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ppMhjEnx6uuj",
        "outputId": "3dfd5e1b-e11d-451d-d899-502aa3b93462"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-8175357d6fb6>:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[features] = scaler.fit_transform(X[features])\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "16/16 [==============================] - 1s 21ms/step - loss: 1151.6027 - val_loss: 78.2926\n",
            "Epoch 2/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1096.9607 - val_loss: 62.1376\n",
            "Epoch 3/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1067.1802 - val_loss: 55.2887\n",
            "Epoch 4/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1051.1959 - val_loss: 51.5779\n",
            "Epoch 5/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1040.1500 - val_loss: 49.1177\n",
            "Epoch 6/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1031.5034 - val_loss: 47.4261\n",
            "Epoch 7/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1023.0557 - val_loss: 46.4139\n",
            "Epoch 8/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1016.6376 - val_loss: 45.6966\n",
            "Epoch 9/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1010.5837 - val_loss: 45.3407\n",
            "Epoch 10/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1004.9240 - val_loss: 45.2947\n",
            "Epoch 11/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 999.6485 - val_loss: 45.5453\n",
            "Epoch 12/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 995.4042 - val_loss: 46.0758\n",
            "Epoch 13/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 990.9944 - val_loss: 46.7124\n",
            "Epoch 14/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 987.4558 - val_loss: 47.5345\n",
            "Epoch 15/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 984.2195 - val_loss: 48.4511\n",
            "Epoch 16/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 981.5616 - val_loss: 49.6646\n",
            "Epoch 17/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 978.7898 - val_loss: 50.8171\n",
            "Epoch 18/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 976.5620 - val_loss: 52.0980\n",
            "Epoch 19/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 974.3542 - val_loss: 53.3636\n",
            "Epoch 20/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 972.5330 - val_loss: 54.7184\n",
            "Epoch 21/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 970.7835 - val_loss: 56.0551\n",
            "Epoch 22/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 969.1389 - val_loss: 57.0739\n",
            "Epoch 23/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 968.0754 - val_loss: 58.6581\n",
            "Epoch 24/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 966.6600 - val_loss: 59.9293\n",
            "Epoch 25/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 965.7253 - val_loss: 61.4751\n",
            "Epoch 26/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 964.5182 - val_loss: 62.3868\n",
            "Epoch 27/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 963.7581 - val_loss: 64.4094\n",
            "Epoch 28/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 962.6910 - val_loss: 65.4115\n",
            "Epoch 29/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 961.9840 - val_loss: 66.8398\n",
            "Epoch 30/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 961.7784 - val_loss: 68.9294\n",
            "Epoch 31/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 960.7518 - val_loss: 69.9595\n",
            "Epoch 32/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 960.3497 - val_loss: 71.5566\n",
            "Epoch 33/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 959.8265 - val_loss: 72.6739\n",
            "Epoch 34/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 959.4485 - val_loss: 73.5978\n",
            "Epoch 35/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 959.1415 - val_loss: 74.3177\n",
            "Epoch 36/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 958.9411 - val_loss: 75.2709\n",
            "Epoch 37/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 958.6937 - val_loss: 76.3659\n",
            "Epoch 38/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 958.7353 - val_loss: 77.6741\n",
            "Epoch 39/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 958.2548 - val_loss: 77.7924\n",
            "Epoch 40/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 958.2280 - val_loss: 79.0102\n",
            "Epoch 41/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 957.9288 - val_loss: 79.1994\n",
            "Epoch 42/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 957.9355 - val_loss: 79.3506\n",
            "Epoch 43/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 957.5966 - val_loss: 81.4110\n",
            "Epoch 44/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 957.5400 - val_loss: 81.6666\n",
            "Epoch 45/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 957.6519 - val_loss: 83.3864\n",
            "Epoch 46/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 957.3158 - val_loss: 83.4614\n",
            "Epoch 47/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 957.2610 - val_loss: 84.6142\n",
            "Epoch 48/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 957.1657 - val_loss: 85.2291\n",
            "Epoch 49/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 957.1236 - val_loss: 85.2925\n",
            "Epoch 50/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 957.1865 - val_loss: 86.1453\n",
            "Epoch 51/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 957.0068 - val_loss: 86.2073\n",
            "Epoch 52/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 957.1304 - val_loss: 86.0722\n",
            "Epoch 53/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 956.9812 - val_loss: 86.3703\n",
            "Epoch 54/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 956.8892 - val_loss: 87.1539\n",
            "Epoch 55/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 956.9117 - val_loss: 87.8328\n",
            "Epoch 56/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 956.9713 - val_loss: 88.4146\n",
            "Epoch 57/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 956.9565 - val_loss: 88.3003\n",
            "Epoch 58/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 956.9702 - val_loss: 89.0513\n",
            "Epoch 59/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 956.8568 - val_loss: 88.7904\n",
            "Epoch 60/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 956.8553 - val_loss: 88.7911\n",
            "Epoch 61/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 956.8355 - val_loss: 89.0596\n",
            "Epoch 62/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 956.7850 - val_loss: 89.3263\n",
            "Epoch 63/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 956.7592 - val_loss: 89.0709\n",
            "Epoch 64/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 956.9513 - val_loss: 89.7903\n",
            "Epoch 65/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 956.6793 - val_loss: 89.7204\n",
            "Epoch 66/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 956.8975 - val_loss: 88.5081\n",
            "Epoch 67/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 956.7863 - val_loss: 89.5585\n",
            "Epoch 68/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 956.4541 - val_loss: 89.2488\n",
            "Epoch 69/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 956.1147 - val_loss: 87.9374\n",
            "Epoch 70/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 955.1909 - val_loss: 84.3894\n",
            "Epoch 71/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 951.9690 - val_loss: 67.8852\n",
            "Epoch 72/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 941.5903 - val_loss: 47.1023\n",
            "Epoch 73/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 927.5535 - val_loss: 42.6648\n",
            "Epoch 74/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 922.5185 - val_loss: 37.9832\n",
            "Epoch 75/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 917.9816 - val_loss: 38.7364\n",
            "Epoch 76/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 914.9896 - val_loss: 39.3495\n",
            "Epoch 77/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 912.4980 - val_loss: 40.4399\n",
            "Epoch 78/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 911.0002 - val_loss: 40.1863\n",
            "Epoch 79/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 909.0930 - val_loss: 41.6578\n",
            "Epoch 80/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 907.8884 - val_loss: 42.3522\n",
            "Epoch 81/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 906.1921 - val_loss: 43.9610\n",
            "Epoch 82/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 904.6696 - val_loss: 44.8283\n",
            "Epoch 83/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 903.3750 - val_loss: 46.2571\n",
            "Epoch 84/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 902.1169 - val_loss: 46.8511\n",
            "Epoch 85/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 902.0096 - val_loss: 47.3465\n",
            "Epoch 86/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 899.7377 - val_loss: 49.4145\n",
            "Epoch 87/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 898.5277 - val_loss: 50.6582\n",
            "Epoch 88/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 898.5155 - val_loss: 50.9942\n",
            "Epoch 89/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 896.3781 - val_loss: 50.5148\n",
            "Epoch 90/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 896.3611 - val_loss: 53.6398\n",
            "Epoch 91/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 895.4355 - val_loss: 54.5079\n",
            "Epoch 92/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 894.4503 - val_loss: 55.6951\n",
            "Epoch 93/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 894.8259 - val_loss: 56.1943\n",
            "Epoch 94/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 894.0132 - val_loss: 59.3306\n",
            "Epoch 95/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 892.6895 - val_loss: 58.5835\n",
            "Epoch 96/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 891.6741 - val_loss: 59.2167\n",
            "Epoch 97/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 891.2349 - val_loss: 59.0735\n",
            "Epoch 98/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 891.6823 - val_loss: 60.5655\n",
            "Epoch 99/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 890.2861 - val_loss: 62.0482\n",
            "Epoch 100/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 889.9349 - val_loss: 63.8159\n",
            "Epoch 101/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 889.2740 - val_loss: 64.1086\n",
            "Epoch 102/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 889.2347 - val_loss: 64.6097\n",
            "Epoch 103/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 888.7416 - val_loss: 66.4323\n",
            "Epoch 104/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 888.0977 - val_loss: 66.7667\n",
            "Epoch 105/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 887.7187 - val_loss: 67.5375\n",
            "Epoch 106/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 887.3161 - val_loss: 69.3554\n",
            "Epoch 107/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 887.1530 - val_loss: 70.8224\n",
            "Epoch 108/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 886.8706 - val_loss: 69.5301\n",
            "Epoch 109/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 886.7002 - val_loss: 72.1173\n",
            "Epoch 110/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 886.0214 - val_loss: 72.6899\n",
            "Epoch 111/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 886.0877 - val_loss: 73.2453\n",
            "Epoch 112/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 885.7712 - val_loss: 73.9332\n",
            "Epoch 113/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 886.1669 - val_loss: 74.3052\n",
            "Epoch 114/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 886.8032 - val_loss: 76.4016\n",
            "Epoch 115/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 884.9624 - val_loss: 75.2055\n",
            "Epoch 116/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 885.2880 - val_loss: 77.4782\n",
            "Epoch 117/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 884.7468 - val_loss: 77.4969\n",
            "Epoch 118/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 885.0025 - val_loss: 77.5691\n",
            "Epoch 119/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 884.7661 - val_loss: 78.7772\n",
            "Epoch 120/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 884.5135 - val_loss: 79.6950\n",
            "Epoch 121/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 884.2391 - val_loss: 79.6225\n",
            "Epoch 122/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 883.8690 - val_loss: 80.0948\n",
            "Epoch 123/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 883.4427 - val_loss: 82.4915\n",
            "Epoch 124/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 883.3806 - val_loss: 81.1037\n",
            "Epoch 125/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 883.3983 - val_loss: 82.3215\n",
            "Epoch 126/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 883.1934 - val_loss: 82.1901\n",
            "Epoch 127/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 883.3325 - val_loss: 82.8070\n",
            "Epoch 128/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 883.2930 - val_loss: 83.3577\n",
            "Epoch 129/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 882.8593 - val_loss: 83.9119\n",
            "Epoch 130/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 882.9274 - val_loss: 84.6532\n",
            "Epoch 131/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 882.9247 - val_loss: 84.0511\n",
            "Epoch 132/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 883.0353 - val_loss: 84.6242\n",
            "Epoch 133/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 882.3427 - val_loss: 85.0808\n",
            "Epoch 134/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 882.6938 - val_loss: 86.4834\n",
            "Epoch 135/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 882.7236 - val_loss: 87.2368\n",
            "Epoch 136/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 882.7085 - val_loss: 87.1688\n",
            "Epoch 137/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 882.5696 - val_loss: 88.4473\n",
            "Epoch 138/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 882.5948 - val_loss: 87.8953\n",
            "Epoch 139/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 882.1342 - val_loss: 88.0432\n",
            "Epoch 140/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 881.7713 - val_loss: 88.1907\n",
            "Epoch 141/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 882.5120 - val_loss: 88.2406\n",
            "Epoch 142/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 882.6766 - val_loss: 89.8808\n",
            "Epoch 143/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 882.6484 - val_loss: 89.8234\n",
            "Epoch 144/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 881.6424 - val_loss: 90.4882\n",
            "Epoch 145/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 881.7065 - val_loss: 89.8810\n",
            "Epoch 146/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 881.8237 - val_loss: 90.9922\n",
            "Epoch 147/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 881.9323 - val_loss: 90.7202\n",
            "Epoch 148/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 881.6284 - val_loss: 91.4379\n",
            "Epoch 149/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 881.8405 - val_loss: 91.2562\n",
            "Epoch 150/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 882.0026 - val_loss: 91.5698\n",
            "Epoch 151/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 882.4507 - val_loss: 91.3916\n",
            "Epoch 152/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 881.5229 - val_loss: 92.5977\n",
            "Epoch 153/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 881.2366 - val_loss: 92.3861\n",
            "Epoch 154/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 881.1810 - val_loss: 92.9852\n",
            "Epoch 155/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 881.1048 - val_loss: 92.9474\n",
            "Epoch 156/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 881.4755 - val_loss: 93.7181\n",
            "Epoch 157/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 881.2874 - val_loss: 94.5216\n",
            "Epoch 158/200\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 881.0981 - val_loss: 94.1681\n",
            "Epoch 159/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 881.0419 - val_loss: 94.2847\n",
            "Epoch 160/200\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 880.8678 - val_loss: 94.7204\n",
            "Epoch 161/200\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 880.8875 - val_loss: 95.7053\n",
            "Epoch 162/200\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 880.6161 - val_loss: 95.2065\n",
            "Epoch 163/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 880.6342 - val_loss: 95.5447\n",
            "Epoch 164/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 880.6082 - val_loss: 95.8022\n",
            "Epoch 165/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 880.5359 - val_loss: 95.9727\n",
            "Epoch 166/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 880.8638 - val_loss: 96.5512\n",
            "Epoch 167/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 880.5706 - val_loss: 96.1623\n",
            "Epoch 168/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 881.1214 - val_loss: 97.5508\n",
            "Epoch 169/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 880.3856 - val_loss: 97.3847\n",
            "Epoch 170/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 880.4708 - val_loss: 97.2732\n",
            "Epoch 171/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 880.3858 - val_loss: 97.6606\n",
            "Epoch 172/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 880.5164 - val_loss: 97.0222\n",
            "Epoch 173/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 880.2919 - val_loss: 97.4982\n",
            "Epoch 174/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 881.0920 - val_loss: 97.7986\n",
            "Epoch 175/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 880.6137 - val_loss: 98.7882\n",
            "Epoch 176/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 880.5598 - val_loss: 98.4036\n",
            "Epoch 177/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 880.4527 - val_loss: 98.3384\n",
            "Epoch 178/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 880.2521 - val_loss: 98.8888\n",
            "Epoch 179/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 880.3758 - val_loss: 97.8639\n",
            "Epoch 180/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 880.3942 - val_loss: 98.6305\n",
            "Epoch 181/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 880.0668 - val_loss: 98.5119\n",
            "Epoch 182/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 880.2391 - val_loss: 98.5894\n",
            "Epoch 183/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 880.0718 - val_loss: 98.2702\n",
            "Epoch 184/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 880.0506 - val_loss: 98.9045\n",
            "Epoch 185/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 880.0344 - val_loss: 99.2796\n",
            "Epoch 186/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 880.2135 - val_loss: 99.7380\n",
            "Epoch 187/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 880.0915 - val_loss: 99.2616\n",
            "Epoch 188/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 880.1242 - val_loss: 99.1675\n",
            "Epoch 189/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 880.1776 - val_loss: 99.3115\n",
            "Epoch 190/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 879.9891 - val_loss: 100.0100\n",
            "Epoch 191/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 879.8759 - val_loss: 100.1670\n",
            "Epoch 192/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 880.0251 - val_loss: 100.6597\n",
            "Epoch 193/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 879.8756 - val_loss: 100.6489\n",
            "Epoch 194/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 879.7561 - val_loss: 100.7799\n",
            "Epoch 195/200\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 879.8976 - val_loss: 100.3473\n",
            "Epoch 196/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 880.1580 - val_loss: 99.7812\n",
            "Epoch 197/200\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 879.5381 - val_loss: 100.7830\n",
            "Epoch 198/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 879.8985 - val_loss: 100.8416\n",
            "Epoch 199/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 880.0208 - val_loss: 100.6896\n",
            "Epoch 200/200\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 879.8297 - val_loss: 101.2650\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "Mean Squared Error on Test Data: 101.27\n",
            "Prediction: 11.66, Actual: 8.55\n",
            "Prediction: 11.21, Actual: 14.46\n",
            "Prediction: 24.66, Actual: 8.50\n",
            "Prediction: 9.60, Actual: 5.00\n",
            "Prediction: 9.88, Actual: 2.79\n",
            "Prediction: 3.61, Actual: 8.01\n",
            "Prediction: 24.66, Actual: 14.75\n",
            "Prediction: 24.66, Actual: 8.03\n",
            "Prediction: 4.12, Actual: 2.21\n",
            "Prediction: 8.96, Actual: 5.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8pqwMRWP7kq4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}