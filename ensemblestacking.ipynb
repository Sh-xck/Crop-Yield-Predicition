{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e12fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_11788\\2083593475.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[features] = scaler.fit_transform(X[features])\n",
      "[I 2024-02-23 10:56:41,882] A new study created in memory with name: no-name-9c5cacee-aeb7-466d-bd9b-37521d93a13a\n",
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_11788\\2083593475.py:31: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "[I 2024-02-23 10:57:11,110] Trial 0 finished with value: 7.602436065673828 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 72, 'num_neurons_layer_1': 46, 'num_neurons_layer_2': 67, 'num_neurons_layer_3': 63, 'learning_rate': 0.0026295448562702775}. Best is trial 0 with value: 7.602436065673828.\n",
      "[I 2024-02-23 10:57:32,486] Trial 1 finished with value: 7.016202926635742 and parameters: {'num_hidden_layers': 2, 'num_neurons_layer_0': 50, 'num_neurons_layer_1': 96, 'learning_rate': 0.07347838619355854}. Best is trial 1 with value: 7.016202926635742.\n",
      "[I 2024-02-23 10:57:52,462] Trial 2 finished with value: 12.890482902526855 and parameters: {'num_hidden_layers': 2, 'num_neurons_layer_0': 25, 'num_neurons_layer_1': 71, 'learning_rate': 0.004339590249576147}. Best is trial 1 with value: 7.016202926635742.\n",
      "[I 2024-02-23 10:58:13,053] Trial 3 finished with value: 6.541358947753906 and parameters: {'num_hidden_layers': 2, 'num_neurons_layer_0': 26, 'num_neurons_layer_1': 36, 'learning_rate': 8.415546363791755e-05}. Best is trial 3 with value: 6.541358947753906.\n",
      "[I 2024-02-23 10:58:38,426] Trial 4 finished with value: 8.51812744140625 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 99, 'num_neurons_layer_1': 78, 'num_neurons_layer_2': 23, 'num_neurons_layer_3': 12, 'learning_rate': 0.024842295858487573}. Best is trial 3 with value: 6.541358947753906.\n",
      "[I 2024-02-23 10:59:02,901] Trial 5 finished with value: 10.164395332336426 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 6, 'num_neurons_layer_1': 46, 'num_neurons_layer_2': 25, 'num_neurons_layer_3': 40, 'learning_rate': 9.781292483117622e-05}. Best is trial 3 with value: 6.541358947753906.\n",
      "[I 2024-02-23 10:59:20,616] Trial 6 finished with value: 11.160238265991211 and parameters: {'num_hidden_layers': 1, 'num_neurons_layer_0': 92, 'learning_rate': 0.09038813205307493}. Best is trial 3 with value: 6.541358947753906.\n",
      "[I 2024-02-23 10:59:43,204] Trial 7 finished with value: 5.579325199127197 and parameters: {'num_hidden_layers': 3, 'num_neurons_layer_0': 73, 'num_neurons_layer_1': 8, 'num_neurons_layer_2': 32, 'learning_rate': 0.004477164358000069}. Best is trial 7 with value: 5.579325199127197.\n",
      "[I 2024-02-23 11:00:01,572] Trial 8 finished with value: 8.993743896484375 and parameters: {'num_hidden_layers': 1, 'num_neurons_layer_0': 46, 'learning_rate': 0.001857664809413378}. Best is trial 7 with value: 5.579325199127197.\n",
      "[I 2024-02-23 11:00:27,043] Trial 9 finished with value: 9.99797534942627 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 71, 'num_neurons_layer_1': 52, 'num_neurons_layer_2': 49, 'num_neurons_layer_3': 18, 'learning_rate': 7.231879702272273e-05}. Best is trial 7 with value: 5.579325199127197.\n",
      "[I 2024-02-23 11:00:50,603] Trial 10 finished with value: 10.924479484558105 and parameters: {'num_hidden_layers': 3, 'num_neurons_layer_0': 76, 'num_neurons_layer_1': 5, 'num_neurons_layer_2': 98, 'learning_rate': 1.1195714874366214e-05}. Best is trial 7 with value: 5.579325199127197.\n",
      "[I 2024-02-23 11:01:13,124] Trial 11 finished with value: 9.600229263305664 and parameters: {'num_hidden_layers': 3, 'num_neurons_layer_0': 31, 'num_neurons_layer_1': 11, 'num_neurons_layer_2': 6, 'learning_rate': 0.00027858369273248905}. Best is trial 7 with value: 5.579325199127197.\n",
      "[I 2024-02-23 11:01:34,119] Trial 12 finished with value: 9.138538360595703 and parameters: {'num_hidden_layers': 2, 'num_neurons_layer_0': 7, 'num_neurons_layer_1': 26, 'learning_rate': 0.0005292155807536622}. Best is trial 7 with value: 5.579325199127197.\n",
      "[I 2024-02-23 11:01:57,022] Trial 13 finished with value: 7.031904220581055 and parameters: {'num_hidden_layers': 3, 'num_neurons_layer_0': 29, 'num_neurons_layer_1': 25, 'num_neurons_layer_2': 54, 'learning_rate': 0.009279345037539216}. Best is trial 7 with value: 5.579325199127197.\n",
      "[I 2024-02-23 11:02:17,611] Trial 14 finished with value: 10.612055778503418 and parameters: {'num_hidden_layers': 2, 'num_neurons_layer_0': 62, 'num_neurons_layer_1': 29, 'learning_rate': 1.5898860941359876e-05}. Best is trial 7 with value: 5.579325199127197.\n",
      "[I 2024-02-23 11:03:52,350] Trial 15 finished with value: 9.97616958618164 and parameters: {'num_hidden_layers': 3, 'num_neurons_layer_0': 37, 'num_neurons_layer_1': 16, 'num_neurons_layer_2': 36, 'learning_rate': 0.00016074759315086473}. Best is trial 7 with value: 5.579325199127197.\n",
      "[W 2024-02-23 11:04:00,872] Trial 16 failed with parameters: {'num_hidden_layers': 1, 'num_neurons_layer_0': 15, 'learning_rate': 0.0006551384564602233} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_11788\\2083593475.py\", line 46, in objective\n",
      "    model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_test, y_test), verbose=0)\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1501, in fit\n",
      "    data_handler = data_adapter.get_data_handler(\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py\", line 1582, in get_data_handler\n",
      "    return DataHandler(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py\", line 1262, in __init__\n",
      "    self._adapter = adapter_cls(\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py\", line 247, in __init__\n",
      "    x, y, sample_weights = _process_tensorlike((x, y, sample_weights))\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py\", line 1142, in _process_tensorlike\n",
      "    inputs = tf.nest.map_structure(_convert_single_tensor, inputs)\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\nest.py\", line 917, in map_structure\n",
      "    structure[0], [func(*x) for x in entries],\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\nest.py\", line 917, in <listcomp>\n",
      "    structure[0], [func(*x) for x in entries],\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py\", line 1137, in _convert_single_tensor\n",
      "    return tf.convert_to_tensor(x, dtype=dtype)\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1176, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1492, in convert_to_tensor_v2_with_dispatch\n",
      "    return convert_to_tensor_v2(\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1498, in convert_to_tensor_v2\n",
      "    return convert_to_tensor(\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\profiler\\trace.py\", line 183, in wrapped\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1638, in convert_to_tensor\n",
      "    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py\", line 48, in _default_conversion_function\n",
      "    return constant_op.constant(value, dtype, name=name)\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 267, in constant\n",
      "    return _constant_impl(value, dtype, shape, name, verify_shape=False,\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 279, in _constant_impl\n",
      "    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n",
      "  File \"C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 304, in _constant_eager_impl\n",
      "    t = convert_to_eager_tensor(value, ctx, dtype)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-02-23 11:04:01,027] Trial 16 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 56>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Create a study object and optimize the objective function\u001b[39;00m\n\u001b[0;32m     55\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# You can adjust the number of trials\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters\u001b[39;00m\n\u001b[0;32m     59\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     43\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(lr\u001b[38;5;241m=\u001b[39mlearning_rate), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[RootMeanSquaredError()])\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[0;32m     49\u001b[0m results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py:1501\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1491\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_coordinator \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1492\u001b[0m         tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mcoordinator\u001b[38;5;241m.\u001b[39mClusterCoordinator(\n\u001b[0;32m   1493\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\n\u001b[0;32m   1494\u001b[0m         )\n\u001b[0;32m   1495\u001b[0m     )\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m     \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   1499\u001b[0m ):\n\u001b[0;32m   1500\u001b[0m     \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1501\u001b[0m     data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1513\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1515\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1518\u001b[0m     \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py:1582\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py:1262\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[0;32m   1261\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1277\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py:247\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    236\u001b[0m     x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    245\u001b[0m ):\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 247\u001b[0m     x, y, sample_weights \u001b[38;5;241m=\u001b[39m \u001b[43m_process_tensorlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m     sample_weight_modes \u001b[38;5;241m=\u001b[39m broadcast_sample_weight_modes(\n\u001b[0;32m    249\u001b[0m         sample_weights, sample_weight_modes\n\u001b[0;32m    250\u001b[0m     )\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;66;03m# If sample_weights are not specified for an output use 1.0 as weights.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py:1142\u001b[0m, in \u001b[0;36m_process_tensorlike\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n\u001b[0;32m   1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m-> 1142\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_convert_single_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mlist_to_tuple(inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py:1137\u001b[0m, in \u001b[0;36m_process_tensorlike.<locals>._convert_single_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39mfloating):\n\u001b[0;32m   1136\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mfloatx()\n\u001b[1;32m-> 1137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_scipy_sparse(x):\n\u001b[0;32m   1139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py:1492\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[0;32m   1431\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1432\u001b[0m   \u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[0;32m   1433\u001b[0m \n\u001b[0;32m   1434\u001b[0m \u001b[38;5;124;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;124;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[0;32m   1491\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1492\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py:1498\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1497\u001b[0m   \u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1498\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1499\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1500\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py:1638\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1629\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1630\u001b[0m           _add_error_prefix(\n\u001b[0;32m   1631\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1634\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1635\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m   1637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1638\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1641\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:48\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[0;32m     47\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m as_ref  \u001b[38;5;66;03m# Unused.\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    172\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    278\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 279\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[0;32m    282\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    303\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Assuming 'df' is your DataFrame with the provided data\n",
    "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
    "df = pd.read_csv('Onion.csv')\n",
    "# Select relevant features for training\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df[features]\n",
    "y = df['Yield']\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X[features] = scaler.fit_transform(X[features])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.1, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 4)\n",
    "    num_neurons = [trial.suggest_int(f'num_neurons_layer_{i}', 5, 100) for i in range(num_hidden_layers)]\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons[0], activation='swish', input_dim=len(features)))\n",
    "    \n",
    "    for i in range(1, num_hidden_layers):\n",
    "        model.add(Dense(num_neurons[i], activation='gelu'))\n",
    "    \n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    rmse_value = results[1]\n",
    "\n",
    "    return rmse_value\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "best_rmse = study.best_value\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best RMSE: {best_rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5faa8ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_11788\\2509214349.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[features] = scaler.fit_transform(X[features])\n",
      "[I 2024-02-23 12:21:45,489] A new study created in memory with name: no-name-242edd48-d9d3-4eb2-b2fa-da4fb865508a\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "[I 2024-02-23 12:22:39,063] Trial 0 finished with value: 6.665468215942383 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 50, 'num_neurons_layer_1': 42, 'num_neurons_layer_2': 96, 'num_neurons_layer_3': 81, 'num_neurons_layer_4': 47, 'num_epochs': 384, 'batch_size': 32}. Best is trial 0 with value: 6.665468215942383.\n",
      "[I 2024-02-23 12:23:07,389] Trial 1 finished with value: 5.544250965118408 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 15, 'num_neurons_layer_1': 49, 'num_neurons_layer_2': 27, 'num_neurons_layer_3': 15, 'num_neurons_layer_4': 48, 'num_epochs': 127, 'batch_size': 10}. Best is trial 1 with value: 5.544250965118408.\n",
      "[I 2024-02-23 12:23:32,539] Trial 2 finished with value: 9.882580757141113 and parameters: {'num_hidden_layers': 3, 'num_neurons_layer_0': 63, 'num_neurons_layer_1': 35, 'num_neurons_layer_2': 10, 'num_epochs': 93, 'batch_size': 9}. Best is trial 1 with value: 5.544250965118408.\n",
      "[I 2024-02-23 12:23:56,699] Trial 3 finished with value: 6.9410624504089355 and parameters: {'num_hidden_layers': 3, 'num_neurons_layer_0': 51, 'num_neurons_layer_1': 43, 'num_neurons_layer_2': 46, 'num_epochs': 220, 'batch_size': 37}. Best is trial 1 with value: 5.544250965118408.\n",
      "[I 2024-02-23 12:25:00,830] Trial 4 finished with value: 8.04916000366211 and parameters: {'num_hidden_layers': 1, 'num_neurons_layer_0': 21, 'num_epochs': 478, 'batch_size': 20}. Best is trial 1 with value: 5.544250965118408.\n",
      "[I 2024-02-23 12:25:50,252] Trial 5 finished with value: 6.138622283935547 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 14, 'num_neurons_layer_1': 39, 'num_neurons_layer_2': 58, 'num_neurons_layer_3': 98, 'num_epochs': 476, 'batch_size': 47}. Best is trial 1 with value: 5.544250965118408.\n",
      "[I 2024-02-23 12:26:52,585] Trial 6 finished with value: 5.59998893737793 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 22, 'num_neurons_layer_1': 46, 'num_neurons_layer_2': 46, 'num_neurons_layer_3': 67, 'num_neurons_layer_4': 56, 'num_epochs': 311, 'batch_size': 14}. Best is trial 1 with value: 5.544250965118408.\n",
      "[I 2024-02-23 12:27:23,277] Trial 7 finished with value: 9.275921821594238 and parameters: {'num_hidden_layers': 2, 'num_neurons_layer_0': 51, 'num_neurons_layer_1': 44, 'num_epochs': 199, 'batch_size': 18}. Best is trial 1 with value: 5.544250965118408.\n",
      "[I 2024-02-23 12:28:07,041] Trial 8 finished with value: 11.537062644958496 and parameters: {'num_hidden_layers': 3, 'num_neurons_layer_0': 42, 'num_neurons_layer_1': 55, 'num_neurons_layer_2': 9, 'num_epochs': 391, 'batch_size': 32}. Best is trial 1 with value: 5.544250965118408.\n",
      "[I 2024-02-23 12:28:56,720] Trial 9 finished with value: 8.01020336151123 and parameters: {'num_hidden_layers': 1, 'num_neurons_layer_0': 69, 'num_epochs': 314, 'batch_size': 14}. Best is trial 1 with value: 5.544250965118408.\n",
      "[I 2024-02-23 12:29:20,723] Trial 10 finished with value: 14.47362232208252 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 100, 'num_neurons_layer_1': 88, 'num_neurons_layer_2': 30, 'num_neurons_layer_3': 6, 'num_epochs': 66, 'batch_size': 5}. Best is trial 1 with value: 5.544250965118408.\n",
      "[I 2024-02-23 12:29:44,722] Trial 11 finished with value: 13.292591094970703 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 6, 'num_neurons_layer_1': 11, 'num_neurons_layer_2': 52, 'num_neurons_layer_3': 36, 'num_neurons_layer_4': 58, 'num_epochs': 155, 'batch_size': 24}. Best is trial 1 with value: 5.544250965118408.\n",
      "[I 2024-02-23 12:30:46,790] Trial 12 finished with value: 6.510430812835693 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 30, 'num_neurons_layer_1': 72, 'num_neurons_layer_2': 32, 'num_neurons_layer_3': 55, 'num_neurons_layer_4': 35, 'num_epochs': 295, 'batch_size': 10}. Best is trial 1 with value: 5.544250965118408.\n",
      "[I 2024-02-23 12:31:29,703] Trial 13 finished with value: 5.875217914581299 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 31, 'num_neurons_layer_1': 64, 'num_neurons_layer_2': 75, 'num_neurons_layer_3': 55, 'num_epochs': 238, 'batch_size': 16}. Best is trial 1 with value: 5.544250965118408.\n",
      "[I 2024-02-23 12:31:50,639] Trial 14 finished with value: 8.214923858642578 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 25, 'num_neurons_layer_1': 21, 'num_neurons_layer_2': 30, 'num_neurons_layer_3': 6, 'num_neurons_layer_4': 97, 'num_epochs': 136, 'batch_size': 24}. Best is trial 1 with value: 5.544250965118408.\n",
      "[I 2024-02-23 12:33:53,624] Trial 15 finished with value: 7.623324871063232 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 6, 'num_neurons_layer_1': 75, 'num_neurons_layer_2': 65, 'num_neurons_layer_3': 34, 'num_epochs': 350, 'batch_size': 5}. Best is trial 1 with value: 5.544250965118408.\n",
      "[I 2024-02-23 12:34:56,352] Trial 16 finished with value: 5.931811809539795 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 38, 'num_neurons_layer_1': 25, 'num_neurons_layer_2': 43, 'num_neurons_layer_3': 74, 'num_neurons_layer_4': 68, 'num_epochs': 265, 'batch_size': 11}. Best is trial 1 with value: 5.544250965118408.\n",
      "[I 2024-02-23 12:37:20,138] Trial 17 finished with value: 8.21963882446289 and parameters: {'num_hidden_layers': 2, 'num_neurons_layer_0': 18, 'num_neurons_layer_1': 57, 'num_epochs': 134, 'batch_size': 22}. Best is trial 1 with value: 5.544250965118408.\n",
      "[I 2024-02-23 12:37:29,331] Trial 18 finished with value: 5.32985782623291 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 90, 'num_neurons_layer_1': 98, 'num_neurons_layer_2': 21, 'num_neurons_layer_3': 32, 'num_epochs': 183, 'batch_size': 45}. Best is trial 18 with value: 5.32985782623291.\n",
      "[I 2024-02-23 12:37:37,928] Trial 19 finished with value: 12.166236877441406 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 95, 'num_neurons_layer_1': 94, 'num_neurons_layer_2': 20, 'num_neurons_layer_3': 29, 'num_epochs': 184, 'batch_size': 47}. Best is trial 18 with value: 5.32985782623291.\n",
      "[I 2024-02-23 12:37:42,605] Trial 20 finished with value: 9.503419876098633 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 79, 'num_neurons_layer_1': 82, 'num_neurons_layer_2': 17, 'num_neurons_layer_3': 23, 'num_epochs': 93, 'batch_size': 40}. Best is trial 18 with value: 5.32985782623291.\n",
      "[I 2024-02-23 12:37:51,033] Trial 21 finished with value: 7.0166850090026855 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 82, 'num_neurons_layer_1': 62, 'num_neurons_layer_2': 34, 'num_neurons_layer_3': 19, 'num_neurons_layer_4': 16, 'num_epochs': 174, 'batch_size': 41}. Best is trial 18 with value: 5.32985782623291.\n",
      "[I 2024-02-23 12:38:03,251] Trial 22 finished with value: 5.267504692077637 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 38, 'num_neurons_layer_1': 30, 'num_neurons_layer_2': 21, 'num_neurons_layer_3': 45, 'num_neurons_layer_4': 74, 'num_epochs': 252, 'batch_size': 28}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:38:16,065] Trial 23 finished with value: 6.445281505584717 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 63, 'num_neurons_layer_1': 31, 'num_neurons_layer_2': 20, 'num_neurons_layer_3': 43, 'num_epochs': 254, 'batch_size': 29}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:38:21,870] Trial 24 finished with value: 7.103892803192139 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 40, 'num_neurons_layer_1': 99, 'num_neurons_layer_2': 6, 'num_neurons_layer_3': 46, 'num_neurons_layer_4': 81, 'num_epochs': 109, 'batch_size': 36}. Best is trial 22 with value: 5.267504692077637.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-23 12:38:31,200] Trial 25 finished with value: 5.423683166503906 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 84, 'num_neurons_layer_1': 11, 'num_neurons_layer_2': 23, 'num_neurons_layer_3': 14, 'num_epochs': 210, 'batch_size': 43}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:38:38,990] Trial 26 finished with value: 9.041749954223633 and parameters: {'num_hidden_layers': 3, 'num_neurons_layer_0': 87, 'num_neurons_layer_1': 11, 'num_neurons_layer_2': 17, 'num_epochs': 217, 'batch_size': 50}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:38:52,088] Trial 27 finished with value: 6.078712463378906 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 91, 'num_neurons_layer_1': 17, 'num_neurons_layer_2': 38, 'num_neurons_layer_3': 27, 'num_epochs': 283, 'batch_size': 43}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:39:00,966] Trial 28 finished with value: 8.330650329589844 and parameters: {'num_hidden_layers': 3, 'num_neurons_layer_0': 75, 'num_neurons_layer_1': 7, 'num_neurons_layer_2': 23, 'num_epochs': 171, 'batch_size': 45}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:39:18,478] Trial 29 finished with value: 8.860477447509766 and parameters: {'num_hidden_layers': 2, 'num_neurons_layer_0': 58, 'num_neurons_layer_1': 27, 'num_epochs': 359, 'batch_size': 35}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:39:31,658] Trial 30 finished with value: 5.831806659698486 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 69, 'num_neurons_layer_1': 19, 'num_neurons_layer_2': 81, 'num_neurons_layer_3': 42, 'num_epochs': 232, 'batch_size': 29}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:39:39,556] Trial 31 finished with value: 6.487148284912109 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 87, 'num_neurons_layer_1': 49, 'num_neurons_layer_2': 26, 'num_neurons_layer_3': 15, 'num_neurons_layer_4': 78, 'num_epochs': 139, 'batch_size': 39}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:39:50,654] Trial 32 finished with value: 6.29829216003418 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 44, 'num_neurons_layer_1': 31, 'num_neurons_layer_2': 11, 'num_neurons_layer_3': 16, 'num_neurons_layer_4': 31, 'num_epochs': 200, 'batch_size': 33}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:39:56,990] Trial 33 finished with value: 11.670578002929688 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 14, 'num_neurons_layer_1': 5, 'num_neurons_layer_2': 39, 'num_neurons_layer_3': 11, 'num_neurons_layer_4': 95, 'num_epochs': 117, 'batch_size': 44}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:40:07,510] Trial 34 finished with value: 10.515767097473145 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 33, 'num_neurons_layer_1': 36, 'num_neurons_layer_2': 15, 'num_neurons_layer_3': 23, 'num_epochs': 203, 'batch_size': 49}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:40:11,589] Trial 35 finished with value: 9.728153228759766 and parameters: {'num_hidden_layers': 3, 'num_neurons_layer_0': 98, 'num_neurons_layer_1': 15, 'num_neurons_layer_2': 100, 'num_epochs': 58, 'batch_size': 26}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:40:27,902] Trial 36 finished with value: 5.409375190734863 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 47, 'num_neurons_layer_1': 70, 'num_neurons_layer_2': 26, 'num_neurons_layer_3': 33, 'num_neurons_layer_4': 40, 'num_epochs': 252, 'batch_size': 41}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:40:42,056] Trial 37 finished with value: 5.464269638061523 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 58, 'num_neurons_layer_1': 80, 'num_neurons_layer_2': 26, 'num_neurons_layer_3': 50, 'num_neurons_layer_4': 24, 'num_epochs': 234, 'batch_size': 42}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:40:56,616] Trial 38 finished with value: 12.203463554382324 and parameters: {'num_hidden_layers': 3, 'num_neurons_layer_0': 47, 'num_neurons_layer_1': 91, 'num_neurons_layer_2': 12, 'num_epochs': 256, 'batch_size': 38}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:41:16,449] Trial 39 finished with value: 5.812687397003174 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 55, 'num_neurons_layer_1': 64, 'num_neurons_layer_2': 38, 'num_neurons_layer_3': 34, 'num_epochs': 424, 'batch_size': 47}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:41:33,748] Trial 40 finished with value: 7.130718231201172 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 72, 'num_neurons_layer_1': 100, 'num_neurons_layer_2': 6, 'num_neurons_layer_3': 60, 'num_epochs': 324, 'batch_size': 34}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:41:45,980] Trial 41 finished with value: 5.331636428833008 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 58, 'num_neurons_layer_1': 81, 'num_neurons_layer_2': 25, 'num_neurons_layer_3': 49, 'num_neurons_layer_4': 15, 'num_epochs': 234, 'batch_size': 42}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:42:00,198] Trial 42 finished with value: 5.938449382781982 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 63, 'num_neurons_layer_1': 85, 'num_neurons_layer_2': 26, 'num_neurons_layer_3': 40, 'num_neurons_layer_4': 5, 'num_epochs': 278, 'batch_size': 46}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:42:13,113] Trial 43 finished with value: 9.6665620803833 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 47, 'num_neurons_layer_1': 71, 'num_neurons_layer_2': 21, 'num_neurons_layer_3': 51, 'num_neurons_layer_4': 5, 'num_epochs': 220, 'batch_size': 42}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:42:29,852] Trial 44 finished with value: 5.557279109954834 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 37, 'num_neurons_layer_1': 77, 'num_neurons_layer_2': 35, 'num_neurons_layer_3': 28, 'num_neurons_layer_4': 37, 'num_epochs': 296, 'batch_size': 37}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:42:40,006] Trial 45 finished with value: 5.7812180519104 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 83, 'num_neurons_layer_1': 93, 'num_neurons_layer_2': 15, 'num_neurons_layer_3': 62, 'num_neurons_layer_4': 21, 'num_epochs': 161, 'batch_size': 31}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:42:52,502] Trial 46 finished with value: 5.667985439300537 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 53, 'num_neurons_layer_1': 70, 'num_neurons_layer_2': 51, 'num_neurons_layer_3': 34, 'num_epochs': 250, 'batch_size': 48}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:43:02,518] Trial 47 finished with value: 5.867778778076172 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 92, 'num_neurons_layer_1': 40, 'num_neurons_layer_2': 28, 'num_neurons_layer_3': 92, 'num_neurons_layer_4': 65, 'num_epochs': 190, 'batch_size': 44}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:43:20,624] Trial 48 finished with value: 12.552943229675293 and parameters: {'num_hidden_layers': 3, 'num_neurons_layer_0': 26, 'num_neurons_layer_1': 85, 'num_neurons_layer_2': 22, 'num_epochs': 330, 'batch_size': 40}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:43:32,205] Trial 49 finished with value: 9.284924507141113 and parameters: {'num_hidden_layers': 2, 'num_neurons_layer_0': 77, 'num_neurons_layer_1': 58, 'num_epochs': 215, 'batch_size': 38}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:43:56,234] Trial 50 finished with value: 5.935637474060059 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 66, 'num_neurons_layer_1': 50, 'num_neurons_layer_2': 59, 'num_neurons_layer_3': 48, 'num_neurons_layer_4': 80, 'num_epochs': 294, 'batch_size': 20}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:44:10,098] Trial 51 finished with value: 6.422189235687256 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 58, 'num_neurons_layer_1': 78, 'num_neurons_layer_2': 25, 'num_neurons_layer_3': 50, 'num_neurons_layer_4': 23, 'num_epochs': 237, 'batch_size': 42}. Best is trial 22 with value: 5.267504692077637.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-23 12:44:21,557] Trial 52 finished with value: 5.641890525817871 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 59, 'num_neurons_layer_1': 81, 'num_neurons_layer_2': 30, 'num_neurons_layer_3': 39, 'num_neurons_layer_4': 15, 'num_epochs': 235, 'batch_size': 43}. Best is trial 22 with value: 5.267504692077637.\n",
      "[I 2024-02-23 12:44:34,604] Trial 53 finished with value: 5.232136249542236 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 49, 'num_neurons_layer_1': 67, 'num_neurons_layer_2': 45, 'num_neurons_layer_3': 59, 'num_neurons_layer_4': 28, 'num_epochs': 266, 'batch_size': 46}. Best is trial 53 with value: 5.232136249542236.\n",
      "[I 2024-02-23 12:44:47,312] Trial 54 finished with value: 5.737349987030029 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 47, 'num_neurons_layer_1': 68, 'num_neurons_layer_2': 44, 'num_neurons_layer_3': 73, 'num_neurons_layer_4': 42, 'num_epochs': 277, 'batch_size': 46}. Best is trial 53 with value: 5.232136249542236.\n",
      "[I 2024-02-23 12:45:00,713] Trial 55 finished with value: 6.195428371429443 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 43, 'num_neurons_layer_1': 74, 'num_neurons_layer_2': 33, 'num_neurons_layer_3': 58, 'num_epochs': 270, 'batch_size': 49}. Best is trial 53 with value: 5.232136249542236.\n",
      "[I 2024-02-23 12:45:09,921] Trial 56 finished with value: 4.969590187072754 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 38, 'num_neurons_layer_1': 66, 'num_neurons_layer_2': 17, 'num_neurons_layer_3': 64, 'num_neurons_layer_4': 29, 'num_epochs': 191, 'batch_size': 44}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:45:21,834] Trial 57 finished with value: 5.78892707824707 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 36, 'num_neurons_layer_1': 67, 'num_neurons_layer_2': 48, 'num_neurons_layer_3': 68, 'num_neurons_layer_4': 30, 'num_epochs': 252, 'batch_size': 45}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:45:28,526] Trial 58 finished with value: 5.767139434814453 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 33, 'num_neurons_layer_1': 61, 'num_neurons_layer_2': 57, 'num_neurons_layer_3': 56, 'num_neurons_layer_4': 14, 'num_epochs': 152, 'batch_size': 50}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:45:37,942] Trial 59 finished with value: 6.272673606872559 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 50, 'num_neurons_layer_1': 53, 'num_neurons_layer_2': 12, 'num_neurons_layer_3': 65, 'num_neurons_layer_4': 28, 'num_epochs': 184, 'batch_size': 40}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:45:53,750] Trial 60 finished with value: 6.063912868499756 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 28, 'num_neurons_layer_1': 46, 'num_neurons_layer_2': 18, 'num_neurons_layer_3': 81, 'num_neurons_layer_4': 41, 'num_epochs': 310, 'batch_size': 36}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:46:05,477] Trial 61 finished with value: 5.889058589935303 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 41, 'num_neurons_layer_1': 59, 'num_neurons_layer_2': 22, 'num_neurons_layer_3': 45, 'num_epochs': 210, 'batch_size': 45}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:46:16,619] Trial 62 finished with value: 8.751769065856934 and parameters: {'num_hidden_layers': 1, 'num_neurons_layer_0': 50, 'num_epochs': 192, 'batch_size': 41}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:46:26,939] Trial 63 finished with value: 5.640661239624023 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 20, 'num_neurons_layer_1': 67, 'num_neurons_layer_2': 73, 'num_neurons_layer_3': 37, 'num_neurons_layer_4': 50, 'num_epochs': 170, 'batch_size': 47}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:46:39,532] Trial 64 finished with value: 7.766657829284668 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 44, 'num_neurons_layer_1': 96, 'num_neurons_layer_2': 9, 'num_neurons_layer_3': 54, 'num_epochs': 219, 'batch_size': 44}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:46:53,198] Trial 65 finished with value: 5.395977973937988 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 38, 'num_neurons_layer_1': 88, 'num_neurons_layer_2': 18, 'num_neurons_layer_3': 70, 'num_neurons_layer_4': 39, 'num_epochs': 247, 'batch_size': 48}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:47:09,169] Trial 66 finished with value: 5.53729772567749 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 34, 'num_neurons_layer_1': 89, 'num_neurons_layer_2': 18, 'num_neurons_layer_3': 72, 'num_neurons_layer_4': 39, 'num_epochs': 265, 'batch_size': 48}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:47:45,920] Trial 67 finished with value: 5.115645885467529 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 39, 'num_neurons_layer_1': 85, 'num_neurons_layer_2': 14, 'num_neurons_layer_3': 79, 'num_neurons_layer_4': 44, 'num_epochs': 243, 'batch_size': 7}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:48:21,532] Trial 68 finished with value: 5.801482200622559 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 38, 'num_neurons_layer_1': 85, 'num_neurons_layer_2': 14, 'num_neurons_layer_3': 79, 'num_neurons_layer_4': 44, 'num_epochs': 290, 'batch_size': 12}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:48:59,229] Trial 69 finished with value: 6.238837718963623 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 31, 'num_neurons_layer_1': 96, 'num_neurons_layer_2': 5, 'num_neurons_layer_3': 92, 'num_neurons_layer_4': 54, 'num_epochs': 304, 'batch_size': 8}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:49:15,013] Trial 70 finished with value: 6.321868419647217 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 40, 'num_neurons_layer_1': 88, 'num_neurons_layer_2': 9, 'num_neurons_layer_3': 88, 'num_neurons_layer_4': 33, 'num_epochs': 243, 'batch_size': 26}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:49:34,508] Trial 71 finished with value: 7.059111595153809 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 47, 'num_neurons_layer_1': 73, 'num_neurons_layer_2': 19, 'num_neurons_layer_3': 64, 'num_neurons_layer_4': 50, 'num_epochs': 262, 'batch_size': 17}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:50:05,360] Trial 72 finished with value: 5.8948750495910645 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 53, 'num_neurons_layer_1': 82, 'num_neurons_layer_2': 29, 'num_neurons_layer_3': 68, 'num_neurons_layer_4': 44, 'num_epochs': 230, 'batch_size': 7}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:50:22,536] Trial 73 finished with value: 5.637424468994141 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 44, 'num_neurons_layer_1': 77, 'num_neurons_layer_2': 15, 'num_neurons_layer_3': 69, 'num_neurons_layer_4': 27, 'num_epochs': 228, 'batch_size': 22}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:50:34,320] Trial 74 finished with value: 6.506937503814697 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 35, 'num_neurons_layer_1': 91, 'num_neurons_layer_2': 23, 'num_neurons_layer_3': 80, 'num_neurons_layer_4': 35, 'num_epochs': 247, 'batch_size': 46}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:50:49,124] Trial 75 finished with value: 5.605199337005615 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 23, 'num_neurons_layer_1': 85, 'num_neurons_layer_2': 36, 'num_neurons_layer_3': 76, 'num_neurons_layer_4': 58, 'num_epochs': 182, 'batch_size': 14}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:51:01,603] Trial 76 finished with value: 5.332538604736328 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 55, 'num_neurons_layer_1': 63, 'num_neurons_layer_2': 42, 'num_neurons_layer_3': 30, 'num_neurons_layer_4': 48, 'num_epochs': 197, 'batch_size': 31}. Best is trial 56 with value: 4.969590187072754.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-23 12:51:13,248] Trial 77 finished with value: 5.853771209716797 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 29, 'num_neurons_layer_1': 55, 'num_neurons_layer_2': 42, 'num_neurons_layer_3': 86, 'num_neurons_layer_4': 47, 'num_epochs': 205, 'batch_size': 29}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:51:24,256] Trial 78 finished with value: 5.707669258117676 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 39, 'num_neurons_layer_1': 64, 'num_neurons_layer_2': 89, 'num_neurons_layer_3': 59, 'num_neurons_layer_4': 63, 'num_epochs': 195, 'batch_size': 31}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:51:33,341] Trial 79 finished with value: 5.130308628082275 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 61, 'num_neurons_layer_1': 98, 'num_neurons_layer_2': 41, 'num_neurons_layer_3': 25, 'num_epochs': 157, 'batch_size': 26}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:51:42,566] Trial 80 finished with value: 5.331084251403809 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 66, 'num_neurons_layer_1': 98, 'num_neurons_layer_2': 55, 'num_neurons_layer_3': 21, 'num_epochs': 149, 'batch_size': 24}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:51:51,796] Trial 81 finished with value: 5.597517967224121 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 65, 'num_neurons_layer_1': 100, 'num_neurons_layer_2': 54, 'num_neurons_layer_3': 23, 'num_epochs': 148, 'batch_size': 24}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:51:59,138] Trial 82 finished with value: 5.447225570678711 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 60, 'num_neurons_layer_1': 97, 'num_neurons_layer_2': 50, 'num_neurons_layer_3': 19, 'num_epochs': 126, 'batch_size': 26}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:52:07,404] Trial 83 finished with value: 10.113816261291504 and parameters: {'num_hidden_layers': 3, 'num_neurons_layer_0': 71, 'num_neurons_layer_1': 93, 'num_neurons_layer_2': 65, 'num_epochs': 166, 'batch_size': 28}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:52:37,310] Trial 84 finished with value: 5.660211563110352 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 55, 'num_neurons_layer_1': 97, 'num_neurons_layer_2': 41, 'num_neurons_layer_3': 27, 'num_epochs': 489, 'batch_size': 22}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:52:43,283] Trial 85 finished with value: 6.120080947875977 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 61, 'num_neurons_layer_1': 95, 'num_neurons_layer_2': 55, 'num_neurons_layer_3': 31, 'num_epochs': 84, 'batch_size': 20}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:52:52,285] Trial 86 finished with value: 5.760393142700195 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 68, 'num_neurons_layer_1': 92, 'num_neurons_layer_2': 45, 'num_neurons_layer_3': 21, 'num_epochs': 141, 'batch_size': 27}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:52:58,249] Trial 87 finished with value: 10.551998138427734 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 56, 'num_neurons_layer_1': 62, 'num_neurons_layer_2': 61, 'num_neurons_layer_3': 26, 'num_epochs': 107, 'batch_size': 31}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:53:07,362] Trial 88 finished with value: 6.0824503898620605 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 62, 'num_neurons_layer_1': 90, 'num_neurons_layer_2': 46, 'num_neurons_layer_3': 10, 'num_neurons_layer_4': 74, 'num_epochs': 176, 'batch_size': 25}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:53:16,280] Trial 89 finished with value: 9.038329124450684 and parameters: {'num_hidden_layers': 2, 'num_neurons_layer_0': 51, 'num_neurons_layer_1': 98, 'num_epochs': 200, 'batch_size': 33}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:53:25,391] Trial 90 finished with value: 6.592993259429932 and parameters: {'num_hidden_layers': 3, 'num_neurons_layer_0': 73, 'num_neurons_layer_1': 83, 'num_neurons_layer_2': 31, 'num_epochs': 163, 'batch_size': 23}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:53:35,709] Trial 91 finished with value: 5.8159990310668945 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 42, 'num_neurons_layer_1': 88, 'num_neurons_layer_2': 49, 'num_neurons_layer_3': 62, 'num_neurons_layer_4': 18, 'num_epochs': 224, 'batch_size': 48}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:53:42,698] Trial 92 finished with value: 5.5394511222839355 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 56, 'num_neurons_layer_1': 94, 'num_neurons_layer_2': 39, 'num_neurons_layer_3': 43, 'num_neurons_layer_4': 12, 'num_epochs': 127, 'batch_size': 30}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:53:53,832] Trial 93 finished with value: 5.760997295379639 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 65, 'num_neurons_layer_1': 79, 'num_neurons_layer_2': 16, 'num_neurons_layer_3': 53, 'num_neurons_layer_4': 92, 'num_epochs': 155, 'batch_size': 20}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:54:05,508] Trial 94 finished with value: 5.07797384262085 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 49, 'num_neurons_layer_1': 87, 'num_neurons_layer_2': 20, 'num_neurons_layer_3': 76, 'num_neurons_layer_4': 58, 'num_epochs': 282, 'batch_size': 50}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:54:16,881] Trial 95 finished with value: 5.719745635986328 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 52, 'num_neurons_layer_1': 75, 'num_neurons_layer_2': 13, 'num_neurons_layer_3': 31, 'num_neurons_layer_4': 72, 'num_epochs': 271, 'batch_size': 50}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:54:35,316] Trial 96 finished with value: 5.610238552093506 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 48, 'num_neurons_layer_1': 66, 'num_neurons_layer_2': 24, 'num_neurons_layer_3': 76, 'num_neurons_layer_4': 62, 'num_epochs': 284, 'batch_size': 27}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:54:58,436] Trial 97 finished with value: 6.002340793609619 and parameters: {'num_hidden_layers': 4, 'num_neurons_layer_0': 54, 'num_neurons_layer_1': 24, 'num_neurons_layer_2': 20, 'num_neurons_layer_3': 47, 'num_epochs': 325, 'batch_size': 16}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:55:08,944] Trial 98 finished with value: 5.79774808883667 and parameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 58, 'num_neurons_layer_1': 86, 'num_neurons_layer_2': 36, 'num_neurons_layer_3': 17, 'num_neurons_layer_4': 10, 'num_epochs': 180, 'batch_size': 28}. Best is trial 56 with value: 4.969590187072754.\n",
      "[I 2024-02-23 12:55:18,339] Trial 99 finished with value: 9.948101997375488 and parameters: {'num_hidden_layers': 3, 'num_neurons_layer_0': 45, 'num_neurons_layer_1': 100, 'num_neurons_layer_2': 9, 'num_epochs': 208, 'batch_size': 43}. Best is trial 56 with value: 4.969590187072754.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'num_hidden_layers': 5, 'num_neurons_layer_0': 38, 'num_neurons_layer_1': 66, 'num_neurons_layer_2': 17, 'num_neurons_layer_3': 64, 'num_neurons_layer_4': 29, 'num_epochs': 191, 'batch_size': 44}\n",
      "Best RMSE: 4.97\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Assuming 'df' is your DataFrame with the provided data\n",
    "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
    "df = pd.read_csv('Onion.csv')\n",
    "# Select relevant features for training\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df[features]\n",
    "y = df['Yield']\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X[features] = scaler.fit_transform(X[features])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.1, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters\n",
    "    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 5)\n",
    "    num_neurons = [trial.suggest_int(f'num_neurons_layer_{i}', 5, 100) for i in range(num_hidden_layers)]\n",
    "    num_epochs = trial.suggest_int('num_epochs', 50, 500)\n",
    "    batch_size = trial.suggest_int('batch_size', 5, 50)\n",
    "\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons[0], activation='relu', input_dim=len(features)))\n",
    "    \n",
    "    for i in range(1, num_hidden_layers):\n",
    "        model.add(Dense(num_neurons[i], activation='relu'))\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    rmse_value = results[1]\n",
    "\n",
    "    return rmse_value\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "best_rmse = study.best_value\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best RMSE: {best_rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b7fd035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Year</th>\n",
       "      <th>AvgTemp</th>\n",
       "      <th>AvgHumidity</th>\n",
       "      <th>TotalRainfall</th>\n",
       "      <th>Crop</th>\n",
       "      <th>Season</th>\n",
       "      <th>Area</th>\n",
       "      <th>Production</th>\n",
       "      <th>Annual_Rainfall</th>\n",
       "      <th>Fertilizer</th>\n",
       "      <th>Pesticide</th>\n",
       "      <th>Yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>2008</td>\n",
       "      <td>27.2</td>\n",
       "      <td>67</td>\n",
       "      <td>950</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Kharif</td>\n",
       "      <td>26723.0</td>\n",
       "      <td>439644.0</td>\n",
       "      <td>944.6</td>\n",
       "      <td>3822457.92</td>\n",
       "      <td>2405.07</td>\n",
       "      <td>15.308889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>2008</td>\n",
       "      <td>27.2</td>\n",
       "      <td>67</td>\n",
       "      <td>950</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>13737.0</td>\n",
       "      <td>289004.0</td>\n",
       "      <td>944.6</td>\n",
       "      <td>1964940.48</td>\n",
       "      <td>1236.33</td>\n",
       "      <td>18.406316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>2009</td>\n",
       "      <td>26.5</td>\n",
       "      <td>64</td>\n",
       "      <td>820</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Kharif</td>\n",
       "      <td>22954.0</td>\n",
       "      <td>374804.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>3576692.28</td>\n",
       "      <td>3902.18</td>\n",
       "      <td>15.856111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>2009</td>\n",
       "      <td>26.5</td>\n",
       "      <td>64</td>\n",
       "      <td>820</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>13608.0</td>\n",
       "      <td>333288.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>2120398.56</td>\n",
       "      <td>2313.36</td>\n",
       "      <td>25.090526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>2010</td>\n",
       "      <td>27.8</td>\n",
       "      <td>69</td>\n",
       "      <td>1080</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Kharif</td>\n",
       "      <td>25612.0</td>\n",
       "      <td>499564.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>4254409.32</td>\n",
       "      <td>6146.88</td>\n",
       "      <td>17.786250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>Jammu and Kashmir</td>\n",
       "      <td>2017</td>\n",
       "      <td>16.2</td>\n",
       "      <td>72</td>\n",
       "      <td>1050</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>552.0</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>1279.1</td>\n",
       "      <td>86906.88</td>\n",
       "      <td>209.76</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>Jammu and Kashmir</td>\n",
       "      <td>2018</td>\n",
       "      <td>16.9</td>\n",
       "      <td>74</td>\n",
       "      <td>1202</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Kharif</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1016.3</td>\n",
       "      <td>2757.40</td>\n",
       "      <td>5.95</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Jammu and Kashmir</td>\n",
       "      <td>2018</td>\n",
       "      <td>16.9</td>\n",
       "      <td>74</td>\n",
       "      <td>1202</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>550.0</td>\n",
       "      <td>1311.0</td>\n",
       "      <td>1016.3</td>\n",
       "      <td>89210.00</td>\n",
       "      <td>192.50</td>\n",
       "      <td>1.992500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>Jammu and Kashmir</td>\n",
       "      <td>2019</td>\n",
       "      <td>16.5</td>\n",
       "      <td>73</td>\n",
       "      <td>1137</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Kharif</td>\n",
       "      <td>26.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1324.1</td>\n",
       "      <td>4465.76</td>\n",
       "      <td>9.62</td>\n",
       "      <td>0.335000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Jammu and Kashmir</td>\n",
       "      <td>2019</td>\n",
       "      <td>16.5</td>\n",
       "      <td>73</td>\n",
       "      <td>1137</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>561.0</td>\n",
       "      <td>1342.0</td>\n",
       "      <td>1324.1</td>\n",
       "      <td>96357.36</td>\n",
       "      <td>207.57</td>\n",
       "      <td>2.426667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>278 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 State  Year  AvgTemp  AvgHumidity  TotalRainfall   Crop  \\\n",
       "0       Andhra Pradesh  2008     27.2           67            950  Onion   \n",
       "1       Andhra Pradesh  2008     27.2           67            950  Onion   \n",
       "2       Andhra Pradesh  2009     26.5           64            820  Onion   \n",
       "3       Andhra Pradesh  2009     26.5           64            820  Onion   \n",
       "4       Andhra Pradesh  2010     27.8           69           1080  Onion   \n",
       "..                 ...   ...      ...          ...            ...    ...   \n",
       "273  Jammu and Kashmir  2017     16.2           72           1050  Onion   \n",
       "274  Jammu and Kashmir  2018     16.9           74           1202  Onion   \n",
       "275  Jammu and Kashmir  2018     16.9           74           1202  Onion   \n",
       "276  Jammu and Kashmir  2019     16.5           73           1137  Onion   \n",
       "277  Jammu and Kashmir  2019     16.5           73           1137  Onion   \n",
       "\n",
       "          Season     Area  Production  Annual_Rainfall  Fertilizer  Pesticide  \\\n",
       "0    Kharif       26723.0    439644.0            944.6  3822457.92    2405.07   \n",
       "1    Rabi         13737.0    289004.0            944.6  1964940.48    1236.33   \n",
       "2    Kharif       22954.0    374804.0            711.0  3576692.28    3902.18   \n",
       "3    Rabi         13608.0    333288.0            711.0  2120398.56    2313.36   \n",
       "4    Kharif       25612.0    499564.0           1297.0  4254409.32    6146.88   \n",
       "..           ...      ...         ...              ...         ...        ...   \n",
       "273  Rabi           552.0      1320.0           1279.1    86906.88     209.76   \n",
       "274  Kharif          17.0         9.0           1016.3     2757.40       5.95   \n",
       "275  Rabi           550.0      1311.0           1016.3    89210.00     192.50   \n",
       "276  Kharif          26.0        13.0           1324.1     4465.76       9.62   \n",
       "277  Rabi           561.0      1342.0           1324.1    96357.36     207.57   \n",
       "\n",
       "         Yield  \n",
       "0    15.308889  \n",
       "1    18.406316  \n",
       "2    15.856111  \n",
       "3    25.090526  \n",
       "4    17.786250  \n",
       "..         ...  \n",
       "273   2.400000  \n",
       "274   0.500000  \n",
       "275   1.992500  \n",
       "276   0.335000  \n",
       "277   2.426667  \n",
       "\n",
       "[278 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09108687",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid comparison between dtype=category and float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m newdf \u001b[38;5;241m=\u001b[39m df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYield\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arraylike.py:48\u001b[0m, in \u001b[0;36mOpsMixin.__lt__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__lt__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__lt__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py:5803\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   5800\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   5801\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 5803\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\ops\\array_ops.py:332\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLengths must match to compare\u001b[39m\u001b[38;5;124m\"\u001b[39m, lvalues\u001b[38;5;241m.\u001b[39mshape, rvalues\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    325\u001b[0m         )\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_extension_dispatch(lvalues, rvalues) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    328\u001b[0m     (\u001b[38;5;28misinstance\u001b[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001b[38;5;129;01mor\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m NaT)\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[0;32m    330\u001b[0m ):\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;66;03m# Call the method on lvalues\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_scalar(rvalues) \u001b[38;5;129;01mand\u001b[39;00m isna(rvalues):  \u001b[38;5;66;03m# TODO: but not pd.NA?\u001b[39;00m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;66;03m# numpy does not like comparisons vs None\u001b[39;00m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m operator\u001b[38;5;241m.\u001b[39mne:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\categorical.py:170\u001b[0m, in \u001b[0;36m_cat_compare_op.<locals>.func\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvalid_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;66;03m# allow categorical vs object dtype array comparisons for equality\u001b[39;00m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# these are only positional comparisons\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m opname \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__ne__\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\ops\\invalid.py:40\u001b[0m, in \u001b[0;36minvalid_comparison\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     typ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(right)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid comparison between dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleft\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtyp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid comparison between dtype=category and float"
     ]
    }
   ],
   "source": [
    "newdf = df[df['Yield'] < float(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf31297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
    "df = pd.read_csv('Onion.csv')\n",
    "# Select relevant features for training\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df[features]\n",
    "y = df['Yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19d0ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = df[df['Yield'] < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9627eb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.954"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df['Yield'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c20aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
    "df = newdf\n",
    "# Select relevant features for training\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df[features]\n",
    "y = df['Yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1636d616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_11788\\1404420473.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[features] = scaler.fit_transform(X[features])\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 11ms/step - loss: 21.8637 - val_loss: 8.3909\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3766 - val_loss: 4.2094\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9739 - val_loss: 3.3434\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.4191 - val_loss: 3.8204\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9036 - val_loss: 3.0351\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.9123 - val_loss: 2.5848\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.3659 - val_loss: 2.0404\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.3252 - val_loss: 2.4591\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.1702 - val_loss: 1.6133\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.8903 - val_loss: 1.5394\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.0230 - val_loss: 1.7908\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.9173 - val_loss: 1.6081\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.6540 - val_loss: 1.2716\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.7643 - val_loss: 1.5654\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.7584 - val_loss: 2.1585\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.5366 - val_loss: 1.2230\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.5523 - val_loss: 1.3599\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.5174 - val_loss: 1.2013\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.3778 - val_loss: 2.0148\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.7428 - val_loss: 1.5918\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.5451 - val_loss: 1.5985\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.4576 - val_loss: 0.9995\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.5951 - val_loss: 1.6662\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.5134 - val_loss: 1.6400\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.3863 - val_loss: 1.1876\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.1760 - val_loss: 1.3430\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.1258 - val_loss: 2.1614\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.1774 - val_loss: 1.5579\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.0666 - val_loss: 1.5635\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.2717 - val_loss: 2.3987\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9671 - val_loss: 1.7753\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.4282 - val_loss: 2.0375\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.3916 - val_loss: 1.4157\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.1308 - val_loss: 2.2747\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9826 - val_loss: 1.8567\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9405 - val_loss: 1.6890\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.8180 - val_loss: 2.1911\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.0221 - val_loss: 2.0124\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.8886 - val_loss: 1.6370\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9383 - val_loss: 1.7893\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.8471 - val_loss: 2.4607\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.8795 - val_loss: 2.1977\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.8198 - val_loss: 1.9758\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.7343 - val_loss: 1.8896\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.7346 - val_loss: 1.7772\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.6935 - val_loss: 2.1739\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.6169 - val_loss: 2.3426\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.6611 - val_loss: 1.9170\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.5381 - val_loss: 2.1904\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.7374 - val_loss: 2.3807\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.4557 - val_loss: 2.4345\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.4684 - val_loss: 1.9036\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.8168 - val_loss: 2.0191\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.4718 - val_loss: 2.0426\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.7265 - val_loss: 1.5649\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.8930 - val_loss: 1.9471\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.4692 - val_loss: 2.2575\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.5114 - val_loss: 3.0238\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.5822 - val_loss: 3.2583\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.4651 - val_loss: 1.8628\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.3496 - val_loss: 1.8745\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.2867 - val_loss: 2.0360\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.2475 - val_loss: 2.0113\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.2859 - val_loss: 2.5166\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.6153 - val_loss: 2.7331\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.5866 - val_loss: 2.2354\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.3416 - val_loss: 2.8309\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.5497 - val_loss: 2.9773\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.9191 - val_loss: 4.2999\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.5184 - val_loss: 1.9986\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.2978 - val_loss: 1.9990\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1716 - val_loss: 2.6110\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1608 - val_loss: 2.8564\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.2710 - val_loss: 3.0039\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.2423 - val_loss: 2.2978\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1465 - val_loss: 2.9240\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.2096 - val_loss: 2.6925\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.3116 - val_loss: 1.7358\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.3030 - val_loss: 1.9277\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.2326 - val_loss: 1.8991\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0842 - val_loss: 1.9059\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.3593 - val_loss: 2.8407\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 4ms/step - loss: 1.4372 - val_loss: 3.0023\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.2814 - val_loss: 3.7514\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.3613 - val_loss: 3.4452\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.2007 - val_loss: 1.6209\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0269 - val_loss: 2.0556\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.2586 - val_loss: 2.1885\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.2687 - val_loss: 1.5915\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1175 - val_loss: 2.2495\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1705 - val_loss: 2.1239\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.2338 - val_loss: 2.7302\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1337 - val_loss: 1.9538\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.3355 - val_loss: 2.4688\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1136 - val_loss: 2.7668\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0726 - val_loss: 3.0868\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0894 - val_loss: 2.4952\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.3782 - val_loss: 2.8500\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1029 - val_loss: 2.5607\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1210 - val_loss: 1.6837\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1136 - val_loss: 1.8939\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0609 - val_loss: 2.5189\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9704 - val_loss: 2.3824\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1642 - val_loss: 3.5299\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1769 - val_loss: 2.2244\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9210 - val_loss: 2.1790\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9244 - val_loss: 1.7351\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8341 - val_loss: 2.4704\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7960 - val_loss: 2.1420\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8073 - val_loss: 2.0086\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7695 - val_loss: 2.2777\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8528 - val_loss: 2.2242\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9178 - val_loss: 1.9373\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.3297 - val_loss: 1.8341\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9674 - val_loss: 2.2476\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9866 - val_loss: 2.1275\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.2107 - val_loss: 1.9806\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.2012 - val_loss: 2.7976\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1354 - val_loss: 1.5022\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.2235 - val_loss: 2.5469\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0238 - val_loss: 2.0350\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0365 - val_loss: 1.4456\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8317 - val_loss: 2.3263\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8798 - val_loss: 3.5420\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7698 - val_loss: 2.3254\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9623 - val_loss: 2.7781\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0591 - val_loss: 1.9900\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8898 - val_loss: 2.3243\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7367 - val_loss: 1.9409\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7576 - val_loss: 2.2820\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8312 - val_loss: 1.8981\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.0281 - val_loss: 2.3547\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1363 - val_loss: 2.6688\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.4442 - val_loss: 2.0701\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9016 - val_loss: 2.6392\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.9728 - val_loss: 2.9183\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9199 - val_loss: 2.3079\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7712 - val_loss: 2.4596\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7224 - val_loss: 2.5443\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6541 - val_loss: 1.7941\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7578 - val_loss: 2.8943\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7019 - val_loss: 1.8548\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7559 - val_loss: 2.5195\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6583 - val_loss: 2.4249\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7322 - val_loss: 2.1171\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8842 - val_loss: 2.8178\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7081 - val_loss: 2.0924\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6645 - val_loss: 2.7505\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6521 - val_loss: 2.0139\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6313 - val_loss: 3.5294\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6870 - val_loss: 1.9440\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7634 - val_loss: 3.5149\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8803 - val_loss: 2.5151\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7920 - val_loss: 1.7453\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8283 - val_loss: 2.4656\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6251 - val_loss: 2.0695\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5786 - val_loss: 1.9521\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5755 - val_loss: 2.0589\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6591 - val_loss: 2.1651\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5983 - val_loss: 2.3038\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5336 - val_loss: 2.4410\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5847 - val_loss: 2.8404\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6135 - val_loss: 2.3409\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8038 - val_loss: 3.1236\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7676 - val_loss: 2.0695\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5546 - val_loss: 2.4717\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6309 - val_loss: 2.5381\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7125 - val_loss: 2.0418\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7176 - val_loss: 2.4046\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5714 - val_loss: 2.3694\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5965 - val_loss: 1.8159\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6334 - val_loss: 2.5992\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6092 - val_loss: 1.8040\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6132 - val_loss: 2.2134\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6624 - val_loss: 2.2542\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5353 - val_loss: 2.3331\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6526 - val_loss: 1.9097\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5397 - val_loss: 2.5751\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5554 - val_loss: 2.4059\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5260 - val_loss: 2.9144\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5309 - val_loss: 2.2143\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5844 - val_loss: 2.5586\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5014 - val_loss: 2.3946\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5397 - val_loss: 2.5372\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5342 - val_loss: 2.8511\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5217 - val_loss: 2.6644\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5547 - val_loss: 2.5291\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5766 - val_loss: 3.4766\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6162 - val_loss: 1.9268\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5114 - val_loss: 3.0287\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7560 - val_loss: 2.0321\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5813 - val_loss: 4.2672\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0091 - val_loss: 2.3854\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7136 - val_loss: 2.2117\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5855 - val_loss: 2.3087\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5092 - val_loss: 2.4907\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5298 - val_loss: 2.1053\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5427 - val_loss: 2.5684\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5057 - val_loss: 2.7880\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5535 - val_loss: 2.7373\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Mean Squared Error on Test Data: 2.74\n",
      "Prediction: 6.40, Actual: 5.97\n",
      "Prediction: 0.17, Actual: 0.00\n",
      "Prediction: 6.95, Actual: 8.03\n",
      "Prediction: 2.78, Actual: 3.35\n",
      "Prediction: 9.21, Actual: 5.79\n",
      "Prediction: 6.85, Actual: 7.24\n",
      "Prediction: 2.86, Actual: 2.39\n",
      "Prediction: 8.13, Actual: 7.29\n",
      "Prediction: 4.94, Actual: 5.72\n",
      "Prediction: -0.03, Actual: 0.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Assuming 'df' is your DataFrame with the provided data\n",
    "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
    "\n",
    "# Select relevant features for training\n",
    "df = pd.read_csv('Onion.csv')\n",
    "newdf = df[df['Yield'].between(0, 10)]\n",
    "df = newdf\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df[features]\n",
    "y = df['Yield']\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X[features] = scaler.fit_transform(X[features])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.1, random_state=42)\n",
    "\n",
    "# Build and train a simple feedforward neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(200, activation='relu', input_dim=len(features)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='linear'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on Test Data: {mse:.2f}\")\n",
    "\n",
    "# Display some predictions and actual values\n",
    "for i in range(10):  # Displaying first 10 predictions for illustration\n",
    "    print(f\"Prediction: {y_pred[i][0]:.2f}, Actual: {y_test[i]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e865ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1  = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2f15d68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Onion.csv')\n",
    "newdf = df[df['Yield'].between(20, 30)]\n",
    "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
    "df = newdf\n",
    "# Select relevant features for training\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df[features]\n",
    "y = df['Yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "22022b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Year</th>\n",
       "      <th>AvgTemp</th>\n",
       "      <th>AvgHumidity</th>\n",
       "      <th>TotalRainfall</th>\n",
       "      <th>Crop</th>\n",
       "      <th>Season</th>\n",
       "      <th>Area</th>\n",
       "      <th>Production</th>\n",
       "      <th>Annual_Rainfall</th>\n",
       "      <th>Fertilizer</th>\n",
       "      <th>Pesticide</th>\n",
       "      <th>Yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>2009</td>\n",
       "      <td>26.5</td>\n",
       "      <td>64</td>\n",
       "      <td>820</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>13608.00</td>\n",
       "      <td>333288.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>2120398.56</td>\n",
       "      <td>2313.3600</td>\n",
       "      <td>25.090526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>2011</td>\n",
       "      <td>26.9</td>\n",
       "      <td>66</td>\n",
       "      <td>890</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>17378.00</td>\n",
       "      <td>444871.0</td>\n",
       "      <td>861.9</td>\n",
       "      <td>2911162.56</td>\n",
       "      <td>5734.7400</td>\n",
       "      <td>24.528947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>2012</td>\n",
       "      <td>27.5</td>\n",
       "      <td>68</td>\n",
       "      <td>980</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>14000.00</td>\n",
       "      <td>386000.0</td>\n",
       "      <td>968.7</td>\n",
       "      <td>2111200.00</td>\n",
       "      <td>4340.0000</td>\n",
       "      <td>25.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>2016</td>\n",
       "      <td>27.9</td>\n",
       "      <td>68</td>\n",
       "      <td>1050</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Kharif</td>\n",
       "      <td>32006.00</td>\n",
       "      <td>555944.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>4904919.50</td>\n",
       "      <td>11202.1000</td>\n",
       "      <td>21.054167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>2016</td>\n",
       "      <td>27.9</td>\n",
       "      <td>68</td>\n",
       "      <td>1050</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>3511.00</td>\n",
       "      <td>77209.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>538060.75</td>\n",
       "      <td>1228.8500</td>\n",
       "      <td>22.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>2019</td>\n",
       "      <td>27.1</td>\n",
       "      <td>66</td>\n",
       "      <td>900</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Kharif</td>\n",
       "      <td>23864.00</td>\n",
       "      <td>524198.0</td>\n",
       "      <td>899.2</td>\n",
       "      <td>4098880.64</td>\n",
       "      <td>8829.6800</td>\n",
       "      <td>22.224444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>2019</td>\n",
       "      <td>27.1</td>\n",
       "      <td>66</td>\n",
       "      <td>900</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>12250.00</td>\n",
       "      <td>256890.0</td>\n",
       "      <td>899.2</td>\n",
       "      <td>2104060.00</td>\n",
       "      <td>4532.5000</td>\n",
       "      <td>21.767500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2008</td>\n",
       "      <td>27.6</td>\n",
       "      <td>62</td>\n",
       "      <td>721</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>54400.00</td>\n",
       "      <td>1502800.0</td>\n",
       "      <td>746.1</td>\n",
       "      <td>7781376.00</td>\n",
       "      <td>4896.0000</td>\n",
       "      <td>26.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2009</td>\n",
       "      <td>27.2</td>\n",
       "      <td>61</td>\n",
       "      <td>654</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>38500.00</td>\n",
       "      <td>1051400.0</td>\n",
       "      <td>618.0</td>\n",
       "      <td>5999070.00</td>\n",
       "      <td>6545.0000</td>\n",
       "      <td>26.692778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2010</td>\n",
       "      <td>28.0</td>\n",
       "      <td>63</td>\n",
       "      <td>812</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Whole Year</td>\n",
       "      <td>67100.00</td>\n",
       "      <td>1847900.0</td>\n",
       "      <td>1107.5</td>\n",
       "      <td>11145981.00</td>\n",
       "      <td>16104.0000</td>\n",
       "      <td>26.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2011</td>\n",
       "      <td>27.5</td>\n",
       "      <td>62</td>\n",
       "      <td>749</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>63600.00</td>\n",
       "      <td>1860500.0</td>\n",
       "      <td>890.5</td>\n",
       "      <td>10654272.00</td>\n",
       "      <td>20988.0000</td>\n",
       "      <td>24.669565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2011</td>\n",
       "      <td>27.5</td>\n",
       "      <td>62</td>\n",
       "      <td>749</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Summer</td>\n",
       "      <td>9200.00</td>\n",
       "      <td>258600.0</td>\n",
       "      <td>890.5</td>\n",
       "      <td>1541184.00</td>\n",
       "      <td>3036.0000</td>\n",
       "      <td>26.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2012</td>\n",
       "      <td>28.1</td>\n",
       "      <td>64</td>\n",
       "      <td>876</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>14500.00</td>\n",
       "      <td>354200.0</td>\n",
       "      <td>460.6</td>\n",
       "      <td>2186600.00</td>\n",
       "      <td>4495.0000</td>\n",
       "      <td>23.321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2012</td>\n",
       "      <td>28.1</td>\n",
       "      <td>64</td>\n",
       "      <td>876</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2200.00</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>460.6</td>\n",
       "      <td>331760.00</td>\n",
       "      <td>682.0000</td>\n",
       "      <td>22.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2013</td>\n",
       "      <td>27.4</td>\n",
       "      <td>61</td>\n",
       "      <td>701</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Summer</td>\n",
       "      <td>11011.00</td>\n",
       "      <td>312805.0</td>\n",
       "      <td>1006.5</td>\n",
       "      <td>1590979.39</td>\n",
       "      <td>2972.9700</td>\n",
       "      <td>27.961250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2014</td>\n",
       "      <td>28.3</td>\n",
       "      <td>65</td>\n",
       "      <td>945</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>38809.00</td>\n",
       "      <td>1081939.0</td>\n",
       "      <td>605.6</td>\n",
       "      <td>5858606.64</td>\n",
       "      <td>12806.9700</td>\n",
       "      <td>28.187241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2014</td>\n",
       "      <td>28.3</td>\n",
       "      <td>65</td>\n",
       "      <td>945</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Summer</td>\n",
       "      <td>8185.00</td>\n",
       "      <td>233276.0</td>\n",
       "      <td>605.6</td>\n",
       "      <td>1235607.60</td>\n",
       "      <td>2701.0500</td>\n",
       "      <td>26.506000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2015</td>\n",
       "      <td>27.7</td>\n",
       "      <td>63</td>\n",
       "      <td>802</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>52885.00</td>\n",
       "      <td>1424267.0</td>\n",
       "      <td>584.3</td>\n",
       "      <td>8351070.35</td>\n",
       "      <td>17452.0500</td>\n",
       "      <td>26.941538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2015</td>\n",
       "      <td>27.7</td>\n",
       "      <td>63</td>\n",
       "      <td>802</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Summer</td>\n",
       "      <td>10091.00</td>\n",
       "      <td>286289.0</td>\n",
       "      <td>584.3</td>\n",
       "      <td>1593469.81</td>\n",
       "      <td>3330.0300</td>\n",
       "      <td>26.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2016</td>\n",
       "      <td>28.5</td>\n",
       "      <td>64</td>\n",
       "      <td>928</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>42408.00</td>\n",
       "      <td>1104672.0</td>\n",
       "      <td>710.5</td>\n",
       "      <td>6499026.00</td>\n",
       "      <td>14842.8000</td>\n",
       "      <td>26.387037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2016</td>\n",
       "      <td>28.5</td>\n",
       "      <td>64</td>\n",
       "      <td>928</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Summer</td>\n",
       "      <td>13489.00</td>\n",
       "      <td>400128.0</td>\n",
       "      <td>710.5</td>\n",
       "      <td>2067189.25</td>\n",
       "      <td>4721.1500</td>\n",
       "      <td>26.555333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2017</td>\n",
       "      <td>27.8</td>\n",
       "      <td>62</td>\n",
       "      <td>765</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>42668.00</td>\n",
       "      <td>1223608.0</td>\n",
       "      <td>814.8</td>\n",
       "      <td>6717649.92</td>\n",
       "      <td>16213.8400</td>\n",
       "      <td>28.763793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2017</td>\n",
       "      <td>27.8</td>\n",
       "      <td>62</td>\n",
       "      <td>765</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Summer</td>\n",
       "      <td>11801.00</td>\n",
       "      <td>318222.0</td>\n",
       "      <td>814.8</td>\n",
       "      <td>1857949.44</td>\n",
       "      <td>4484.3800</td>\n",
       "      <td>25.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2018</td>\n",
       "      <td>28.7</td>\n",
       "      <td>66</td>\n",
       "      <td>1014</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>23847.00</td>\n",
       "      <td>673001.0</td>\n",
       "      <td>1125.4</td>\n",
       "      <td>3867983.40</td>\n",
       "      <td>8346.4500</td>\n",
       "      <td>27.705200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2018</td>\n",
       "      <td>28.7</td>\n",
       "      <td>66</td>\n",
       "      <td>1014</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Summer</td>\n",
       "      <td>1854.00</td>\n",
       "      <td>51062.0</td>\n",
       "      <td>1125.4</td>\n",
       "      <td>300718.80</td>\n",
       "      <td>648.9000</td>\n",
       "      <td>27.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2019</td>\n",
       "      <td>28.0</td>\n",
       "      <td>64</td>\n",
       "      <td>856</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>41408.00</td>\n",
       "      <td>1211977.0</td>\n",
       "      <td>1067.8</td>\n",
       "      <td>7112238.08</td>\n",
       "      <td>15320.9600</td>\n",
       "      <td>28.892500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2019</td>\n",
       "      <td>28.0</td>\n",
       "      <td>64</td>\n",
       "      <td>856</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Summer</td>\n",
       "      <td>7176.00</td>\n",
       "      <td>204332.0</td>\n",
       "      <td>1067.8</td>\n",
       "      <td>1232549.76</td>\n",
       "      <td>2655.1200</td>\n",
       "      <td>28.464000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Telangana</td>\n",
       "      <td>2014</td>\n",
       "      <td>28.4</td>\n",
       "      <td>66</td>\n",
       "      <td>1029</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>8367.00</td>\n",
       "      <td>228542.0</td>\n",
       "      <td>746.4</td>\n",
       "      <td>1263082.32</td>\n",
       "      <td>2761.1100</td>\n",
       "      <td>26.845556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Telangana</td>\n",
       "      <td>2015</td>\n",
       "      <td>27.8</td>\n",
       "      <td>64</td>\n",
       "      <td>876</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>10462.00</td>\n",
       "      <td>315376.0</td>\n",
       "      <td>747.9</td>\n",
       "      <td>1652054.42</td>\n",
       "      <td>3452.4600</td>\n",
       "      <td>28.958889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Telangana</td>\n",
       "      <td>2018</td>\n",
       "      <td>28.7</td>\n",
       "      <td>66</td>\n",
       "      <td>1052</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>7422.00</td>\n",
       "      <td>216343.0</td>\n",
       "      <td>1350.3</td>\n",
       "      <td>1203848.40</td>\n",
       "      <td>2597.7000</td>\n",
       "      <td>28.155185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Telangana</td>\n",
       "      <td>2019</td>\n",
       "      <td>28.2</td>\n",
       "      <td>65</td>\n",
       "      <td>947</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Kharif</td>\n",
       "      <td>2383.00</td>\n",
       "      <td>53176.0</td>\n",
       "      <td>1031.7</td>\n",
       "      <td>409304.08</td>\n",
       "      <td>881.7100</td>\n",
       "      <td>21.954783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Haryana</td>\n",
       "      <td>2012</td>\n",
       "      <td>21.3</td>\n",
       "      <td>65</td>\n",
       "      <td>610</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Whole Year</td>\n",
       "      <td>2330.00</td>\n",
       "      <td>65900.0</td>\n",
       "      <td>307.9</td>\n",
       "      <td>351364.00</td>\n",
       "      <td>722.3000</td>\n",
       "      <td>25.969091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Haryana</td>\n",
       "      <td>2013</td>\n",
       "      <td>20.5</td>\n",
       "      <td>63</td>\n",
       "      <td>678</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Whole Year</td>\n",
       "      <td>30163.00</td>\n",
       "      <td>672164.0</td>\n",
       "      <td>452.2</td>\n",
       "      <td>4358251.87</td>\n",
       "      <td>8144.0100</td>\n",
       "      <td>22.457143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Haryana</td>\n",
       "      <td>2014</td>\n",
       "      <td>21.9</td>\n",
       "      <td>67</td>\n",
       "      <td>789</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Whole Year</td>\n",
       "      <td>28688.00</td>\n",
       "      <td>640215.0</td>\n",
       "      <td>301.3</td>\n",
       "      <td>4330740.48</td>\n",
       "      <td>9467.0400</td>\n",
       "      <td>22.560476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Haryana</td>\n",
       "      <td>2015</td>\n",
       "      <td>21.4</td>\n",
       "      <td>64</td>\n",
       "      <td>632</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Whole Year</td>\n",
       "      <td>30645.00</td>\n",
       "      <td>705795.0</td>\n",
       "      <td>426.8</td>\n",
       "      <td>4839151.95</td>\n",
       "      <td>10112.8500</td>\n",
       "      <td>23.163333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Haryana</td>\n",
       "      <td>2016</td>\n",
       "      <td>22.0</td>\n",
       "      <td>68</td>\n",
       "      <td>815</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Whole Year</td>\n",
       "      <td>28164.00</td>\n",
       "      <td>650394.0</td>\n",
       "      <td>554.7</td>\n",
       "      <td>4316133.00</td>\n",
       "      <td>9857.4000</td>\n",
       "      <td>24.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Haryana</td>\n",
       "      <td>2017</td>\n",
       "      <td>21.7</td>\n",
       "      <td>65</td>\n",
       "      <td>654</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Whole Year</td>\n",
       "      <td>29931.00</td>\n",
       "      <td>701504.0</td>\n",
       "      <td>417.1</td>\n",
       "      <td>4712336.64</td>\n",
       "      <td>11373.7800</td>\n",
       "      <td>23.976667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Haryana</td>\n",
       "      <td>2018</td>\n",
       "      <td>22.2</td>\n",
       "      <td>69</td>\n",
       "      <td>870</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Whole Year</td>\n",
       "      <td>32010.00</td>\n",
       "      <td>780150.0</td>\n",
       "      <td>533.2</td>\n",
       "      <td>5192022.00</td>\n",
       "      <td>11203.5000</td>\n",
       "      <td>24.738636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Haryana</td>\n",
       "      <td>2019</td>\n",
       "      <td>21.8</td>\n",
       "      <td>66</td>\n",
       "      <td>725</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Whole Year</td>\n",
       "      <td>23749.00</td>\n",
       "      <td>610443.0</td>\n",
       "      <td>351.8</td>\n",
       "      <td>4079128.24</td>\n",
       "      <td>8787.1300</td>\n",
       "      <td>24.541818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>2015</td>\n",
       "      <td>26.5</td>\n",
       "      <td>69</td>\n",
       "      <td>1086</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Whole Year</td>\n",
       "      <td>147866.95</td>\n",
       "      <td>3413490.0</td>\n",
       "      <td>1000.7</td>\n",
       "      <td>23349670.07</td>\n",
       "      <td>48796.0935</td>\n",
       "      <td>22.430196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>2016</td>\n",
       "      <td>27.2</td>\n",
       "      <td>70</td>\n",
       "      <td>1154</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Whole Year</td>\n",
       "      <td>150839.21</td>\n",
       "      <td>3821047.0</td>\n",
       "      <td>1048.4</td>\n",
       "      <td>23116108.93</td>\n",
       "      <td>52793.7235</td>\n",
       "      <td>23.961373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>2017</td>\n",
       "      <td>26.7</td>\n",
       "      <td>69</td>\n",
       "      <td>1020</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Whole Year</td>\n",
       "      <td>150865.60</td>\n",
       "      <td>3701014.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>23752280.06</td>\n",
       "      <td>57328.9280</td>\n",
       "      <td>23.005686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>2018</td>\n",
       "      <td>27.4</td>\n",
       "      <td>71</td>\n",
       "      <td>1242</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Whole Year</td>\n",
       "      <td>149843.45</td>\n",
       "      <td>3683098.0</td>\n",
       "      <td>1102.2</td>\n",
       "      <td>24304607.59</td>\n",
       "      <td>52445.2075</td>\n",
       "      <td>23.083922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>2019</td>\n",
       "      <td>26.9</td>\n",
       "      <td>70</td>\n",
       "      <td>1107</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Whole Year</td>\n",
       "      <td>173886.50</td>\n",
       "      <td>4270702.0</td>\n",
       "      <td>1446.7</td>\n",
       "      <td>29866745.24</td>\n",
       "      <td>64338.0050</td>\n",
       "      <td>23.217451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>2011</td>\n",
       "      <td>24.5</td>\n",
       "      <td>72</td>\n",
       "      <td>789</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>11464.00</td>\n",
       "      <td>262239.0</td>\n",
       "      <td>777.4</td>\n",
       "      <td>1920449.28</td>\n",
       "      <td>3783.1200</td>\n",
       "      <td>22.875694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>2012</td>\n",
       "      <td>25.2</td>\n",
       "      <td>74</td>\n",
       "      <td>945</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>10934.00</td>\n",
       "      <td>251513.0</td>\n",
       "      <td>746.3</td>\n",
       "      <td>1648847.20</td>\n",
       "      <td>3389.5400</td>\n",
       "      <td>23.000811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>2013</td>\n",
       "      <td>24.7</td>\n",
       "      <td>71</td>\n",
       "      <td>807</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>12232.00</td>\n",
       "      <td>248779.0</td>\n",
       "      <td>995.2</td>\n",
       "      <td>1767401.68</td>\n",
       "      <td>3302.6400</td>\n",
       "      <td>20.336267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>2013</td>\n",
       "      <td>27.7</td>\n",
       "      <td>74</td>\n",
       "      <td>1342</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>3640.00</td>\n",
       "      <td>24604.0</td>\n",
       "      <td>1253.6</td>\n",
       "      <td>525943.60</td>\n",
       "      <td>982.8000</td>\n",
       "      <td>21.769565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>2013</td>\n",
       "      <td>26.4</td>\n",
       "      <td>71</td>\n",
       "      <td>1342</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Rabi</td>\n",
       "      <td>87.00</td>\n",
       "      <td>1769.0</td>\n",
       "      <td>1418.3</td>\n",
       "      <td>12570.63</td>\n",
       "      <td>23.4900</td>\n",
       "      <td>20.330000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              State  Year  AvgTemp  AvgHumidity  TotalRainfall   Crop  \\\n",
       "3    Andhra Pradesh  2009     26.5           64            820  Onion   \n",
       "7    Andhra Pradesh  2011     26.9           66            890  Onion   \n",
       "9    Andhra Pradesh  2012     27.5           68            980  Onion   \n",
       "16   Andhra Pradesh  2016     27.9           68           1050  Onion   \n",
       "17   Andhra Pradesh  2016     27.9           68           1050  Onion   \n",
       "22   Andhra Pradesh  2019     27.1           66            900  Onion   \n",
       "23   Andhra Pradesh  2019     27.1           66            900  Onion   \n",
       "48          Gujarat  2008     27.6           62            721  Onion   \n",
       "49          Gujarat  2009     27.2           61            654  Onion   \n",
       "50          Gujarat  2010     28.0           63            812  Onion   \n",
       "51          Gujarat  2011     27.5           62            749  Onion   \n",
       "52          Gujarat  2011     27.5           62            749  Onion   \n",
       "53          Gujarat  2012     28.1           64            876  Onion   \n",
       "54          Gujarat  2012     28.1           64            876  Onion   \n",
       "56          Gujarat  2013     27.4           61            701  Onion   \n",
       "57          Gujarat  2014     28.3           65            945  Onion   \n",
       "58          Gujarat  2014     28.3           65            945  Onion   \n",
       "59          Gujarat  2015     27.7           63            802  Onion   \n",
       "60          Gujarat  2015     27.7           63            802  Onion   \n",
       "61          Gujarat  2016     28.5           64            928  Onion   \n",
       "62          Gujarat  2016     28.5           64            928  Onion   \n",
       "63          Gujarat  2017     27.8           62            765  Onion   \n",
       "64          Gujarat  2017     27.8           62            765  Onion   \n",
       "65          Gujarat  2018     28.7           66           1014  Onion   \n",
       "66          Gujarat  2018     28.7           66           1014  Onion   \n",
       "67          Gujarat  2019     28.0           64            856  Onion   \n",
       "68          Gujarat  2019     28.0           64            856  Onion   \n",
       "72        Telangana  2014     28.4           66           1029  Onion   \n",
       "74        Telangana  2015     27.8           64            876  Onion   \n",
       "80        Telangana  2018     28.7           66           1052  Onion   \n",
       "81        Telangana  2019     28.2           65            947  Onion   \n",
       "87          Haryana  2012     21.3           65            610  Onion   \n",
       "88          Haryana  2013     20.5           63            678  Onion   \n",
       "89          Haryana  2014     21.9           67            789  Onion   \n",
       "90          Haryana  2015     21.4           64            632  Onion   \n",
       "91          Haryana  2016     22.0           68            815  Onion   \n",
       "92          Haryana  2017     21.7           65            654  Onion   \n",
       "93          Haryana  2018     22.2           69            870  Onion   \n",
       "94          Haryana  2019     21.8           66            725  Onion   \n",
       "101  Madhya Pradesh  2015     26.5           69           1086  Onion   \n",
       "102  Madhya Pradesh  2016     27.2           70           1154  Onion   \n",
       "103  Madhya Pradesh  2017     26.7           69           1020  Onion   \n",
       "104  Madhya Pradesh  2018     27.4           71           1242  Onion   \n",
       "105  Madhya Pradesh  2019     26.9           70           1107  Onion   \n",
       "110   Uttar Pradesh  2011     24.5           72            789  Onion   \n",
       "112   Uttar Pradesh  2012     25.2           74            945  Onion   \n",
       "114   Uttar Pradesh  2013     24.7           71            807  Onion   \n",
       "175       Jharkhand  2013     27.7           74           1342  Onion   \n",
       "248    Chhattisgarh  2013     26.4           71           1342  Onion   \n",
       "\n",
       "          Season       Area  Production  Annual_Rainfall   Fertilizer  \\\n",
       "3    Rabi          13608.00    333288.0            711.0   2120398.56   \n",
       "7    Rabi          17378.00    444871.0            861.9   2911162.56   \n",
       "9    Rabi          14000.00    386000.0            968.7   2111200.00   \n",
       "16   Kharif        32006.00    555944.0            890.0   4904919.50   \n",
       "17   Rabi           3511.00     77209.0            890.0    538060.75   \n",
       "22   Kharif        23864.00    524198.0            899.2   4098880.64   \n",
       "23   Rabi          12250.00    256890.0            899.2   2104060.00   \n",
       "48   Rabi          54400.00   1502800.0            746.1   7781376.00   \n",
       "49   Rabi          38500.00   1051400.0            618.0   5999070.00   \n",
       "50   Whole Year    67100.00   1847900.0           1107.5  11145981.00   \n",
       "51   Rabi          63600.00   1860500.0            890.5  10654272.00   \n",
       "52   Summer         9200.00    258600.0            890.5   1541184.00   \n",
       "53   Rabi          14500.00    354200.0            460.6   2186600.00   \n",
       "54   Summer         2200.00     55000.0            460.6    331760.00   \n",
       "56   Summer        11011.00    312805.0           1006.5   1590979.39   \n",
       "57   Rabi          38809.00   1081939.0            605.6   5858606.64   \n",
       "58   Summer         8185.00    233276.0            605.6   1235607.60   \n",
       "59   Rabi          52885.00   1424267.0            584.3   8351070.35   \n",
       "60   Summer        10091.00    286289.0            584.3   1593469.81   \n",
       "61   Rabi          42408.00   1104672.0            710.5   6499026.00   \n",
       "62   Summer        13489.00    400128.0            710.5   2067189.25   \n",
       "63   Rabi          42668.00   1223608.0            814.8   6717649.92   \n",
       "64   Summer        11801.00    318222.0            814.8   1857949.44   \n",
       "65   Rabi          23847.00    673001.0           1125.4   3867983.40   \n",
       "66   Summer         1854.00     51062.0           1125.4    300718.80   \n",
       "67   Rabi          41408.00   1211977.0           1067.8   7112238.08   \n",
       "68   Summer         7176.00    204332.0           1067.8   1232549.76   \n",
       "72   Rabi           8367.00    228542.0            746.4   1263082.32   \n",
       "74   Rabi          10462.00    315376.0            747.9   1652054.42   \n",
       "80   Rabi           7422.00    216343.0           1350.3   1203848.40   \n",
       "81   Kharif         2383.00     53176.0           1031.7    409304.08   \n",
       "87   Whole Year     2330.00     65900.0            307.9    351364.00   \n",
       "88   Whole Year    30163.00    672164.0            452.2   4358251.87   \n",
       "89   Whole Year    28688.00    640215.0            301.3   4330740.48   \n",
       "90   Whole Year    30645.00    705795.0            426.8   4839151.95   \n",
       "91   Whole Year    28164.00    650394.0            554.7   4316133.00   \n",
       "92   Whole Year    29931.00    701504.0            417.1   4712336.64   \n",
       "93   Whole Year    32010.00    780150.0            533.2   5192022.00   \n",
       "94   Whole Year    23749.00    610443.0            351.8   4079128.24   \n",
       "101  Whole Year   147866.95   3413490.0           1000.7  23349670.07   \n",
       "102  Whole Year   150839.21   3821047.0           1048.4  23116108.93   \n",
       "103  Whole Year   150865.60   3701014.0            800.0  23752280.06   \n",
       "104  Whole Year   149843.45   3683098.0           1102.2  24304607.59   \n",
       "105  Whole Year   173886.50   4270702.0           1446.7  29866745.24   \n",
       "110  Rabi          11464.00    262239.0            777.4   1920449.28   \n",
       "112  Rabi          10934.00    251513.0            746.3   1648847.20   \n",
       "114  Rabi          12232.00    248779.0            995.2   1767401.68   \n",
       "175  Rabi           3640.00     24604.0           1253.6    525943.60   \n",
       "248  Rabi             87.00      1769.0           1418.3     12570.63   \n",
       "\n",
       "      Pesticide      Yield  \n",
       "3     2313.3600  25.090526  \n",
       "7     5734.7400  24.528947  \n",
       "9     4340.0000  25.343750  \n",
       "16   11202.1000  21.054167  \n",
       "17    1228.8500  22.110000  \n",
       "22    8829.6800  22.224444  \n",
       "23    4532.5000  21.767500  \n",
       "48    4896.0000  26.605000  \n",
       "49    6545.0000  26.692778  \n",
       "50   16104.0000  26.036500  \n",
       "51   20988.0000  24.669565  \n",
       "52    3036.0000  26.965000  \n",
       "53    4495.0000  23.321000  \n",
       "54     682.0000  22.094000  \n",
       "56    2972.9700  27.961250  \n",
       "57   12806.9700  28.187241  \n",
       "58    2701.0500  26.506000  \n",
       "59   17452.0500  26.941538  \n",
       "60    3330.0300  26.179000  \n",
       "61   14842.8000  26.387037  \n",
       "62    4721.1500  26.555333  \n",
       "63   16213.8400  28.763793  \n",
       "64    4484.3800  25.194000  \n",
       "65    8346.4500  27.705200  \n",
       "66     648.9000  27.540000  \n",
       "67   15320.9600  28.892500  \n",
       "68    2655.1200  28.464000  \n",
       "72    2761.1100  26.845556  \n",
       "74    3452.4600  28.958889  \n",
       "80    2597.7000  28.155185  \n",
       "81     881.7100  21.954783  \n",
       "87     722.3000  25.969091  \n",
       "88    8144.0100  22.457143  \n",
       "89    9467.0400  22.560476  \n",
       "90   10112.8500  23.163333  \n",
       "91    9857.4000  24.830000  \n",
       "92   11373.7800  23.976667  \n",
       "93   11203.5000  24.738636  \n",
       "94    8787.1300  24.541818  \n",
       "101  48796.0935  22.430196  \n",
       "102  52793.7235  23.961373  \n",
       "103  57328.9280  23.005686  \n",
       "104  52445.2075  23.083922  \n",
       "105  64338.0050  23.217451  \n",
       "110   3783.1200  22.875694  \n",
       "112   3389.5400  23.000811  \n",
       "114   3302.6400  20.336267  \n",
       "175    982.8000  21.769565  \n",
       "248     23.4900  20.330000  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "10307f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_11788\\3853190578.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[features] = scaler.fit_transform(X[features])\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 29ms/step - loss: 619.3412 - val_loss: 514.0605\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 573.3376 - val_loss: 443.0624\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 475.4892 - val_loss: 284.9120\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 282.6022 - val_loss: 75.0925\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 123.8241 - val_loss: 134.3879\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 87.7154 - val_loss: 77.8430\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 37.6706 - val_loss: 46.8770\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 36.5554 - val_loss: 50.0553\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 28.3638 - val_loss: 66.2417\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 30.2363 - val_loss: 63.3707\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 22.6707 - val_loss: 41.6741\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 17.8625 - val_loss: 36.8436\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 15.5225 - val_loss: 48.2219\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.2619 - val_loss: 50.7378\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.3434 - val_loss: 35.5151\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 13.5132 - val_loss: 43.9512\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 12.0873 - val_loss: 44.3256\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.9684 - val_loss: 49.3635\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 10.7541 - val_loss: 36.6895\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 10.3284 - val_loss: 38.4200\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 10.0277 - val_loss: 41.8050\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.4075 - val_loss: 39.5344\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.0337 - val_loss: 46.3279\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 9.1040 - val_loss: 43.4165\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8.0774 - val_loss: 36.9764\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.9788 - val_loss: 40.5128\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.5653 - val_loss: 42.3401\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.3540 - val_loss: 42.4119\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.0252 - val_loss: 40.0104\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.8573 - val_loss: 34.7656\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.0032 - val_loss: 40.1891\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6.2872 - val_loss: 34.2029\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6.2250 - val_loss: 38.7633\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6.0802 - val_loss: 41.2569\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.0836 - val_loss: 35.1899\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5.5414 - val_loss: 44.0502\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 5.6854 - val_loss: 39.4139\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 5.3683 - val_loss: 32.3031\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 5.7459 - val_loss: 34.8884\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 5.1235 - val_loss: 36.1525\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 5.0368 - val_loss: 29.7141\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4.7312 - val_loss: 38.3179\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4.9424 - val_loss: 31.9794\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.7922 - val_loss: 29.0071\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4.4226 - val_loss: 35.1267\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.5708 - val_loss: 30.2250\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.5536 - val_loss: 30.0705\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.7051 - val_loss: 25.5653\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 5.3956 - val_loss: 31.6979\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3.7259 - val_loss: 19.7693\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4.4369 - val_loss: 32.5035\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4.2619 - val_loss: 22.0628\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3.9962 - val_loss: 26.1737\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4.0676 - val_loss: 31.2558\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3.8799 - val_loss: 21.0198\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3.5420 - val_loss: 28.0596\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.8065 - val_loss: 16.7834\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3.3275 - val_loss: 22.9916\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.1040 - val_loss: 19.5440\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.1734 - val_loss: 21.9241\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.9460 - val_loss: 16.4677\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.0121 - val_loss: 17.6044\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7148 - val_loss: 17.4873\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0809 - val_loss: 19.8469\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3.3704 - val_loss: 22.7864\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3.7309 - val_loss: 10.9412\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7902 - val_loss: 18.3600\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6903 - val_loss: 10.0184\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.8318 - val_loss: 13.0995\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3.0044 - val_loss: 13.8497\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.5013 - val_loss: 11.1104\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.6625 - val_loss: 14.9053\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.2668 - val_loss: 10.1004\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.3904 - val_loss: 15.7317\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.3226 - val_loss: 9.8415\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.1842 - val_loss: 11.2352\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.0654 - val_loss: 12.7518\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.0841 - val_loss: 8.6446\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.5287 - val_loss: 10.0176\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4693 - val_loss: 9.8496\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2.2955 - val_loss: 6.7891\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.3050 - val_loss: 11.4080\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 8ms/step - loss: 1.6666 - val_loss: 6.3153\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.3910 - val_loss: 8.8244\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.8744 - val_loss: 7.3746\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.7578 - val_loss: 8.3510\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.7495 - val_loss: 7.8635\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.7743 - val_loss: 8.1660\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8880 - val_loss: 6.2165\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6511 - val_loss: 5.3074\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6971 - val_loss: 5.8765\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.6780 - val_loss: 4.0337\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6108 - val_loss: 5.2595\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7144 - val_loss: 4.8969\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.7147 - val_loss: 5.8072\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.5196 - val_loss: 4.6573\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6424 - val_loss: 5.2350\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.7074 - val_loss: 4.1731\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4609 - val_loss: 4.6631\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.5786 - val_loss: 3.6726\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.0135 - val_loss: 4.0746\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9457 - val_loss: 3.4408\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.7168 - val_loss: 3.9235\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.4183 - val_loss: 4.6236\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4053 - val_loss: 3.7758\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.4335 - val_loss: 3.9473\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.3517 - val_loss: 3.7470\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5350 - val_loss: 3.8461\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.5771 - val_loss: 3.4363\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.2702 - val_loss: 3.4511\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.4957 - val_loss: 2.7995\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.9350 - val_loss: 3.2300\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.5262 - val_loss: 2.9646\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.4088 - val_loss: 2.9617\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.2119 - val_loss: 3.8766\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2810 - val_loss: 3.1756\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4500 - val_loss: 3.3177\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4493 - val_loss: 2.7442\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4528 - val_loss: 4.1218\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4904 - val_loss: 3.0842\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.4216 - val_loss: 3.5147\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.2059 - val_loss: 3.2955\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1423 - val_loss: 2.5933\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.3122 - val_loss: 2.9353\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.1983 - val_loss: 3.2043\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2079 - val_loss: 3.1880\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.0831 - val_loss: 3.1598\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2718 - val_loss: 3.3035\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0631 - val_loss: 3.0075\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.0403 - val_loss: 2.7629\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.0351 - val_loss: 2.9875\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.0928 - val_loss: 3.3752\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.4844 - val_loss: 3.1142\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.6261 - val_loss: 3.3781\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4928 - val_loss: 3.1772\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2536 - val_loss: 2.9799\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.0867 - val_loss: 2.4020\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.4515 - val_loss: 3.2277\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1473 - val_loss: 2.6439\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.2306 - val_loss: 2.8625\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9428 - val_loss: 3.4039\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.1321 - val_loss: 3.0161\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7898 - val_loss: 2.6218\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1207 - val_loss: 2.8481\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.9001 - val_loss: 2.9530\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.9162 - val_loss: 2.8408\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.7783 - val_loss: 4.0693\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.3561 - val_loss: 3.9209\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.8382 - val_loss: 4.5083\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.6341 - val_loss: 3.1203\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.2825 - val_loss: 2.8098\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.0572 - val_loss: 2.9165\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.8402 - val_loss: 3.2197\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.8540 - val_loss: 3.2053\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8572 - val_loss: 2.6830\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.8414 - val_loss: 3.2060\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.0435 - val_loss: 2.8715\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0362 - val_loss: 3.4354\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2878 - val_loss: 3.7865\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.3614 - val_loss: 2.9127\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.8839 - val_loss: 4.2885\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8008 - val_loss: 3.7967\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9719 - val_loss: 2.4916\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.4164 - val_loss: 3.1430\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.0590 - val_loss: 3.2409\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0020 - val_loss: 2.8753\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.9191 - val_loss: 2.4695\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.8161 - val_loss: 2.9794\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.8720 - val_loss: 2.8472\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.9807 - val_loss: 2.8425\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0841 - val_loss: 3.5289\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.7812 - val_loss: 3.6061\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.7789 - val_loss: 2.6386\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7428 - val_loss: 2.7210\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.8754 - val_loss: 3.3584\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.7941 - val_loss: 2.9305\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.1538 - val_loss: 3.7461\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.1921 - val_loss: 4.4918\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.8786 - val_loss: 2.7807\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.7015 - val_loss: 3.4702\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.7811 - val_loss: 3.6713\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.7330 - val_loss: 2.9786\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.8612 - val_loss: 3.2448\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.7775 - val_loss: 3.0254\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.5943 - val_loss: 3.1414\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.5718 - val_loss: 3.1120\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.6178 - val_loss: 3.2405\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.7028 - val_loss: 3.1976\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6036 - val_loss: 3.8264\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5983 - val_loss: 3.7739\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5500 - val_loss: 3.8520\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5507 - val_loss: 3.3049\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6787 - val_loss: 4.7910\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.8763 - val_loss: 3.5452\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6004 - val_loss: 3.6045\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.6966 - val_loss: 2.4867\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.6851 - val_loss: 2.5976\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.6671 - val_loss: 3.7600\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.6054 - val_loss: 3.2153\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5027 - val_loss: 3.6302\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "Mean Squared Error on Test Data: 3.63\n",
      "Prediction: 24.17, Actual: 22.09\n",
      "Prediction: 21.08, Actual: 23.00\n",
      "Prediction: 22.32, Actual: 21.77\n",
      "Prediction: 20.45, Actual: 22.88\n",
      "Prediction: 24.95, Actual: 26.94\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [90]\u001b[0m, in \u001b[0;36m<cell line: 40>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Display some predictions and actual values\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# Displaying first 10 predictions for illustration\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_pred[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Actual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_test[i]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 5 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Assuming 'df' is your DataFrame with the provided data\n",
    "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
    "\n",
    "# Select relevant features for training\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X[features] = scaler.fit_transform(X[features])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.1, random_state=42)\n",
    "\n",
    "# Build and train a simple feedforward neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(200, activation='relu', input_dim=len(features)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='linear'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on Test Data: {mse:.2f}\")\n",
    "\n",
    "# Display some predictions and actual values\n",
    "for i in range(10):  # Displaying first 10 predictions for illustration\n",
    "    print(f\"Prediction: {y_pred[i][0]:.2f}, Actual: {y_test[i]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9e5332df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8dbdcd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x283eb38a310>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "65e7979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 0s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[24.169115],\n",
       "       [21.08476 ],\n",
       "       [22.32435 ],\n",
       "       [20.448416],\n",
       "       [24.947948]], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dd04d926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.574236 ],\n",
       "       [4.7363257],\n",
       "       [2.9080608],\n",
       "       [3.7298281],\n",
       "       [5.32154  ]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "116152e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22.094     , 23.00081081, 21.76956522, 22.87569444, 26.94153846])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a26ba07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_11788\\2301369307.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[features] = scaler.fit_transform(X[features])\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Data (Stacking): 4.41\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming 'df' is your DataFrame with the provided data\n",
    "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
    "\n",
    "# Select relevant features for training\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df[features]\n",
    "y = df['Yield']\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X[features] = scaler.fit_transform(X[features])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.1, random_state=42)\n",
    "\n",
    "# Define individual regressors\n",
    "model1 = MLPRegressor(random_state=42, max_iter=200)\n",
    "model2 = RandomForestRegressor(random_state=42)\n",
    "model3 = SVR()\n",
    "\n",
    "# Define the stacking regressor with a final meta-regressor\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('model1', model1),\n",
    "        ('model2', model2),\n",
    "        ('model3', model3)\n",
    "    ],\n",
    "    final_estimator=MLPRegressor(random_state=42, max_iter=200)\n",
    ")\n",
    "\n",
    "# Train the stacking regressor\n",
    "stacking_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = stacking_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the stacking regressor\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on Test Data (Stacking): {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a2e4233a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_11788\\2080360075.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[features] = scaler.fit_transform(X[features])\n",
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3/3 [==============================] - 2s 203ms/step - loss: 540.3102 - val_loss: 448.8078\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 515.9800 - val_loss: 427.3500\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 493.2957 - val_loss: 407.5595\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 472.4359 - val_loss: 389.4828\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 453.4442 - val_loss: 373.0356\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 436.3278 - val_loss: 358.1037\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 420.5112 - val_loss: 344.6171\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 406.3135 - val_loss: 332.4274\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 393.3286 - val_loss: 321.4396\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 381.9421 - val_loss: 311.5129\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 371.2909 - val_loss: 302.6326\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 361.9862 - val_loss: 294.6535\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 353.4088 - val_loss: 287.4961\n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 345.8277 - val_loss: 281.0046\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 338.8923 - val_loss: 275.0749\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 332.5310 - val_loss: 269.6002\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 326.6264 - val_loss: 264.4918\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 321.1322 - val_loss: 259.6713\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 315.8494 - val_loss: 255.0976\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 310.9323 - val_loss: 250.7048\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 306.0393 - val_loss: 246.4917\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 301.4801 - val_loss: 242.3939\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 297.0097 - val_loss: 238.4044\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 292.5958 - val_loss: 234.5168\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 288.3481 - val_loss: 230.7029\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 284.1444 - val_loss: 226.9643\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 280.0099 - val_loss: 223.2922\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 275.9677 - val_loss: 219.6787\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 271.9992 - val_loss: 216.1233\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 268.0538 - val_loss: 212.6369\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 264.1670 - val_loss: 209.2160\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 260.3612 - val_loss: 205.8516\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 256.6132 - val_loss: 202.5439\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 252.9759 - val_loss: 199.2805\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 249.2979 - val_loss: 196.0786\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 245.7514 - val_loss: 192.9193\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 242.2427 - val_loss: 189.8071\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 238.7812 - val_loss: 186.7417\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 235.3540 - val_loss: 183.7254\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 231.9712 - val_loss: 180.7576\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 228.6550 - val_loss: 177.8316\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 225.4078 - val_loss: 174.9452\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 222.1844 - val_loss: 172.1030\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 218.9578 - val_loss: 169.3126\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 215.8784 - val_loss: 166.5490\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 212.7187 - val_loss: 163.8408\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 209.6746 - val_loss: 161.1663\n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 206.6868 - val_loss: 158.5207\n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 203.6846 - val_loss: 155.9183\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 200.7688 - val_loss: 153.3482\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 197.8286 - val_loss: 150.8226\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 194.9864 - val_loss: 148.3251\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 192.1835 - val_loss: 145.8566\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 189.3523 - val_loss: 143.4307\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 186.6213 - val_loss: 141.0311\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 183.9105 - val_loss: 138.6633\n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 181.1775 - val_loss: 136.3388\n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 178.5648 - val_loss: 134.0354\n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 175.9716 - val_loss: 131.7610\n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 173.3974 - val_loss: 129.5225\n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 170.8076 - val_loss: 127.3277\n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 168.2848 - val_loss: 125.1649\n",
      "Epoch 63/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 165.8123 - val_loss: 123.0282\n",
      "Epoch 64/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 163.3920 - val_loss: 120.9153\n",
      "Epoch 65/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 160.9529 - val_loss: 118.8388\n",
      "Epoch 66/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 158.5322 - val_loss: 116.7969\n",
      "Epoch 67/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 156.2117 - val_loss: 114.7729\n",
      "Epoch 68/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 153.8884 - val_loss: 112.7781\n",
      "Epoch 69/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 151.5804 - val_loss: 110.8141\n",
      "Epoch 70/200\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 149.3251 - val_loss: 108.8742\n",
      "Epoch 71/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 147.0683 - val_loss: 106.9646\n",
      "Epoch 72/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 144.8677 - val_loss: 105.0800\n",
      "Epoch 73/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 142.6974 - val_loss: 103.2208\n",
      "Epoch 74/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 140.5375 - val_loss: 101.3894\n",
      "Epoch 75/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 138.3911 - val_loss: 99.5874\n",
      "Epoch 76/200\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 136.3104 - val_loss: 97.8066\n",
      "Epoch 77/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 134.2318 - val_loss: 96.0532\n",
      "Epoch 78/200\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 132.2140 - val_loss: 94.3213\n",
      "Epoch 79/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 130.1788 - val_loss: 92.6211\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 38ms/step - loss: 128.2029 - val_loss: 90.9426\n",
      "Epoch 81/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 126.1751 - val_loss: 89.3004\n",
      "Epoch 82/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 124.2813 - val_loss: 87.6684\n",
      "Epoch 83/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 122.3886 - val_loss: 86.0547\n",
      "Epoch 84/200\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 120.5024 - val_loss: 84.4651\n",
      "Epoch 85/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 118.6185 - val_loss: 82.9051\n",
      "Epoch 86/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 116.7638 - val_loss: 81.3713\n",
      "Epoch 87/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 114.9746 - val_loss: 79.8546\n",
      "Epoch 88/200\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 113.1852 - val_loss: 78.3618\n",
      "Epoch 89/200\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 111.4109 - val_loss: 76.8946\n",
      "Epoch 90/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 109.6647 - val_loss: 75.4504\n",
      "Epoch 91/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 107.9477 - val_loss: 74.0282\n",
      "Epoch 92/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 106.2440 - val_loss: 72.6297\n",
      "Epoch 93/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 104.5882 - val_loss: 71.2486\n",
      "Epoch 94/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 102.9296 - val_loss: 69.8913\n",
      "Epoch 95/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 101.3190 - val_loss: 68.5519\n",
      "Epoch 96/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 99.7215 - val_loss: 67.2341\n",
      "Epoch 97/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 98.1108 - val_loss: 65.9435\n",
      "Epoch 98/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 96.5663 - val_loss: 64.6683\n",
      "Epoch 99/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 95.0278 - val_loss: 63.4126\n",
      "Epoch 100/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 93.5296 - val_loss: 62.1730\n",
      "Epoch 101/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 92.0206 - val_loss: 60.9558\n",
      "Epoch 102/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 90.5432 - val_loss: 59.7569\n",
      "Epoch 103/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 89.0765 - val_loss: 58.5778\n",
      "Epoch 104/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 87.6697 - val_loss: 57.4111\n",
      "Epoch 105/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 86.2146 - val_loss: 56.2716\n",
      "Epoch 106/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 84.8835 - val_loss: 55.1384\n",
      "Epoch 107/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 83.4854 - val_loss: 54.0324\n",
      "Epoch 108/200\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 82.1183 - val_loss: 52.9483\n",
      "Epoch 109/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 80.7985 - val_loss: 51.8802\n",
      "Epoch 110/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 79.5008 - val_loss: 50.8281\n",
      "Epoch 111/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 78.1641 - val_loss: 49.8037\n",
      "Epoch 112/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 76.8930 - val_loss: 48.7940\n",
      "Epoch 113/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 75.6623 - val_loss: 47.7933\n",
      "Epoch 114/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 74.4068 - val_loss: 46.8121\n",
      "Epoch 115/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 73.2064 - val_loss: 45.8425\n",
      "Epoch 116/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 72.0014 - val_loss: 44.8907\n",
      "Epoch 117/200\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 70.8053 - val_loss: 43.9587\n",
      "Epoch 118/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 69.6331 - val_loss: 43.0442\n",
      "Epoch 119/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 68.5062 - val_loss: 42.1419\n",
      "Epoch 120/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 67.3548 - val_loss: 41.2603\n",
      "Epoch 121/200\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 66.2304 - val_loss: 40.3949\n",
      "Epoch 122/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 65.1423 - val_loss: 39.5408\n",
      "Epoch 123/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 64.0556 - val_loss: 38.7011\n",
      "Epoch 124/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 62.9959 - val_loss: 37.8741\n",
      "Epoch 125/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 61.9365 - val_loss: 37.0639\n",
      "Epoch 126/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 60.9068 - val_loss: 36.2663\n",
      "Epoch 127/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 59.8949 - val_loss: 35.4815\n",
      "Epoch 128/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 58.8770 - val_loss: 34.7135\n",
      "Epoch 129/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 57.8911 - val_loss: 33.9578\n",
      "Epoch 130/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 56.9139 - val_loss: 33.2162\n",
      "Epoch 131/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 55.9554 - val_loss: 32.4874\n",
      "Epoch 132/200\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 55.0008 - val_loss: 31.7722\n",
      "Epoch 133/200\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 54.0670 - val_loss: 31.0700\n",
      "Epoch 134/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 53.1806 - val_loss: 30.3757\n",
      "Epoch 135/200\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 52.2674 - val_loss: 29.6979\n",
      "Epoch 136/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 51.3841 - val_loss: 29.0332\n",
      "Epoch 137/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 50.4886 - val_loss: 28.3863\n",
      "Epoch 138/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 49.6209 - val_loss: 27.7527\n",
      "Epoch 139/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 48.8118 - val_loss: 27.1244\n",
      "Epoch 140/200\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 47.9677 - val_loss: 26.5130\n",
      "Epoch 141/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 47.1423 - val_loss: 25.9155\n",
      "Epoch 142/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 46.3379 - val_loss: 25.3293\n",
      "Epoch 143/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 45.5753 - val_loss: 24.7499\n",
      "Epoch 144/200\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 44.7699 - val_loss: 24.1879\n",
      "Epoch 145/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 44.0320 - val_loss: 23.6321\n",
      "Epoch 146/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 43.2688 - val_loss: 23.0911\n",
      "Epoch 147/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 42.5216 - val_loss: 22.5629\n",
      "Epoch 148/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 41.8383 - val_loss: 22.0378\n",
      "Epoch 149/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 41.0620 - val_loss: 21.5357\n",
      "Epoch 150/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 40.3970 - val_loss: 21.0352\n",
      "Epoch 151/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 39.6947 - val_loss: 20.5468\n",
      "Epoch 152/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 39.0182 - val_loss: 20.0677\n",
      "Epoch 153/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 38.3602 - val_loss: 19.5965\n",
      "Epoch 154/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 37.6780 - val_loss: 19.1398\n",
      "Epoch 155/200\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 37.0528 - val_loss: 18.6880\n",
      "Epoch 156/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 36.4310 - val_loss: 18.2450\n",
      "Epoch 157/200\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 35.7829 - val_loss: 17.8172\n",
      "Epoch 158/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 35.1949 - val_loss: 17.3942\n",
      "Epoch 159/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 34.5862 - val_loss: 16.9827\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 29ms/step - loss: 33.9966 - val_loss: 16.5815\n",
      "Epoch 161/200\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 33.4259 - val_loss: 16.1891\n",
      "Epoch 162/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 32.8580 - val_loss: 15.8068\n",
      "Epoch 163/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 32.2879 - val_loss: 15.4356\n",
      "Epoch 164/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 31.7393 - val_loss: 15.0728\n",
      "Epoch 165/200\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 31.2119 - val_loss: 14.7158\n",
      "Epoch 166/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 30.6741 - val_loss: 14.3685\n",
      "Epoch 167/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 30.1685 - val_loss: 14.0269\n",
      "Epoch 168/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 29.6362 - val_loss: 13.6967\n",
      "Epoch 169/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 29.1531 - val_loss: 13.3697\n",
      "Epoch 170/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 28.6640 - val_loss: 13.0497\n",
      "Epoch 171/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 28.1576 - val_loss: 12.7407\n",
      "Epoch 172/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 27.7006 - val_loss: 12.4352\n",
      "Epoch 173/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 27.2329 - val_loss: 12.1373\n",
      "Epoch 174/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 26.7773 - val_loss: 11.8463\n",
      "Epoch 175/200\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 26.3200 - val_loss: 11.5641\n",
      "Epoch 176/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 25.8863 - val_loss: 11.2879\n",
      "Epoch 177/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 25.4444 - val_loss: 11.0208\n",
      "Epoch 178/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 25.0242 - val_loss: 10.7599\n",
      "Epoch 179/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 24.6050 - val_loss: 10.5062\n",
      "Epoch 180/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 24.1964 - val_loss: 10.2596\n",
      "Epoch 181/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 23.7939 - val_loss: 10.0195\n",
      "Epoch 182/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 23.4104 - val_loss: 9.7834\n",
      "Epoch 183/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 23.0234 - val_loss: 9.5535\n",
      "Epoch 184/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 22.6479 - val_loss: 9.3296\n",
      "Epoch 185/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 22.2757 - val_loss: 9.1121\n",
      "Epoch 186/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 21.8978 - val_loss: 8.9025\n",
      "Epoch 187/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 21.5543 - val_loss: 8.6964\n",
      "Epoch 188/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 21.2053 - val_loss: 8.4957\n",
      "Epoch 189/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 20.8544 - val_loss: 8.3011\n",
      "Epoch 190/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 20.5129 - val_loss: 8.1123\n",
      "Epoch 191/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 20.1873 - val_loss: 7.9278\n",
      "Epoch 192/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 19.8704 - val_loss: 7.7479\n",
      "Epoch 193/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 19.5456 - val_loss: 7.5748\n",
      "Epoch 194/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 19.2318 - val_loss: 7.4067\n",
      "Epoch 195/200\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 18.9350 - val_loss: 7.2426\n",
      "Epoch 196/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 18.6382 - val_loss: 7.0833\n",
      "Epoch 197/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 18.3379 - val_loss: 6.9300\n",
      "Epoch 198/200\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 18.0628 - val_loss: 6.7802\n",
      "Epoch 199/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 17.7745 - val_loss: 6.6365\n",
      "Epoch 200/200\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 17.4985 - val_loss: 6.4976\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Mean Squared Error on Test Data (Stacking with Random Forest): 0.85\n",
      "Stacking RF Prediction: 23.26, Actual: 22.09\n",
      "Stacking RF Prediction: 22.67, Actual: 23.00\n",
      "Stacking RF Prediction: 22.14, Actual: 21.77\n",
      "Stacking RF Prediction: 22.60, Actual: 22.88\n",
      "Stacking RF Prediction: 25.34, Actual: 26.94\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [104]\u001b[0m, in \u001b[0;36m<cell line: 57>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Display some predictions and actual values from stacking model\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# Displaying first 10 predictions for illustration\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStacking RF Prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_pred_stacking_rf[i]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Actual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_test[i]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 5 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load your dataset (replace 'your_data.csv' with your actual file)\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Assuming 'df' is your DataFrame with the provided data\n",
    "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
    "\n",
    "# Select relevant features for training\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df[features]\n",
    "y = df['Yield']\n",
    "\n",
    "# Normalize numerical features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X[features] = scaler.fit_transform(X[features])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.1, random_state=42)\n",
    "\n",
    "# Build and train a simple feedforward neural network model (base model)\n",
    "model_nn_base = Sequential()\n",
    "model_nn_base.add(Dense(200, activation='sigmoid', input_dim=len(features)))\n",
    "model_nn_base.add(Dense(100, activation='sigmoid'))\n",
    "model_nn_base.add(Dense(50, activation='sigmoid'))\n",
    "model_nn_base.add(Dense(1))\n",
    "model_nn_base.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "model_nn_base.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from base neural network model\n",
    "y_pred_nn_base = model_nn_base.predict(X_test)\n",
    "\n",
    "# Build and train a Random Forest Regressor as a meta-learner\n",
    "model_rf_meta = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_rf_meta.fit(y_pred_nn_base, y_test)\n",
    "\n",
    "# Make predictions using the meta-learner\n",
    "X_meta_test = model_nn_base.predict(X_test)  # Use base model predictions as input for meta-learner\n",
    "y_pred_stacking_rf = model_rf_meta.predict(X_meta_test)\n",
    "\n",
    "# Evaluate the stacking model with Random Forest as meta-learner\n",
    "mse_stacking_rf = mean_squared_error(y_test, y_pred_stacking_rf)\n",
    "print(f\"Mean Squared Error on Test Data (Stacking with Random Forest): {mse_stacking_rf:.2f}\")\n",
    "\n",
    "# Display some predictions and actual values from stacking model\n",
    "for i in range(10):  # Displaying first 10 predictions for illustration\n",
    "    print(f\"Stacking RF Prediction: {y_pred_stacking_rf[i]:.2f}, Actual: {y_test[i]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6d5e2819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_11788\\4182557709.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[features] = scaler.fit_transform(X[features])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 194ms/step - loss: 632.4147 - val_loss: 546.6656\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 628.7537 - val_loss: 543.9543\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 625.1417 - val_loss: 541.2313\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 622.0046 - val_loss: 538.5419\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 619.1100 - val_loss: 535.9323\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 616.2795 - val_loss: 533.3527\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 613.4742 - val_loss: 530.7606\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 610.4921 - val_loss: 528.1815\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 607.6484 - val_loss: 525.4375\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 604.5193 - val_loss: 522.5594\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 601.3088 - val_loss: 519.4656\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 597.7685 - val_loss: 516.1523\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 593.9562 - val_loss: 512.5607\n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 589.5684 - val_loss: 508.7032\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 584.8036 - val_loss: 504.4407\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 579.4241 - val_loss: 499.8438\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 573.4356 - val_loss: 494.7510\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 566.7751 - val_loss: 489.0331\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 559.7556 - val_loss: 482.6367\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 551.2964 - val_loss: 475.5843\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 542.1119 - val_loss: 467.8049\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 532.0117 - val_loss: 459.1806\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 520.6071 - val_loss: 449.6643\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 508.2529 - val_loss: 439.3335\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 494.9734 - val_loss: 428.0846\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 479.8155 - val_loss: 415.7641\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 464.2129 - val_loss: 402.5338\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 446.7701 - val_loss: 388.3193\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 428.0783 - val_loss: 372.9825\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 408.1155 - val_loss: 356.7106\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 386.9320 - val_loss: 339.5193\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 364.7086 - val_loss: 321.4886\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 340.8367 - val_loss: 302.4987\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 316.7368 - val_loss: 282.7678\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 291.8905 - val_loss: 262.3560\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 266.5665 - val_loss: 241.3841\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 241.4156 - val_loss: 220.1523\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 215.7268 - val_loss: 199.0741\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 191.3793 - val_loss: 178.3723\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 166.4328 - val_loss: 158.4633\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 144.4622 - val_loss: 139.5862\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 123.4889 - val_loss: 121.9503\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 102.6950 - val_loss: 106.1132\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 84.4451 - val_loss: 92.2881\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 68.9516 - val_loss: 80.4904\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 56.4468 - val_loss: 70.7068\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 45.5280 - val_loss: 63.3259\n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 37.5608 - val_loss: 57.8013\n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 31.8584 - val_loss: 53.9699\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 28.6341 - val_loss: 51.6287\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 25.1332 - val_loss: 50.3561\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 23.9407 - val_loss: 49.7599\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 23.1393 - val_loss: 49.4801\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 22.4872 - val_loss: 49.4596\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 21.8861 - val_loss: 49.4388\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 21.2962 - val_loss: 49.2094\n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 20.8460 - val_loss: 48.8477\n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 20.2142 - val_loss: 48.3147\n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 19.9127 - val_loss: 47.6941\n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 19.3470 - val_loss: 46.9953\n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 19.0921 - val_loss: 46.2671\n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 18.6555 - val_loss: 45.4600\n",
      "Epoch 63/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 18.2605 - val_loss: 44.5760\n",
      "Epoch 64/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 17.9558 - val_loss: 43.7360\n",
      "Epoch 65/200\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 17.6248 - val_loss: 42.8134\n",
      "Epoch 66/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 17.2698 - val_loss: 42.0281\n",
      "Epoch 67/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 17.0235 - val_loss: 41.1467\n",
      "Epoch 68/200\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 16.7020 - val_loss: 40.3580\n",
      "Epoch 69/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 16.4448 - val_loss: 39.5671\n",
      "Epoch 70/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 16.1545 - val_loss: 38.8465\n",
      "Epoch 71/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 15.8815 - val_loss: 38.1875\n",
      "Epoch 72/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 15.6375 - val_loss: 37.5965\n",
      "Epoch 73/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 15.4108 - val_loss: 37.1074\n",
      "Epoch 74/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 15.1408 - val_loss: 36.5902\n",
      "Epoch 75/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 14.9403 - val_loss: 36.0135\n",
      "Epoch 76/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 14.7429 - val_loss: 35.5281\n",
      "Epoch 77/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 14.4854 - val_loss: 35.0313\n",
      "Epoch 78/200\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 14.2541 - val_loss: 34.6225\n",
      "Epoch 79/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 14.1205 - val_loss: 34.3743\n",
      "Epoch 80/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 13.8630 - val_loss: 33.9965\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 32ms/step - loss: 13.6792 - val_loss: 33.4872\n",
      "Epoch 82/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 13.4391 - val_loss: 33.0256\n",
      "Epoch 83/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 13.2588 - val_loss: 32.5285\n",
      "Epoch 84/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 13.0429 - val_loss: 32.1279\n",
      "Epoch 85/200\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 12.8891 - val_loss: 31.7320\n",
      "Epoch 86/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 12.6835 - val_loss: 31.3213\n",
      "Epoch 87/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 12.5101 - val_loss: 30.9649\n",
      "Epoch 88/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 12.3353 - val_loss: 30.6273\n",
      "Epoch 89/200\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 12.1574 - val_loss: 30.2935\n",
      "Epoch 90/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 11.9933 - val_loss: 29.9784\n",
      "Epoch 91/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 11.8208 - val_loss: 29.6536\n",
      "Epoch 92/200\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 11.6582 - val_loss: 29.3278\n",
      "Epoch 93/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 11.5234 - val_loss: 29.0279\n",
      "Epoch 94/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 11.3823 - val_loss: 28.7959\n",
      "Epoch 95/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 11.2130 - val_loss: 28.4627\n",
      "Epoch 96/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 11.0693 - val_loss: 28.2294\n",
      "Epoch 97/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 10.9139 - val_loss: 27.8974\n",
      "Epoch 98/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 10.7868 - val_loss: 27.6656\n",
      "Epoch 99/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 10.6505 - val_loss: 27.3840\n",
      "Epoch 100/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 10.5333 - val_loss: 27.1462\n",
      "Epoch 101/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 10.3880 - val_loss: 26.9457\n",
      "Epoch 102/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 10.2725 - val_loss: 26.7192\n",
      "Epoch 103/200\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 10.1421 - val_loss: 26.4899\n",
      "Epoch 104/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 10.0340 - val_loss: 26.3714\n",
      "Epoch 105/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 9.9292 - val_loss: 26.2322\n",
      "Epoch 106/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 9.7977 - val_loss: 26.0237\n",
      "Epoch 107/200\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 9.6877 - val_loss: 25.7917\n",
      "Epoch 108/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 9.5963 - val_loss: 25.5088\n",
      "Epoch 109/200\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 9.4839 - val_loss: 25.3387\n",
      "Epoch 110/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 9.3539 - val_loss: 25.1775\n",
      "Epoch 111/200\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 9.2711 - val_loss: 25.1781\n",
      "Epoch 112/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 9.1594 - val_loss: 25.0440\n",
      "Epoch 113/200\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 9.0656 - val_loss: 24.9276\n",
      "Epoch 114/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 8.9961 - val_loss: 24.9162\n",
      "Epoch 115/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 8.8676 - val_loss: 24.6522\n",
      "Epoch 116/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 8.7640 - val_loss: 24.4464\n",
      "Epoch 117/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 8.6829 - val_loss: 24.2200\n",
      "Epoch 118/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 8.5786 - val_loss: 24.0896\n",
      "Epoch 119/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 8.5014 - val_loss: 23.9435\n",
      "Epoch 120/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 8.4262 - val_loss: 23.8922\n",
      "Epoch 121/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 8.3223 - val_loss: 23.7487\n",
      "Epoch 122/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 8.2337 - val_loss: 23.6918\n",
      "Epoch 123/200\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 8.1668 - val_loss: 23.5790\n",
      "Epoch 124/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 8.0777 - val_loss: 23.5414\n",
      "Epoch 125/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 7.9953 - val_loss: 23.5187\n",
      "Epoch 126/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 7.9146 - val_loss: 23.4254\n",
      "Epoch 127/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 7.8378 - val_loss: 23.3613\n",
      "Epoch 128/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 7.7670 - val_loss: 23.2134\n",
      "Epoch 129/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 7.6919 - val_loss: 22.9627\n",
      "Epoch 130/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 7.6325 - val_loss: 22.7133\n",
      "Epoch 131/200\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 7.5499 - val_loss: 22.5304\n",
      "Epoch 132/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 7.4843 - val_loss: 22.4963\n",
      "Epoch 133/200\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 7.4160 - val_loss: 22.4604\n",
      "Epoch 134/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 7.3476 - val_loss: 22.1569\n",
      "Epoch 135/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 7.2799 - val_loss: 22.0230\n",
      "Epoch 136/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 7.2163 - val_loss: 21.8903\n",
      "Epoch 137/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 7.1463 - val_loss: 21.8280\n",
      "Epoch 138/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 7.0841 - val_loss: 21.8301\n",
      "Epoch 139/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 7.0304 - val_loss: 21.8762\n",
      "Epoch 140/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 6.9714 - val_loss: 21.9840\n",
      "Epoch 141/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 6.9061 - val_loss: 22.0195\n",
      "Epoch 142/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 6.8830 - val_loss: 22.1279\n",
      "Epoch 143/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 6.8060 - val_loss: 21.8798\n",
      "Epoch 144/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 6.7436 - val_loss: 21.7244\n",
      "Epoch 145/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 6.6776 - val_loss: 21.5635\n",
      "Epoch 146/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 6.6234 - val_loss: 21.4273\n",
      "Epoch 147/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 6.5802 - val_loss: 21.2280\n",
      "Epoch 148/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 6.5236 - val_loss: 21.0668\n",
      "Epoch 149/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 6.4761 - val_loss: 20.9903\n",
      "Epoch 150/200\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 6.4333 - val_loss: 20.7797\n",
      "Epoch 151/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 6.3811 - val_loss: 20.7137\n",
      "Epoch 152/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 6.3263 - val_loss: 20.7098\n",
      "Epoch 153/200\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 6.2774 - val_loss: 20.8029\n",
      "Epoch 154/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 6.2348 - val_loss: 20.7127\n",
      "Epoch 155/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 6.1991 - val_loss: 20.7659\n",
      "Epoch 156/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 6.1455 - val_loss: 20.6341\n",
      "Epoch 157/200\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 6.1050 - val_loss: 20.5002\n",
      "Epoch 158/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 6.0898 - val_loss: 20.5304\n",
      "Epoch 159/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 6.0167 - val_loss: 20.2698\n",
      "Epoch 160/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 5.9907 - val_loss: 20.0709\n",
      "Epoch 161/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 5.9458 - val_loss: 20.0351\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 35ms/step - loss: 5.9089 - val_loss: 20.0619\n",
      "Epoch 163/200\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 5.8663 - val_loss: 20.1716\n",
      "Epoch 164/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 5.8283 - val_loss: 20.1788\n",
      "Epoch 165/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 5.7958 - val_loss: 20.1031\n",
      "Epoch 166/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 5.7569 - val_loss: 20.2091\n",
      "Epoch 167/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 5.7248 - val_loss: 20.2642\n",
      "Epoch 168/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 5.6861 - val_loss: 20.2465\n",
      "Epoch 169/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.6506 - val_loss: 20.2712\n",
      "Epoch 170/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 5.6244 - val_loss: 20.2878\n",
      "Epoch 171/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 5.5913 - val_loss: 20.1190\n",
      "Epoch 172/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 5.5535 - val_loss: 20.0156\n",
      "Epoch 173/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 5.5419 - val_loss: 19.7552\n",
      "Epoch 174/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 5.4890 - val_loss: 19.6544\n",
      "Epoch 175/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 5.4627 - val_loss: 19.6896\n",
      "Epoch 176/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 5.4398 - val_loss: 19.5764\n",
      "Epoch 177/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 5.4068 - val_loss: 19.5176\n",
      "Epoch 178/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 5.3641 - val_loss: 19.6993\n",
      "Epoch 179/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 5.3532 - val_loss: 19.8347\n",
      "Epoch 180/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 5.3103 - val_loss: 19.7552\n",
      "Epoch 181/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 5.2847 - val_loss: 19.8887\n",
      "Epoch 182/200\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 5.2546 - val_loss: 19.8115\n",
      "Epoch 183/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 5.2317 - val_loss: 19.5884\n",
      "Epoch 184/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 5.2175 - val_loss: 19.6565\n",
      "Epoch 185/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 5.2278 - val_loss: 19.2263\n",
      "Epoch 186/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 5.1402 - val_loss: 19.2111\n",
      "Epoch 187/200\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 5.1220 - val_loss: 19.1756\n",
      "Epoch 188/200\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 5.0976 - val_loss: 19.2013\n",
      "Epoch 189/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 5.0653 - val_loss: 19.1487\n",
      "Epoch 190/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 5.0620 - val_loss: 19.3044\n",
      "Epoch 191/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 5.0221 - val_loss: 19.2273\n",
      "Epoch 192/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.0130 - val_loss: 18.8397\n",
      "Epoch 193/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 4.9717 - val_loss: 18.7633\n",
      "Epoch 194/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 4.9509 - val_loss: 18.6495\n",
      "Epoch 195/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 4.9326 - val_loss: 18.6145\n",
      "Epoch 196/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 4.9027 - val_loss: 18.5141\n",
      "Epoch 197/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 4.8795 - val_loss: 18.4581\n",
      "Epoch 198/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 4.8647 - val_loss: 18.6117\n",
      "Epoch 199/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 4.8348 - val_loss: 18.5687\n",
      "Epoch 200/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 4.8186 - val_loss: 18.4787\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 63ms/step - loss: 623.9841 - val_loss: 628.8761\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 623.5862 - val_loss: 628.7748\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 623.2726 - val_loss: 628.6623\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 622.9367 - val_loss: 628.5768\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 622.6094 - val_loss: 628.4288\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 622.2648 - val_loss: 628.2983\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 621.9344 - val_loss: 628.1760\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 621.6060 - val_loss: 628.0321\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 621.2691 - val_loss: 627.8843\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 620.9456 - val_loss: 627.7367\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 620.6181 - val_loss: 627.5742\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 620.2774 - val_loss: 627.4056\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 619.9585 - val_loss: 627.2390\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 619.6241 - val_loss: 627.0905\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 619.3060 - val_loss: 626.9484\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 618.9697 - val_loss: 626.8101\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 618.6661 - val_loss: 626.7054\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 618.3293 - val_loss: 626.5177\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 618.0012 - val_loss: 626.3303\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 617.6793 - val_loss: 626.1653\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 617.3631 - val_loss: 626.0044\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 617.0374 - val_loss: 625.8157\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 616.7190 - val_loss: 625.6481\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 616.3969 - val_loss: 625.4931\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 616.0758 - val_loss: 625.3446\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 615.7414 - val_loss: 625.1522\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 615.4201 - val_loss: 625.0112\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 615.0888 - val_loss: 624.8353\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 614.7801 - val_loss: 624.6512\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 614.4514 - val_loss: 624.4911\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 614.1280 - val_loss: 624.3711\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 613.8083 - val_loss: 624.2244\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 613.4941 - val_loss: 624.0393\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 613.1573 - val_loss: 623.8941\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 612.8396 - val_loss: 623.7599\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 612.5223 - val_loss: 623.5808\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 612.2017 - val_loss: 623.4709\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 611.8705 - val_loss: 623.3251\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 611.5673 - val_loss: 623.2197\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 611.2351 - val_loss: 623.0184\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 610.9130 - val_loss: 622.8580\n",
      "Epoch 42/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 17ms/step - loss: 610.5839 - val_loss: 622.6821\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 610.2623 - val_loss: 622.5114\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 609.9502 - val_loss: 622.3679\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 609.6140 - val_loss: 622.2595\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 609.2889 - val_loss: 622.1265\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 608.9538 - val_loss: 621.9745\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 608.6482 - val_loss: 621.8343\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 608.2999 - val_loss: 621.6879\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 607.9815 - val_loss: 621.5519\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 607.6426 - val_loss: 621.4363\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 607.3245 - val_loss: 621.3409\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 606.9882 - val_loss: 621.1983\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 606.6840 - val_loss: 621.0120\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 606.3655 - val_loss: 620.8656\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 606.0323 - val_loss: 620.6656\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 605.7172 - val_loss: 620.4728\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 605.4109 - val_loss: 620.3010\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 605.0889 - val_loss: 620.1620\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 604.7608 - val_loss: 620.0234\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 604.4521 - val_loss: 619.8705\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 604.1251 - val_loss: 619.7204\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 603.8149 - val_loss: 619.5916\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 603.4962 - val_loss: 619.4240\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 603.1618 - val_loss: 619.2174\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 602.8445 - val_loss: 619.0298\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 602.5350 - val_loss: 618.8718\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 602.2106 - val_loss: 618.6866\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 601.9047 - val_loss: 618.5463\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 601.5836 - val_loss: 618.3759\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 601.2753 - val_loss: 618.2191\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 600.9608 - val_loss: 618.0647\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 600.6275 - val_loss: 617.9164\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 600.3121 - val_loss: 617.7599\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 599.9968 - val_loss: 617.6223\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 599.6736 - val_loss: 617.4430\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 599.3563 - val_loss: 617.2759\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 599.0551 - val_loss: 617.1410\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 598.7212 - val_loss: 616.9662\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 598.4051 - val_loss: 616.8079\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 598.0883 - val_loss: 616.6898\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 597.7726 - val_loss: 616.5549\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 597.4504 - val_loss: 616.3727\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 597.1428 - val_loss: 616.2235\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 596.8187 - val_loss: 616.0797\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 596.5236 - val_loss: 615.9304\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 596.1905 - val_loss: 615.7537\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 595.8718 - val_loss: 615.6021\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 595.5521 - val_loss: 615.4319\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 595.2687 - val_loss: 615.2982\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 594.9264 - val_loss: 615.1312\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 594.6103 - val_loss: 614.9559\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 594.3154 - val_loss: 614.7654\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 593.9858 - val_loss: 614.5659\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 593.7012 - val_loss: 614.4369\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 593.3664 - val_loss: 614.2665\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 593.0526 - val_loss: 614.1046\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 592.7332 - val_loss: 613.9752\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 592.4140 - val_loss: 613.8371\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 592.1052 - val_loss: 613.7083\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 591.7877 - val_loss: 613.5822\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 591.4685 - val_loss: 613.4744\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 591.1566 - val_loss: 613.3744\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 590.8352 - val_loss: 613.2026\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 590.5082 - val_loss: 613.0522\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 590.1920 - val_loss: 612.8928\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 589.8895 - val_loss: 612.7306\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 589.5743 - val_loss: 612.5620\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 589.2697 - val_loss: 612.3882\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 588.9467 - val_loss: 612.2739\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 588.6396 - val_loss: 612.1194\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 588.3435 - val_loss: 611.9873\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 588.0074 - val_loss: 611.8105\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 587.6954 - val_loss: 611.6785\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 587.3907 - val_loss: 611.5518\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 587.0731 - val_loss: 611.4016\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 586.7657 - val_loss: 611.2949\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 586.4578 - val_loss: 611.1161\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 586.1359 - val_loss: 610.9667\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 585.8173 - val_loss: 610.8290\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 15ms/step - loss: 585.5110 - val_loss: 610.6929\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 585.1892 - val_loss: 610.5392\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 584.8796 - val_loss: 610.3915\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 584.5624 - val_loss: 610.2354\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 584.2604 - val_loss: 610.0748\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 583.9507 - val_loss: 609.9552\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 583.6395 - val_loss: 609.7548\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 583.3445 - val_loss: 609.6099\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 583.0427 - val_loss: 609.5140\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 582.7397 - val_loss: 609.3431\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 582.4285 - val_loss: 609.1866\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 582.1244 - val_loss: 609.0456\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 581.8222 - val_loss: 608.9217\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 581.5019 - val_loss: 608.7809\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 581.1974 - val_loss: 608.6095\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 580.8830 - val_loss: 608.4327\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 580.5847 - val_loss: 608.2673\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 580.2735 - val_loss: 608.1479\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 579.9556 - val_loss: 608.0327\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 579.6431 - val_loss: 607.8776\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 579.3413 - val_loss: 607.7355\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 579.0148 - val_loss: 607.6049\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 578.7073 - val_loss: 607.4466\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 578.4188 - val_loss: 607.3014\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 578.1069 - val_loss: 607.1538\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 577.7976 - val_loss: 606.9952\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 577.4922 - val_loss: 606.8300\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 577.2071 - val_loss: 606.7354\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 576.8931 - val_loss: 606.5578\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 576.5878 - val_loss: 606.3784\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 576.2924 - val_loss: 606.2313\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 575.9821 - val_loss: 606.1066\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 575.6831 - val_loss: 605.9800\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 575.3759 - val_loss: 605.8527\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 575.0690 - val_loss: 605.6926\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 574.7733 - val_loss: 605.5366\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 574.4603 - val_loss: 605.3918\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 574.1647 - val_loss: 605.2365\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 573.8521 - val_loss: 605.1032\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 573.5373 - val_loss: 604.9750\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 573.2332 - val_loss: 604.8410\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 572.9332 - val_loss: 604.7001\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 572.6308 - val_loss: 604.5912\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 572.3305 - val_loss: 604.4299\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 572.0034 - val_loss: 604.2964\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 571.7026 - val_loss: 604.1479\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 571.4020 - val_loss: 603.9971\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 571.1036 - val_loss: 603.8635\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 570.8005 - val_loss: 603.6984\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 570.5078 - val_loss: 603.6072\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 570.2115 - val_loss: 603.4615\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 569.9111 - val_loss: 603.3387\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 569.6109 - val_loss: 603.1828\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 569.3112 - val_loss: 603.0587\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 569.0095 - val_loss: 602.9049\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 568.7071 - val_loss: 602.7635\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 568.4047 - val_loss: 602.6219\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 568.0996 - val_loss: 602.4625\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 567.8047 - val_loss: 602.3201\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 567.5125 - val_loss: 602.2274\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 567.2042 - val_loss: 602.0485\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 566.8989 - val_loss: 601.9030\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 566.5952 - val_loss: 601.7482\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 566.3035 - val_loss: 601.6005\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 565.9954 - val_loss: 601.4460\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 565.7003 - val_loss: 601.2539\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 565.4000 - val_loss: 601.1113\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 565.0966 - val_loss: 600.9236\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 564.7977 - val_loss: 600.7610\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 564.5139 - val_loss: 600.5914\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 564.2081 - val_loss: 600.4585\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 563.8918 - val_loss: 600.2962\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 563.5875 - val_loss: 600.1281\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 563.2932 - val_loss: 599.9872\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 563.0100 - val_loss: 599.8165\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 562.6855 - val_loss: 599.6703\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 562.3858 - val_loss: 599.5544\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 562.0862 - val_loss: 599.4274\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 561.7866 - val_loss: 599.2587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 561.4741 - val_loss: 599.1266\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Mean Squared Error on Test Data (Stacking with Random Forest): 0.50\n",
      "Stacking RF Prediction: 22.41, Actual: 22.09\n",
      "Stacking RF Prediction: 22.64, Actual: 23.00\n",
      "Stacking RF Prediction: 22.12, Actual: 21.77\n",
      "Stacking RF Prediction: 23.11, Actual: 22.88\n",
      "Stacking RF Prediction: 25.49, Actual: 26.94\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [107]\u001b[0m, in \u001b[0;36m<cell line: 73>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Display some predictions and actual values from stacking model\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# Displaying first 10 predictions for illustration\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStacking RF Prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_pred_stacking_rf[i]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Actual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_test[i]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 5 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load your dataset (replace 'your_data.csv' with your actual file)\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Assuming 'df' is your DataFrame with the provided data\n",
    "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
    "\n",
    "# Select relevant features for training\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df[features]\n",
    "y = df['Yield']\n",
    "\n",
    "# Normalize numerical features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X[features] = scaler.fit_transform(X[features])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.1, random_state=42)\n",
    "\n",
    "# Build and train the first neural network (base model 1)\n",
    "model_nn_base_1 = Sequential()\n",
    "model_nn_base_1.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(10, activation='relu'))\n",
    "model_nn_base_1.add(Dense(1, activation='linear'))\n",
    "model_nn_base_1.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "model_nn_base_1.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the first base neural network model\n",
    "y_pred_nn_base_1 = model_nn_base_1.predict(X_test)\n",
    "\n",
    "# Build and train the second neural network (base model 2)\n",
    "model_nn_base_2 = Sequential()\n",
    "model_nn_base_1.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_1.add(Dense(25, activation='relu'))\n",
    "model_nn_base_1.add(Dense(10, activation='relu'))\n",
    "model_nn_base_1.add(Dense(5, activation='relu'))\n",
    "model_nn_base_2.add(Dense(1, activation='linear'))\n",
    "model_nn_base_2.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "model_nn_base_2.fit(X_train, y_train, epochs=200, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the second base neural network model\n",
    "y_pred_nn_base_2 = model_nn_base_2.predict(X_test)\n",
    "\n",
    "# Combine predictions from both base models\n",
    "X_meta_train = np.concatenate((y_pred_nn_base_1, y_pred_nn_base_2), axis=1)\n",
    "\n",
    "# Build and train a Random Forest Regressor as a meta-learner\n",
    "model_rf_meta = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_rf_meta.fit(X_meta_train, y_test)\n",
    "\n",
    "# Make predictions using the base models and meta-learner\n",
    "X_meta_test = np.concatenate((model_nn_base_1.predict(X_test), model_nn_base_2.predict(X_test)), axis=1)\n",
    "y_pred_stacking_rf = model_rf_meta.predict(X_meta_test)\n",
    "\n",
    "# Evaluate the stacking model with Random Forest as meta-learner\n",
    "mse_stacking_rf = mean_squared_error(y_test, y_pred_stacking_rf)\n",
    "print(f\"Mean Squared Error on Test Data (Stacking with Random Forest): {mse_stacking_rf:.2f}\")\n",
    "\n",
    "# Display some predictions and actual values from stacking model\n",
    "for i in range(10):  # Displaying first 10 predictions for illustration\n",
    "    print(f\"Stacking RF Prediction: {y_pred_stacking_rf[i]:.2f}, Actual: {y_test[i]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "970b6534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_11788\\12919860.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[features] = scaler.fit_transform(X[features])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 6s 2s/step - loss: 628.6588 - val_loss: 553.5969\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 625.7600 - val_loss: 550.5045\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 622.6680 - val_loss: 546.9431\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 619.6068 - val_loss: 543.1630\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 616.5020 - val_loss: 539.8273\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 613.2422 - val_loss: 536.8101\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 609.9078 - val_loss: 533.7501\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 606.5329 - val_loss: 530.4692\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 602.5201 - val_loss: 526.9831\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 598.2409 - val_loss: 522.9606\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 594.0841 - val_loss: 518.1863\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 588.8748 - val_loss: 512.1659\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 583.5225 - val_loss: 505.2047\n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 577.2094 - val_loss: 497.6619\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 570.2611 - val_loss: 489.5876\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 563.0020 - val_loss: 480.8984\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 554.9752 - val_loss: 471.6105\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 546.4317 - val_loss: 461.8373\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 536.6706 - val_loss: 451.4998\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 526.3868 - val_loss: 440.4268\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 514.8463 - val_loss: 428.4531\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 502.5440 - val_loss: 415.6483\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 489.2662 - val_loss: 401.8956\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 474.6455 - val_loss: 387.2484\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 459.7535 - val_loss: 371.4505\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 443.0194 - val_loss: 354.8836\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 425.3924 - val_loss: 337.3931\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 406.7127 - val_loss: 318.9276\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 386.8996 - val_loss: 299.7397\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 366.3749 - val_loss: 280.1605\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 344.6806 - val_loss: 260.1667\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 322.3473 - val_loss: 239.7574\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 299.6135 - val_loss: 219.0947\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 276.3813 - val_loss: 198.4094\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 252.9892 - val_loss: 178.0198\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 229.0348 - val_loss: 158.2070\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 205.8407 - val_loss: 139.1790\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 182.5131 - val_loss: 121.1975\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 160.0983 - val_loss: 104.4535\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 138.8418 - val_loss: 89.2787\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 119.0469 - val_loss: 75.9007\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 100.9740 - val_loss: 64.5577\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 83.4766 - val_loss: 55.2553\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 68.7901 - val_loss: 48.0521\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 56.4149 - val_loss: 42.8754\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 46.3250 - val_loss: 39.6234\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 38.2707 - val_loss: 38.0294\n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 32.4469 - val_loss: 37.5221\n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 28.0164 - val_loss: 37.8220\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 25.7100 - val_loss: 38.4661\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 23.5751 - val_loss: 39.0515\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 22.5502 - val_loss: 39.4432\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 21.6403 - val_loss: 39.5078\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 20.7732 - val_loss: 39.2297\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 20.2966 - val_loss: 38.5878\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 19.6473 - val_loss: 37.9466\n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 19.1049 - val_loss: 36.8953\n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 18.4756 - val_loss: 35.8870\n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 18.0620 - val_loss: 34.8758\n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 17.4809 - val_loss: 34.0085\n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 17.2063 - val_loss: 33.1742\n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 16.8138 - val_loss: 32.5210\n",
      "Epoch 63/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 16.5651 - val_loss: 31.9158\n",
      "Epoch 64/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 16.2227 - val_loss: 31.3525\n",
      "Epoch 65/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 15.9104 - val_loss: 30.8389\n",
      "Epoch 66/200\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 15.5646 - val_loss: 30.3949\n",
      "Epoch 67/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 15.3390 - val_loss: 30.0179\n",
      "Epoch 68/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 14.9947 - val_loss: 29.5964\n",
      "Epoch 69/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 14.7222 - val_loss: 29.2852\n",
      "Epoch 70/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 14.4439 - val_loss: 29.0191\n",
      "Epoch 71/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 14.2121 - val_loss: 28.8681\n",
      "Epoch 72/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 14.0440 - val_loss: 28.7338\n",
      "Epoch 73/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 13.7899 - val_loss: 28.5341\n",
      "Epoch 74/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 13.5937 - val_loss: 28.3227\n",
      "Epoch 75/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 13.3907 - val_loss: 27.8183\n",
      "Epoch 76/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 13.1617 - val_loss: 27.5969\n",
      "Epoch 77/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 12.9450 - val_loss: 27.2507\n",
      "Epoch 78/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 12.7319 - val_loss: 27.0113\n",
      "Epoch 79/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 12.5805 - val_loss: 26.7686\n",
      "Epoch 80/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 12.3861 - val_loss: 26.7010\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 36ms/step - loss: 12.1963 - val_loss: 26.5361\n",
      "Epoch 82/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 12.0311 - val_loss: 26.5648\n",
      "Epoch 83/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 11.8557 - val_loss: 26.3893\n",
      "Epoch 84/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 11.6846 - val_loss: 26.3001\n",
      "Epoch 85/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 11.5469 - val_loss: 26.1615\n",
      "Epoch 86/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 11.4218 - val_loss: 26.4337\n",
      "Epoch 87/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 11.1974 - val_loss: 26.3842\n",
      "Epoch 88/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 11.0637 - val_loss: 26.4869\n",
      "Epoch 89/200\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 10.9352 - val_loss: 26.5880\n",
      "Epoch 90/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 10.7826 - val_loss: 26.5625\n",
      "Epoch 91/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 10.6605 - val_loss: 26.5043\n",
      "Epoch 92/200\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 10.5249 - val_loss: 26.6889\n",
      "Epoch 93/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 10.3775 - val_loss: 26.7556\n",
      "Epoch 94/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 10.2462 - val_loss: 26.6526\n",
      "Epoch 95/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 10.1268 - val_loss: 26.7995\n",
      "Epoch 96/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 10.0025 - val_loss: 26.8980\n",
      "Epoch 97/200\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 9.8510 - val_loss: 26.8473\n",
      "Epoch 98/200\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 9.7610 - val_loss: 26.7196\n",
      "Epoch 99/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 9.6093 - val_loss: 26.6966\n",
      "Epoch 100/200\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 9.5103 - val_loss: 26.7312\n",
      "Epoch 101/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 9.4275 - val_loss: 26.8627\n",
      "Epoch 102/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 9.2905 - val_loss: 27.0344\n",
      "Epoch 103/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 9.1920 - val_loss: 27.1452\n",
      "Epoch 104/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 9.0830 - val_loss: 27.3478\n",
      "Epoch 105/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 8.9724 - val_loss: 27.4809\n",
      "Epoch 106/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 8.8888 - val_loss: 27.7492\n",
      "Epoch 107/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 8.7922 - val_loss: 27.8441\n",
      "Epoch 108/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 8.6869 - val_loss: 27.9281\n",
      "Epoch 109/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 8.6055 - val_loss: 27.9086\n",
      "Epoch 110/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 8.5058 - val_loss: 27.8360\n",
      "Epoch 111/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 8.4124 - val_loss: 27.8147\n",
      "Epoch 112/200\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 8.3283 - val_loss: 27.8324\n",
      "Epoch 113/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 8.2356 - val_loss: 27.7439\n",
      "Epoch 114/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 8.1457 - val_loss: 27.7677\n",
      "Epoch 115/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 8.0608 - val_loss: 27.6475\n",
      "Epoch 116/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 7.9930 - val_loss: 27.7356\n",
      "Epoch 117/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 7.9029 - val_loss: 27.8549\n",
      "Epoch 118/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 7.8180 - val_loss: 27.9574\n",
      "Epoch 119/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 7.7629 - val_loss: 28.0367\n",
      "Epoch 120/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 7.6748 - val_loss: 28.2484\n",
      "Epoch 121/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 7.6101 - val_loss: 28.3129\n",
      "Epoch 122/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 7.5449 - val_loss: 28.5171\n",
      "Epoch 123/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 7.4795 - val_loss: 28.8024\n",
      "Epoch 124/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 7.4159 - val_loss: 28.9616\n",
      "Epoch 125/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 7.3738 - val_loss: 29.1481\n",
      "Epoch 126/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 7.2976 - val_loss: 28.9954\n",
      "Epoch 127/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 7.2308 - val_loss: 28.9790\n",
      "Epoch 128/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 7.1792 - val_loss: 29.0259\n",
      "Epoch 129/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 7.1332 - val_loss: 28.9222\n",
      "Epoch 130/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 7.0653 - val_loss: 29.1047\n",
      "Epoch 131/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 7.0181 - val_loss: 29.1700\n",
      "Epoch 132/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 6.9711 - val_loss: 29.3695\n",
      "Epoch 133/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 6.9182 - val_loss: 29.3939\n",
      "Epoch 134/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 6.8734 - val_loss: 29.3816\n",
      "Epoch 135/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 6.8216 - val_loss: 29.4616\n",
      "Epoch 136/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 6.7690 - val_loss: 29.3542\n",
      "Epoch 137/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 6.7248 - val_loss: 29.4518\n",
      "Epoch 138/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 6.6788 - val_loss: 29.3647\n",
      "Epoch 139/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 6.6370 - val_loss: 29.2772\n",
      "Epoch 140/200\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 6.5833 - val_loss: 29.1661\n",
      "Epoch 141/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 6.5403 - val_loss: 28.9393\n",
      "Epoch 142/200\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 6.4979 - val_loss: 28.8693\n",
      "Epoch 143/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 6.4560 - val_loss: 28.7155\n",
      "Epoch 144/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 6.4264 - val_loss: 28.6357\n",
      "Epoch 145/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 6.3743 - val_loss: 28.7671\n",
      "Epoch 146/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 6.3345 - val_loss: 29.0672\n",
      "Epoch 147/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 6.2949 - val_loss: 29.1294\n",
      "Epoch 148/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 6.2555 - val_loss: 29.0850\n",
      "Epoch 149/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 6.2185 - val_loss: 29.3415\n",
      "Epoch 150/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 6.1981 - val_loss: 29.7012\n",
      "Epoch 151/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 6.1384 - val_loss: 29.6478\n",
      "Epoch 152/200\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 6.1063 - val_loss: 29.5543\n",
      "Epoch 153/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 6.0697 - val_loss: 29.3977\n",
      "Epoch 154/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 6.0347 - val_loss: 29.2257\n",
      "Epoch 155/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 5.9871 - val_loss: 29.2647\n",
      "Epoch 156/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 5.9630 - val_loss: 29.3209\n",
      "Epoch 157/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 5.9265 - val_loss: 29.0722\n",
      "Epoch 158/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.8864 - val_loss: 28.9402\n",
      "Epoch 159/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 5.8527 - val_loss: 28.8567\n",
      "Epoch 160/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 5.8142 - val_loss: 28.7140\n",
      "Epoch 161/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.7932 - val_loss: 28.3349\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 30ms/step - loss: 5.7523 - val_loss: 28.2126\n",
      "Epoch 163/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 5.7179 - val_loss: 28.4548\n",
      "Epoch 164/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 5.6935 - val_loss: 28.7254\n",
      "Epoch 165/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 5.6555 - val_loss: 28.6005\n",
      "Epoch 166/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 5.6260 - val_loss: 28.5672\n",
      "Epoch 167/200\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 5.5899 - val_loss: 28.3787\n",
      "Epoch 168/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.5638 - val_loss: 28.3643\n",
      "Epoch 169/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.5431 - val_loss: 28.2817\n",
      "Epoch 170/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 5.5240 - val_loss: 28.3141\n",
      "Epoch 171/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 5.4760 - val_loss: 28.1633\n",
      "Epoch 172/200\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 5.4469 - val_loss: 27.9730\n",
      "Epoch 173/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 5.4247 - val_loss: 27.8655\n",
      "Epoch 174/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 5.4165 - val_loss: 27.5611\n",
      "Epoch 175/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.3802 - val_loss: 27.6967\n",
      "Epoch 176/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 5.3532 - val_loss: 27.6736\n",
      "Epoch 177/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 5.3264 - val_loss: 27.6460\n",
      "Epoch 178/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 5.3017 - val_loss: 27.8037\n",
      "Epoch 179/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 5.2737 - val_loss: 27.8235\n",
      "Epoch 180/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 5.2450 - val_loss: 27.8542\n",
      "Epoch 181/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 5.2203 - val_loss: 27.9530\n",
      "Epoch 182/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 5.2068 - val_loss: 28.1757\n",
      "Epoch 183/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 5.1734 - val_loss: 28.0553\n",
      "Epoch 184/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 5.1697 - val_loss: 27.7247\n",
      "Epoch 185/200\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 5.1193 - val_loss: 27.7697\n",
      "Epoch 186/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 5.1108 - val_loss: 27.8991\n",
      "Epoch 187/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 5.0788 - val_loss: 27.7262\n",
      "Epoch 188/200\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 5.0555 - val_loss: 27.4602\n",
      "Epoch 189/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 5.0316 - val_loss: 27.1625\n",
      "Epoch 190/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 5.0014 - val_loss: 26.9175\n",
      "Epoch 191/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 4.9920 - val_loss: 26.5423\n",
      "Epoch 192/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 4.9702 - val_loss: 26.5350\n",
      "Epoch 193/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 4.9353 - val_loss: 26.2744\n",
      "Epoch 194/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 4.9148 - val_loss: 26.1393\n",
      "Epoch 195/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 4.8970 - val_loss: 25.9823\n",
      "Epoch 196/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 4.8754 - val_loss: 26.0118\n",
      "Epoch 197/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 4.8482 - val_loss: 26.2005\n",
      "Epoch 198/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 4.8359 - val_loss: 26.3448\n",
      "Epoch 199/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.8050 - val_loss: 26.5325\n",
      "Epoch 200/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 4.7842 - val_loss: 26.4416\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 60ms/step - loss: 609.1138 - val_loss: 537.3124\n",
      "Epoch 2/200\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 599.3102 - val_loss: 528.9713\n",
      "Epoch 3/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 589.3625 - val_loss: 517.6880\n",
      "Epoch 4/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 577.1800 - val_loss: 503.1531\n",
      "Epoch 5/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 562.3089 - val_loss: 485.7575\n",
      "Epoch 6/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 542.7737 - val_loss: 464.4688\n",
      "Epoch 7/200\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 517.5502 - val_loss: 440.0333\n",
      "Epoch 8/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 489.2846 - val_loss: 409.9802\n",
      "Epoch 9/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 453.8693 - val_loss: 372.2196\n",
      "Epoch 10/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 411.8055 - val_loss: 329.8089\n",
      "Epoch 11/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 364.1029 - val_loss: 282.0059\n",
      "Epoch 12/200\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 310.7277 - val_loss: 227.4421\n",
      "Epoch 13/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 252.5779 - val_loss: 174.2187\n",
      "Epoch 14/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 195.8389 - val_loss: 124.0086\n",
      "Epoch 15/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 142.9329 - val_loss: 82.4571\n",
      "Epoch 16/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 100.6753 - val_loss: 56.6705\n",
      "Epoch 17/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 71.7261 - val_loss: 45.1520\n",
      "Epoch 18/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 51.9548 - val_loss: 42.5944\n",
      "Epoch 19/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 40.7925 - val_loss: 40.5966\n",
      "Epoch 20/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 33.6317 - val_loss: 39.0673\n",
      "Epoch 21/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 30.0722 - val_loss: 36.8413\n",
      "Epoch 22/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 26.9819 - val_loss: 33.7526\n",
      "Epoch 23/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 24.6082 - val_loss: 31.7000\n",
      "Epoch 24/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 23.0444 - val_loss: 29.3904\n",
      "Epoch 25/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 22.0614 - val_loss: 27.9300\n",
      "Epoch 26/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 20.8816 - val_loss: 26.1495\n",
      "Epoch 27/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 19.7160 - val_loss: 24.8503\n",
      "Epoch 28/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 19.1533 - val_loss: 24.3442\n",
      "Epoch 29/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 18.0868 - val_loss: 22.8388\n",
      "Epoch 30/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 16.8426 - val_loss: 20.5942\n",
      "Epoch 31/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 17.0890 - val_loss: 19.1404\n",
      "Epoch 32/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 17.2762 - val_loss: 18.0903\n",
      "Epoch 33/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 16.7236 - val_loss: 17.7312\n",
      "Epoch 34/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 15.4040 - val_loss: 17.7162\n",
      "Epoch 35/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 14.0580 - val_loss: 18.5248\n",
      "Epoch 36/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 13.9305 - val_loss: 19.5309\n",
      "Epoch 37/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 13.6559 - val_loss: 18.5164\n",
      "Epoch 38/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 13.3649 - val_loss: 15.7222\n",
      "Epoch 39/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 12.6235 - val_loss: 15.3107\n",
      "Epoch 40/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 11.9938 - val_loss: 15.4837\n",
      "Epoch 41/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 11.5465 - val_loss: 15.6530\n",
      "Epoch 42/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 11.4051 - val_loss: 16.4786\n",
      "Epoch 43/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 11.0625 - val_loss: 15.5001\n",
      "Epoch 44/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 10.5564 - val_loss: 15.9407\n",
      "Epoch 45/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 10.5120 - val_loss: 16.6132\n",
      "Epoch 46/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 10.3148 - val_loss: 15.6981\n",
      "Epoch 47/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 9.9465 - val_loss: 15.0557\n",
      "Epoch 48/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 9.7007 - val_loss: 13.9337\n",
      "Epoch 49/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 9.4434 - val_loss: 12.9393\n",
      "Epoch 50/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 9.1181 - val_loss: 13.4396\n",
      "Epoch 51/200\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 8.8516 - val_loss: 13.9765\n",
      "Epoch 52/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8.5920 - val_loss: 13.9149\n",
      "Epoch 53/200\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 8.5879 - val_loss: 13.2486\n",
      "Epoch 54/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8.2663 - val_loss: 11.5950\n",
      "Epoch 55/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 8.2637 - val_loss: 8.6802\n",
      "Epoch 56/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 8.0704 - val_loss: 8.2413\n",
      "Epoch 57/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 7.9677 - val_loss: 8.0453\n",
      "Epoch 58/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 7.7297 - val_loss: 7.3561\n",
      "Epoch 59/200\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 7.3036 - val_loss: 7.9965\n",
      "Epoch 60/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 7.0561 - val_loss: 9.6789\n",
      "Epoch 61/200\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 7.1586 - val_loss: 9.9333\n",
      "Epoch 62/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 7.2470 - val_loss: 9.6396\n",
      "Epoch 63/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 7.0731 - val_loss: 9.2726\n",
      "Epoch 64/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 6.8399 - val_loss: 10.3484\n",
      "Epoch 65/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 6.4919 - val_loss: 8.8245\n",
      "Epoch 66/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 6.5553 - val_loss: 8.2006\n",
      "Epoch 67/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 6.3845 - val_loss: 8.7053\n",
      "Epoch 68/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 6.2244 - val_loss: 9.4759\n",
      "Epoch 69/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 6.1286 - val_loss: 9.6864\n",
      "Epoch 70/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 6.0464 - val_loss: 9.5575\n",
      "Epoch 71/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 5.9089 - val_loss: 9.0761\n",
      "Epoch 72/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 5.8068 - val_loss: 8.5513\n",
      "Epoch 73/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 5.7646 - val_loss: 8.5434\n",
      "Epoch 74/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 5.6838 - val_loss: 8.1948\n",
      "Epoch 75/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 5.6007 - val_loss: 7.8949\n",
      "Epoch 76/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 5.5295 - val_loss: 7.3553\n",
      "Epoch 77/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 5.4639 - val_loss: 5.4307\n",
      "Epoch 78/200\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 5.4666 - val_loss: 5.3333\n",
      "Epoch 79/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 5.4047 - val_loss: 5.3279\n",
      "Epoch 80/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 5.3999 - val_loss: 5.8317\n",
      "Epoch 81/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 5.2582 - val_loss: 6.4831\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 15ms/step - loss: 5.0327 - val_loss: 7.5944\n",
      "Epoch 83/200\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 5.1853 - val_loss: 7.5227\n",
      "Epoch 84/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 4.9609 - val_loss: 5.9732\n",
      "Epoch 85/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 4.9705 - val_loss: 5.3816\n",
      "Epoch 86/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4.9541 - val_loss: 5.6447\n",
      "Epoch 87/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 4.8471 - val_loss: 6.0064\n",
      "Epoch 88/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 4.8850 - val_loss: 6.6735\n",
      "Epoch 89/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 4.7785 - val_loss: 6.0525\n",
      "Epoch 90/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 4.6618 - val_loss: 5.7460\n",
      "Epoch 91/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 4.6697 - val_loss: 5.0394\n",
      "Epoch 92/200\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 4.6045 - val_loss: 5.2451\n",
      "Epoch 93/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 4.5580 - val_loss: 4.9822\n",
      "Epoch 94/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 4.5555 - val_loss: 5.1636\n",
      "Epoch 95/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4.4797 - val_loss: 5.3652\n",
      "Epoch 96/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 4.4039 - val_loss: 4.6680\n",
      "Epoch 97/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4.4112 - val_loss: 4.4214\n",
      "Epoch 98/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 4.3340 - val_loss: 4.6378\n",
      "Epoch 99/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4.2965 - val_loss: 4.7028\n",
      "Epoch 100/200\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 4.4937 - val_loss: 4.4408\n",
      "Epoch 101/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 4.3573 - val_loss: 5.0530\n",
      "Epoch 102/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4.3349 - val_loss: 5.1484\n",
      "Epoch 103/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4.2115 - val_loss: 5.0373\n",
      "Epoch 104/200\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 4.1267 - val_loss: 4.9277\n",
      "Epoch 105/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 4.0955 - val_loss: 4.8678\n",
      "Epoch 106/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 4.0652 - val_loss: 4.9212\n",
      "Epoch 107/200\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 4.0445 - val_loss: 5.7767\n",
      "Epoch 108/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 4.1183 - val_loss: 5.3343\n",
      "Epoch 109/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.9437 - val_loss: 4.1059\n",
      "Epoch 110/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.9951 - val_loss: 3.8161\n",
      "Epoch 111/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.9054 - val_loss: 4.2515\n",
      "Epoch 112/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.8839 - val_loss: 4.2710\n",
      "Epoch 113/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.8270 - val_loss: 3.8802\n",
      "Epoch 114/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.8243 - val_loss: 3.8641\n",
      "Epoch 115/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.8262 - val_loss: 3.7515\n",
      "Epoch 116/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.8284 - val_loss: 3.9755\n",
      "Epoch 117/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.7364 - val_loss: 4.2675\n",
      "Epoch 118/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.7158 - val_loss: 4.3209\n",
      "Epoch 119/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.7999 - val_loss: 3.8925\n",
      "Epoch 120/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.6541 - val_loss: 4.3414\n",
      "Epoch 121/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.6309 - val_loss: 4.2311\n",
      "Epoch 122/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.6120 - val_loss: 4.0155\n",
      "Epoch 123/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.5811 - val_loss: 3.8728\n",
      "Epoch 124/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.5530 - val_loss: 3.5610\n",
      "Epoch 125/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.5843 - val_loss: 3.8088\n",
      "Epoch 126/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.4740 - val_loss: 3.2093\n",
      "Epoch 127/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.5375 - val_loss: 2.7126\n",
      "Epoch 128/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.6091 - val_loss: 2.6490\n",
      "Epoch 129/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.4679 - val_loss: 2.7798\n",
      "Epoch 130/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.3738 - val_loss: 3.0959\n",
      "Epoch 131/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.3859 - val_loss: 3.0194\n",
      "Epoch 132/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.3772 - val_loss: 2.8789\n",
      "Epoch 133/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.3254 - val_loss: 3.1239\n",
      "Epoch 134/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.2732 - val_loss: 3.5046\n",
      "Epoch 135/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 3.2949 - val_loss: 3.8754\n",
      "Epoch 136/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 3.2841 - val_loss: 3.1921\n",
      "Epoch 137/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.2910 - val_loss: 2.9883\n",
      "Epoch 138/200\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 3.3407 - val_loss: 2.1117\n",
      "Epoch 139/200\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 3.3689 - val_loss: 2.0552\n",
      "Epoch 140/200\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 3.5447 - val_loss: 2.3085\n",
      "Epoch 141/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.2000 - val_loss: 1.7611\n",
      "Epoch 142/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.3011 - val_loss: 1.7438\n",
      "Epoch 143/200\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 3.1933 - val_loss: 2.1020\n",
      "Epoch 144/200\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 3.2460 - val_loss: 2.3622\n",
      "Epoch 145/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.1466 - val_loss: 2.3337\n",
      "Epoch 146/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.1072 - val_loss: 2.1690\n",
      "Epoch 147/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.1348 - val_loss: 2.2469\n",
      "Epoch 148/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.0674 - val_loss: 1.9395\n",
      "Epoch 149/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.0733 - val_loss: 2.6201\n",
      "Epoch 150/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.1330 - val_loss: 3.0242\n",
      "Epoch 151/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.0057 - val_loss: 2.2934\n",
      "Epoch 152/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.9233 - val_loss: 3.0995\n",
      "Epoch 153/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.1911 - val_loss: 4.2955\n",
      "Epoch 154/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.2775 - val_loss: 3.5557\n",
      "Epoch 155/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.9788 - val_loss: 2.7443\n",
      "Epoch 156/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.8870 - val_loss: 2.9088\n",
      "Epoch 157/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.0394 - val_loss: 2.6171\n",
      "Epoch 158/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 3.1091 - val_loss: 3.3576\n",
      "Epoch 159/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.2051 - val_loss: 2.1495\n",
      "Epoch 160/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.8678 - val_loss: 2.2869\n",
      "Epoch 161/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.7884 - val_loss: 2.3987\n",
      "Epoch 162/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.6909 - val_loss: 3.1813\n",
      "Epoch 163/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.0678 - val_loss: 3.5940\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 18ms/step - loss: 2.9624 - val_loss: 2.5293\n",
      "Epoch 165/200\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 2.8062 - val_loss: 2.3112\n",
      "Epoch 166/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 2.7413 - val_loss: 2.3519\n",
      "Epoch 167/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.6845 - val_loss: 2.5084\n",
      "Epoch 168/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.6765 - val_loss: 2.8542\n",
      "Epoch 169/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.8895 - val_loss: 2.7262\n",
      "Epoch 170/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.0418 - val_loss: 2.2003\n",
      "Epoch 171/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 2.8006 - val_loss: 2.0635\n",
      "Epoch 172/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.6808 - val_loss: 2.0476\n",
      "Epoch 173/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 2.7671 - val_loss: 2.0300\n",
      "Epoch 174/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.6163 - val_loss: 2.7793\n",
      "Epoch 175/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.7936 - val_loss: 2.9660\n",
      "Epoch 176/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.8010 - val_loss: 2.6622\n",
      "Epoch 177/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.7621 - val_loss: 2.6659\n",
      "Epoch 178/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.5757 - val_loss: 2.2614\n",
      "Epoch 179/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.5874 - val_loss: 2.0163\n",
      "Epoch 180/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.6196 - val_loss: 2.1290\n",
      "Epoch 181/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.5770 - val_loss: 2.2263\n",
      "Epoch 182/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.6439 - val_loss: 1.8061\n",
      "Epoch 183/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 2.5950 - val_loss: 1.9781\n",
      "Epoch 184/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.5058 - val_loss: 2.1615\n",
      "Epoch 185/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.5163 - val_loss: 2.1524\n",
      "Epoch 186/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.5401 - val_loss: 1.7437\n",
      "Epoch 187/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.5367 - val_loss: 1.8762\n",
      "Epoch 188/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.4394 - val_loss: 2.2137\n",
      "Epoch 189/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.6601 - val_loss: 3.1023\n",
      "Epoch 190/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.6073 - val_loss: 2.5770\n",
      "Epoch 191/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.5344 - val_loss: 2.1671\n",
      "Epoch 192/200\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 2.4960 - val_loss: 2.1939\n",
      "Epoch 193/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.4311 - val_loss: 2.0883\n",
      "Epoch 194/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 2.4276 - val_loss: 1.6965\n",
      "Epoch 195/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.3991 - val_loss: 1.6526\n",
      "Epoch 196/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.3541 - val_loss: 1.8377\n",
      "Epoch 197/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.3861 - val_loss: 2.2520\n",
      "Epoch 198/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.4438 - val_loss: 2.0260\n",
      "Epoch 199/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.3664 - val_loss: 1.7558\n",
      "Epoch 200/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.3683 - val_loss: 1.3471\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Mean Squared Error on Entire Data (Stacking with Random Forest): 0.33\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load your dataset (replace 'your_data.csv' with your actual file)\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Assuming 'df' is your DataFrame with the provided data\n",
    "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
    "\n",
    "# Select relevant features for training\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df[features]\n",
    "y = df['Yield']\n",
    "\n",
    "# Normalize numerical features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X[features] = scaler.fit_transform(X[features])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.1, random_state=42)\n",
    "\n",
    "# Build and train the first neural network (base model 1)\n",
    "model_nn_base_1 = Sequential()\n",
    "model_nn_base_1.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(10, activation='relu'))\n",
    "model_nn_base_1.add(Dense(1, activation='linear'))\n",
    "model_nn_base_1.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "model_nn_base_1.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the first base neural network model\n",
    "y_pred_nn_base_1 = model_nn_base_1.predict(X)\n",
    "\n",
    "# Build and train the second neural network (base model 2)\n",
    "model_nn_base_2 = Sequential()\n",
    "model_nn_base_2.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(10, activation='relu'))\n",
    "model_nn_base_2.add(Dense(5, activation='relu'))\n",
    "model_nn_base_2.add(Dense(1, activation='linear'))\n",
    "model_nn_base_2.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "model_nn_base_2.fit(X, y, epochs=200, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the second base neural network model\n",
    "y_pred_nn_base_2 = model_nn_base_2.predict(X)\n",
    "\n",
    "# Combine predictions from both base models\n",
    "X_meta_train = np.concatenate((y_pred_nn_base_1, y_pred_nn_base_2), axis=1)\n",
    "\n",
    "# Build and train a Random Forest Regressor as a meta-learner\n",
    "model_rf_meta = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_rf_meta.fit(X_meta_train, y)\n",
    "\n",
    "# Make predictions using the base models and meta-learner on the entire dataset\n",
    "X_meta_all = np.concatenate((model_nn_base_1.predict(X), model_nn_base_2.predict(X)), axis=1)\n",
    "y_pred_stacking_rf_all = model_rf_meta.predict(X_meta_all)\n",
    "\n",
    "# Evaluate the stacking model with Random Forest as meta-learner on the entire dataset\n",
    "mse_stacking_rf_all = mean_squared_error(y, y_pred_stacking_rf_all)\n",
    "print(f\"Mean Squared Error on Entire Data (Stacking with Random Forest): {mse_stacking_rf_all:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "48f0f209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_11788\\2614906700.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[features] = scaler.fit_transform(X[features])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 169ms/step - loss: 620.8599 - val_loss: 543.5037\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 615.5701 - val_loss: 539.8961\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 609.0839 - val_loss: 535.4968\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 601.7123 - val_loss: 530.1943\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 594.0231 - val_loss: 524.1860\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 585.8845 - val_loss: 518.1154\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 577.0186 - val_loss: 511.9754\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 567.6624 - val_loss: 505.3867\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 557.7410 - val_loss: 498.2506\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 546.9283 - val_loss: 490.5186\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 535.7958 - val_loss: 481.6625\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 523.8928 - val_loss: 472.5045\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 511.0652 - val_loss: 462.8456\n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 497.9337 - val_loss: 452.4117\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 483.6526 - val_loss: 441.2822\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 468.7122 - val_loss: 429.5235\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 452.1855 - val_loss: 417.1089\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 435.1069 - val_loss: 403.9420\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 417.3472 - val_loss: 390.4248\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 399.4456 - val_loss: 376.3848\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 380.0249 - val_loss: 361.4714\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 360.5637 - val_loss: 346.0853\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 340.4239 - val_loss: 330.2480\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 319.7682 - val_loss: 314.0565\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 299.2629 - val_loss: 297.4733\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 278.4740 - val_loss: 280.7042\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 257.9525 - val_loss: 263.8900\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 236.8808 - val_loss: 247.0340\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 216.7026 - val_loss: 230.4346\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 197.4403 - val_loss: 214.1620\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 178.6115 - val_loss: 198.1924\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 160.1970 - val_loss: 182.9651\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 143.3732 - val_loss: 168.5363\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 127.2881 - val_loss: 155.2268\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 113.4503 - val_loss: 143.1464\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 99.3008 - val_loss: 132.1949\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 87.7500 - val_loss: 122.4119\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 77.6508 - val_loss: 113.9069\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 68.5357 - val_loss: 106.8287\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 60.2119 - val_loss: 101.1983\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 53.8973 - val_loss: 96.5772\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 48.6731 - val_loss: 92.8091\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 44.1872 - val_loss: 89.6967\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 40.4380 - val_loss: 87.3207\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 37.4171 - val_loss: 85.5064\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 35.2910 - val_loss: 84.0459\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 33.1338 - val_loss: 82.6215\n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 31.4313 - val_loss: 81.4519\n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 30.3380 - val_loss: 80.3361\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 29.3456 - val_loss: 79.0872\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 28.3853 - val_loss: 77.8760\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 27.6430 - val_loss: 76.6270\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 26.9827 - val_loss: 75.3616\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 26.3237 - val_loss: 73.8608\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 25.8147 - val_loss: 72.3602\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 25.2327 - val_loss: 70.8171\n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 24.7651 - val_loss: 69.3197\n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 24.3372 - val_loss: 67.8072\n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 23.8758 - val_loss: 66.3371\n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 23.4283 - val_loss: 64.8079\n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 23.0211 - val_loss: 63.2392\n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 22.6230 - val_loss: 61.7220\n",
      "Epoch 63/200\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 22.2614 - val_loss: 60.2163\n",
      "Epoch 64/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 21.8836 - val_loss: 58.6032\n",
      "Epoch 65/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 21.4581 - val_loss: 57.2518\n",
      "Epoch 66/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 21.0787 - val_loss: 55.8558\n",
      "Epoch 67/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 20.7134 - val_loss: 54.5672\n",
      "Epoch 68/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 20.4049 - val_loss: 53.3870\n",
      "Epoch 69/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 20.0041 - val_loss: 52.2316\n",
      "Epoch 70/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 19.7249 - val_loss: 51.1624\n",
      "Epoch 71/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 19.3605 - val_loss: 50.0704\n",
      "Epoch 72/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 19.0480 - val_loss: 49.0368\n",
      "Epoch 73/200\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 18.7077 - val_loss: 48.1507\n",
      "Epoch 74/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 18.4018 - val_loss: 47.1652\n",
      "Epoch 75/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 18.1030 - val_loss: 46.2520\n",
      "Epoch 76/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 17.7831 - val_loss: 45.3400\n",
      "Epoch 77/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 17.5099 - val_loss: 44.4791\n",
      "Epoch 78/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 17.1703 - val_loss: 43.7363\n",
      "Epoch 79/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 16.9181 - val_loss: 42.9709\n",
      "Epoch 80/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 16.6054 - val_loss: 42.2176\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 32ms/step - loss: 16.3514 - val_loss: 41.4523\n",
      "Epoch 82/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 16.0780 - val_loss: 40.7221\n",
      "Epoch 83/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 15.7866 - val_loss: 39.9954\n",
      "Epoch 84/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 15.5385 - val_loss: 39.3539\n",
      "Epoch 85/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 15.2695 - val_loss: 38.6207\n",
      "Epoch 86/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 15.0218 - val_loss: 38.0159\n",
      "Epoch 87/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 14.7931 - val_loss: 37.4124\n",
      "Epoch 88/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 14.5314 - val_loss: 36.9081\n",
      "Epoch 89/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 14.2839 - val_loss: 36.3177\n",
      "Epoch 90/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 14.0642 - val_loss: 35.7557\n",
      "Epoch 91/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 13.8295 - val_loss: 35.2635\n",
      "Epoch 92/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 13.6413 - val_loss: 34.7161\n",
      "Epoch 93/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 13.3951 - val_loss: 34.2681\n",
      "Epoch 94/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 13.1815 - val_loss: 33.7653\n",
      "Epoch 95/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 13.0217 - val_loss: 33.2998\n",
      "Epoch 96/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 12.7769 - val_loss: 32.8445\n",
      "Epoch 97/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 12.5864 - val_loss: 32.4110\n",
      "Epoch 98/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 12.3873 - val_loss: 31.9449\n",
      "Epoch 99/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 12.1955 - val_loss: 31.4958\n",
      "Epoch 100/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 12.0139 - val_loss: 31.0836\n",
      "Epoch 101/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 11.8488 - val_loss: 30.7013\n",
      "Epoch 102/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 11.6482 - val_loss: 30.2951\n",
      "Epoch 103/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 11.5067 - val_loss: 29.9009\n",
      "Epoch 104/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 11.3367 - val_loss: 29.6232\n",
      "Epoch 105/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 11.1874 - val_loss: 29.3497\n",
      "Epoch 106/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 10.9996 - val_loss: 29.0750\n",
      "Epoch 107/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 10.8725 - val_loss: 28.7646\n",
      "Epoch 108/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 10.7188 - val_loss: 28.4803\n",
      "Epoch 109/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 10.5690 - val_loss: 28.3029\n",
      "Epoch 110/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 10.4319 - val_loss: 28.1568\n",
      "Epoch 111/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 10.2822 - val_loss: 27.8936\n",
      "Epoch 112/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 10.1439 - val_loss: 27.5858\n",
      "Epoch 113/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 9.9975 - val_loss: 27.3978\n",
      "Epoch 114/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 9.8753 - val_loss: 27.1518\n",
      "Epoch 115/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 9.7543 - val_loss: 26.9031\n",
      "Epoch 116/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 9.6581 - val_loss: 26.6281\n",
      "Epoch 117/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 9.5147 - val_loss: 26.4926\n",
      "Epoch 118/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 9.4126 - val_loss: 26.4389\n",
      "Epoch 119/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 9.2628 - val_loss: 26.3364\n",
      "Epoch 120/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 9.1659 - val_loss: 26.1487\n",
      "Epoch 121/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 9.0526 - val_loss: 26.0742\n",
      "Epoch 122/200\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 8.9459 - val_loss: 25.9975\n",
      "Epoch 123/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 8.8452 - val_loss: 25.9047\n",
      "Epoch 124/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 8.7255 - val_loss: 25.7663\n",
      "Epoch 125/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 8.6443 - val_loss: 25.6487\n",
      "Epoch 126/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 8.5286 - val_loss: 25.4006\n",
      "Epoch 127/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 8.4459 - val_loss: 25.2530\n",
      "Epoch 128/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 8.3477 - val_loss: 25.0932\n",
      "Epoch 129/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 8.2759 - val_loss: 24.8513\n",
      "Epoch 130/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 8.1710 - val_loss: 24.6795\n",
      "Epoch 131/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 8.0796 - val_loss: 24.6060\n",
      "Epoch 132/200\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 7.9887 - val_loss: 24.4853\n",
      "Epoch 133/200\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 7.9065 - val_loss: 24.4146\n",
      "Epoch 134/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 7.8401 - val_loss: 24.4875\n",
      "Epoch 135/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 7.7479 - val_loss: 24.4193\n",
      "Epoch 136/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 7.6902 - val_loss: 24.4211\n",
      "Epoch 137/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 7.6029 - val_loss: 24.1634\n",
      "Epoch 138/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 7.5447 - val_loss: 24.1427\n",
      "Epoch 139/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 7.4375 - val_loss: 23.9495\n",
      "Epoch 140/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 7.3689 - val_loss: 23.7215\n",
      "Epoch 141/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 7.3314 - val_loss: 23.4410\n",
      "Epoch 142/200\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 7.2371 - val_loss: 23.3643\n",
      "Epoch 143/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 7.1862 - val_loss: 23.4009\n",
      "Epoch 144/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 7.0944 - val_loss: 23.2962\n",
      "Epoch 145/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 7.0367 - val_loss: 23.2241\n",
      "Epoch 146/200\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 6.9741 - val_loss: 23.0631\n",
      "Epoch 147/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 6.9146 - val_loss: 23.0181\n",
      "Epoch 148/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 6.8655 - val_loss: 23.0715\n",
      "Epoch 149/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 6.7967 - val_loss: 22.8563\n",
      "Epoch 150/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 6.7298 - val_loss: 22.8393\n",
      "Epoch 151/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 6.6934 - val_loss: 22.8168\n",
      "Epoch 152/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 6.6151 - val_loss: 22.6777\n",
      "Epoch 153/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 6.5592 - val_loss: 22.6349\n",
      "Epoch 154/200\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 6.5119 - val_loss: 22.4791\n",
      "Epoch 155/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 6.4479 - val_loss: 22.5133\n",
      "Epoch 156/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 6.3950 - val_loss: 22.4593\n",
      "Epoch 157/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 6.3495 - val_loss: 22.4831\n",
      "Epoch 158/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 6.2899 - val_loss: 22.4407\n",
      "Epoch 159/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 6.2530 - val_loss: 22.3309\n",
      "Epoch 160/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 6.1922 - val_loss: 22.2737\n",
      "Epoch 161/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 6.1422 - val_loss: 22.3002\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 48ms/step - loss: 6.1002 - val_loss: 22.3924\n",
      "Epoch 163/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 6.0561 - val_loss: 22.2763\n",
      "Epoch 164/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 6.0000 - val_loss: 22.1918\n",
      "Epoch 165/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 5.9631 - val_loss: 22.0370\n",
      "Epoch 166/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 5.9132 - val_loss: 22.0336\n",
      "Epoch 167/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 5.8795 - val_loss: 22.0495\n",
      "Epoch 168/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 5.8301 - val_loss: 21.9432\n",
      "Epoch 169/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 5.7881 - val_loss: 21.8852\n",
      "Epoch 170/200\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 5.7408 - val_loss: 21.8255\n",
      "Epoch 171/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 5.7044 - val_loss: 21.7095\n",
      "Epoch 172/200\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 5.6807 - val_loss: 21.4070\n",
      "Epoch 173/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.6352 - val_loss: 21.2017\n",
      "Epoch 174/200\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 5.6004 - val_loss: 21.1833\n",
      "Epoch 175/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.5723 - val_loss: 21.0037\n",
      "Epoch 176/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 5.5190 - val_loss: 21.0788\n",
      "Epoch 177/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 5.4773 - val_loss: 21.0893\n",
      "Epoch 178/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 5.4298 - val_loss: 21.1533\n",
      "Epoch 179/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 5.4230 - val_loss: 21.2211\n",
      "Epoch 180/200\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 5.3765 - val_loss: 21.1441\n",
      "Epoch 181/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 5.3341 - val_loss: 21.0435\n",
      "Epoch 182/200\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 5.2928 - val_loss: 20.9401\n",
      "Epoch 183/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 5.2629 - val_loss: 20.9505\n",
      "Epoch 184/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 5.2356 - val_loss: 20.7658\n",
      "Epoch 185/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.1964 - val_loss: 20.7426\n",
      "Epoch 186/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 5.1726 - val_loss: 20.7201\n",
      "Epoch 187/200\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 5.1609 - val_loss: 20.5015\n",
      "Epoch 188/200\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 5.1026 - val_loss: 20.4893\n",
      "Epoch 189/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 5.0688 - val_loss: 20.6062\n",
      "Epoch 190/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 5.0354 - val_loss: 20.5358\n",
      "Epoch 191/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.0113 - val_loss: 20.6189\n",
      "Epoch 192/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 4.9915 - val_loss: 20.4384\n",
      "Epoch 193/200\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.9577 - val_loss: 20.5405\n",
      "Epoch 194/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 4.9209 - val_loss: 20.4943\n",
      "Epoch 195/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 4.8979 - val_loss: 20.3277\n",
      "Epoch 196/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 4.8737 - val_loss: 20.1559\n",
      "Epoch 197/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 4.8381 - val_loss: 20.1021\n",
      "Epoch 198/200\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 4.8104 - val_loss: 20.0199\n",
      "Epoch 199/200\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 4.7915 - val_loss: 19.9102\n",
      "Epoch 200/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 4.7637 - val_loss: 20.1219\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 70ms/step - loss: 623.0128 - val_loss: 544.1751\n",
      "Epoch 2/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 614.5001 - val_loss: 536.9427\n",
      "Epoch 3/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 608.0405 - val_loss: 532.2336\n",
      "Epoch 4/200\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 602.1055 - val_loss: 525.9033\n",
      "Epoch 5/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 594.9382 - val_loss: 517.4830\n",
      "Epoch 6/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 585.9270 - val_loss: 508.1447\n",
      "Epoch 7/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 574.7969 - val_loss: 497.0207\n",
      "Epoch 8/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 560.8438 - val_loss: 482.6703\n",
      "Epoch 9/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 543.7802 - val_loss: 464.5861\n",
      "Epoch 10/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 521.6884 - val_loss: 440.8087\n",
      "Epoch 11/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 494.9694 - val_loss: 412.2722\n",
      "Epoch 12/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 462.9157 - val_loss: 379.1707\n",
      "Epoch 13/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 424.8769 - val_loss: 338.3344\n",
      "Epoch 14/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 380.3845 - val_loss: 294.1170\n",
      "Epoch 15/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 331.0637 - val_loss: 247.0927\n",
      "Epoch 16/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 278.2502 - val_loss: 198.5265\n",
      "Epoch 17/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 224.4915 - val_loss: 152.9857\n",
      "Epoch 18/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 174.3539 - val_loss: 113.8577\n",
      "Epoch 19/200\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 128.6341 - val_loss: 84.5059\n",
      "Epoch 20/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 90.6683 - val_loss: 66.2680\n",
      "Epoch 21/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 60.7476 - val_loss: 56.6533\n",
      "Epoch 22/200\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 41.3651 - val_loss: 52.6329\n",
      "Epoch 23/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 32.4899 - val_loss: 51.2688\n",
      "Epoch 24/200\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 26.4851 - val_loss: 48.0311\n",
      "Epoch 25/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 22.5063 - val_loss: 44.0898\n",
      "Epoch 26/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 20.0464 - val_loss: 39.5871\n",
      "Epoch 27/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 17.9655 - val_loss: 37.0775\n",
      "Epoch 28/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 16.8911 - val_loss: 35.5386\n",
      "Epoch 29/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 16.1306 - val_loss: 34.1861\n",
      "Epoch 30/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 15.6938 - val_loss: 33.3607\n",
      "Epoch 31/200\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 15.0617 - val_loss: 32.7381\n",
      "Epoch 32/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 14.7156 - val_loss: 32.0229\n",
      "Epoch 33/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 14.1956 - val_loss: 30.2949\n",
      "Epoch 34/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 13.4920 - val_loss: 28.1452\n",
      "Epoch 35/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 13.1824 - val_loss: 25.8494\n",
      "Epoch 36/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 12.7706 - val_loss: 24.8211\n",
      "Epoch 37/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 12.3996 - val_loss: 24.5190\n",
      "Epoch 38/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 12.0076 - val_loss: 25.2660\n",
      "Epoch 39/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 11.8895 - val_loss: 25.4125\n",
      "Epoch 40/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 11.5944 - val_loss: 24.6086\n",
      "Epoch 41/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 11.3849 - val_loss: 23.3287\n",
      "Epoch 42/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 11.0116 - val_loss: 22.8461\n",
      "Epoch 43/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 10.9126 - val_loss: 22.4452\n",
      "Epoch 44/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 10.5838 - val_loss: 21.9181\n",
      "Epoch 45/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 10.4737 - val_loss: 22.4209\n",
      "Epoch 46/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 10.3330 - val_loss: 22.1988\n",
      "Epoch 47/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 10.0157 - val_loss: 21.0258\n",
      "Epoch 48/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 9.8949 - val_loss: 19.6218\n",
      "Epoch 49/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 9.6975 - val_loss: 18.8860\n",
      "Epoch 50/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 9.4865 - val_loss: 18.6785\n",
      "Epoch 51/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 9.3084 - val_loss: 18.6663\n",
      "Epoch 52/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 9.1627 - val_loss: 18.8754\n",
      "Epoch 53/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 9.0175 - val_loss: 18.5033\n",
      "Epoch 54/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8.8844 - val_loss: 18.0595\n",
      "Epoch 55/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 8.8111 - val_loss: 17.2619\n",
      "Epoch 56/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 8.7232 - val_loss: 18.3316\n",
      "Epoch 57/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 8.5848 - val_loss: 17.9602\n",
      "Epoch 58/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 8.3115 - val_loss: 17.8371\n",
      "Epoch 59/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 8.1774 - val_loss: 17.0353\n",
      "Epoch 60/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 8.0697 - val_loss: 16.5901\n",
      "Epoch 61/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 8.1735 - val_loss: 16.4365\n",
      "Epoch 62/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 8.1915 - val_loss: 15.8739\n",
      "Epoch 63/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 7.9448 - val_loss: 15.6798\n",
      "Epoch 64/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 7.6790 - val_loss: 15.0339\n",
      "Epoch 65/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 7.6161 - val_loss: 13.8664\n",
      "Epoch 66/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 7.4710 - val_loss: 13.2440\n",
      "Epoch 67/200\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 7.3670 - val_loss: 13.0943\n",
      "Epoch 68/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 7.0285 - val_loss: 14.5725\n",
      "Epoch 69/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 7.0203 - val_loss: 15.2349\n",
      "Epoch 70/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 7.0544 - val_loss: 14.0638\n",
      "Epoch 71/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 6.8322 - val_loss: 13.4898\n",
      "Epoch 72/200\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 6.7206 - val_loss: 12.5651\n",
      "Epoch 73/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 6.5865 - val_loss: 12.0733\n",
      "Epoch 74/200\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 6.5036 - val_loss: 11.5574\n",
      "Epoch 75/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 6.5097 - val_loss: 11.0754\n",
      "Epoch 76/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 6.4526 - val_loss: 11.1584\n",
      "Epoch 77/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 6.2606 - val_loss: 10.8540\n",
      "Epoch 78/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 6.2199 - val_loss: 10.8279\n",
      "Epoch 79/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 6.0076 - val_loss: 11.6494\n",
      "Epoch 80/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 6.0032 - val_loss: 11.9254\n",
      "Epoch 81/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 6.0234 - val_loss: 10.9540\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 17ms/step - loss: 5.9059 - val_loss: 10.2704\n",
      "Epoch 83/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 5.8052 - val_loss: 10.7207\n",
      "Epoch 84/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 5.7478 - val_loss: 11.0496\n",
      "Epoch 85/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 5.6926 - val_loss: 10.9319\n",
      "Epoch 86/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 5.6432 - val_loss: 11.0994\n",
      "Epoch 87/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 5.8612 - val_loss: 12.1918\n",
      "Epoch 88/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 5.5034 - val_loss: 10.2520\n",
      "Epoch 89/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 5.4268 - val_loss: 9.4695\n",
      "Epoch 90/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 5.4247 - val_loss: 9.9794\n",
      "Epoch 91/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 5.3150 - val_loss: 9.2609\n",
      "Epoch 92/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 5.1923 - val_loss: 9.0056\n",
      "Epoch 93/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 5.1947 - val_loss: 8.9371\n",
      "Epoch 94/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 5.1027 - val_loss: 8.0755\n",
      "Epoch 95/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 5.0878 - val_loss: 8.0634\n",
      "Epoch 96/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 5.0812 - val_loss: 8.3423\n",
      "Epoch 97/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 5.0307 - val_loss: 8.2869\n",
      "Epoch 98/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 5.0014 - val_loss: 9.9904\n",
      "Epoch 99/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 5.0823 - val_loss: 9.8222\n",
      "Epoch 100/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 4.9807 - val_loss: 8.2743\n",
      "Epoch 101/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 4.7444 - val_loss: 7.8085\n",
      "Epoch 102/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4.6945 - val_loss: 7.6665\n",
      "Epoch 103/200\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 4.6616 - val_loss: 8.0021\n",
      "Epoch 104/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4.5809 - val_loss: 7.6828\n",
      "Epoch 105/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4.5425 - val_loss: 7.5850\n",
      "Epoch 106/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 4.5383 - val_loss: 7.7265\n",
      "Epoch 107/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 4.4764 - val_loss: 7.6881\n",
      "Epoch 108/200\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 4.4704 - val_loss: 8.1290\n",
      "Epoch 109/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 4.4760 - val_loss: 7.7744\n",
      "Epoch 110/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 4.3574 - val_loss: 6.6868\n",
      "Epoch 111/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4.3530 - val_loss: 6.3578\n",
      "Epoch 112/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4.2871 - val_loss: 6.6957\n",
      "Epoch 113/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4.2550 - val_loss: 7.8983\n",
      "Epoch 114/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 4.2846 - val_loss: 7.4887\n",
      "Epoch 115/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4.1704 - val_loss: 6.4677\n",
      "Epoch 116/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4.0727 - val_loss: 6.3096\n",
      "Epoch 117/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4.0423 - val_loss: 6.1590\n",
      "Epoch 118/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 4.0283 - val_loss: 6.4496\n",
      "Epoch 119/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 4.0693 - val_loss: 6.5340\n",
      "Epoch 120/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 4.0003 - val_loss: 5.9282\n",
      "Epoch 121/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 3.9021 - val_loss: 5.2633\n",
      "Epoch 122/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.9769 - val_loss: 4.7415\n",
      "Epoch 123/200\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 4.0410 - val_loss: 4.6283\n",
      "Epoch 124/200\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 3.9473 - val_loss: 5.3422\n",
      "Epoch 125/200\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 3.8406 - val_loss: 5.4566\n",
      "Epoch 126/200\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 3.7843 - val_loss: 5.0541\n",
      "Epoch 127/200\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 3.9928 - val_loss: 4.0615\n",
      "Epoch 128/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.9191 - val_loss: 4.3989\n",
      "Epoch 129/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.7408 - val_loss: 4.7055\n",
      "Epoch 130/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.6845 - val_loss: 6.1513\n",
      "Epoch 131/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.8082 - val_loss: 6.1548\n",
      "Epoch 132/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.7401 - val_loss: 5.9887\n",
      "Epoch 133/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.6669 - val_loss: 4.7941\n",
      "Epoch 134/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 3.5747 - val_loss: 4.5035\n",
      "Epoch 135/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 3.5437 - val_loss: 4.4982\n",
      "Epoch 136/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.4987 - val_loss: 5.0359\n",
      "Epoch 137/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.5215 - val_loss: 4.5845\n",
      "Epoch 138/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.3823 - val_loss: 3.9371\n",
      "Epoch 139/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.4146 - val_loss: 3.9791\n",
      "Epoch 140/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.3701 - val_loss: 4.1783\n",
      "Epoch 141/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 3.2866 - val_loss: 4.5702\n",
      "Epoch 142/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.6166 - val_loss: 5.7766\n",
      "Epoch 143/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.3390 - val_loss: 4.5987\n",
      "Epoch 144/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.5482 - val_loss: 3.2556\n",
      "Epoch 145/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.4995 - val_loss: 4.0620\n",
      "Epoch 146/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.1879 - val_loss: 4.3595\n",
      "Epoch 147/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.2086 - val_loss: 4.4556\n",
      "Epoch 148/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.2950 - val_loss: 5.1439\n",
      "Epoch 149/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.3619 - val_loss: 4.6580\n",
      "Epoch 150/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.2018 - val_loss: 4.1113\n",
      "Epoch 151/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.1635 - val_loss: 3.7624\n",
      "Epoch 152/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.1220 - val_loss: 3.7544\n",
      "Epoch 153/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.0574 - val_loss: 3.8213\n",
      "Epoch 154/200\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 3.0460 - val_loss: 3.9977\n",
      "Epoch 155/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.0344 - val_loss: 3.7160\n",
      "Epoch 156/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.0105 - val_loss: 3.4929\n",
      "Epoch 157/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3.1192 - val_loss: 3.9078\n",
      "Epoch 158/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.0067 - val_loss: 3.5537\n",
      "Epoch 159/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.9513 - val_loss: 3.2867\n",
      "Epoch 160/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.9208 - val_loss: 3.2686\n",
      "Epoch 161/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.8949 - val_loss: 3.3156\n",
      "Epoch 162/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.9110 - val_loss: 3.2345\n",
      "Epoch 163/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 2.8999 - val_loss: 3.4732\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 16ms/step - loss: 2.8673 - val_loss: 3.3287\n",
      "Epoch 165/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.8808 - val_loss: 3.2617\n",
      "Epoch 166/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.8342 - val_loss: 3.2241\n",
      "Epoch 167/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.8233 - val_loss: 4.2417\n",
      "Epoch 168/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.1069 - val_loss: 4.4824\n",
      "Epoch 169/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 3.0359 - val_loss: 3.4755\n",
      "Epoch 170/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.8347 - val_loss: 3.4113\n",
      "Epoch 171/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.7539 - val_loss: 2.9856\n",
      "Epoch 172/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.1102 - val_loss: 1.9669\n",
      "Epoch 173/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.9004 - val_loss: 2.2670\n",
      "Epoch 174/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.8398 - val_loss: 2.8307\n",
      "Epoch 175/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.7457 - val_loss: 2.5627\n",
      "Epoch 176/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 2.7401 - val_loss: 2.4918\n",
      "Epoch 177/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.6374 - val_loss: 2.2460\n",
      "Epoch 178/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.6494 - val_loss: 2.2367\n",
      "Epoch 179/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.6934 - val_loss: 2.5360\n",
      "Epoch 180/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.7195 - val_loss: 2.9574\n",
      "Epoch 181/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.7905 - val_loss: 2.4206\n",
      "Epoch 182/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.5996 - val_loss: 2.4155\n",
      "Epoch 183/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.5830 - val_loss: 2.4830\n",
      "Epoch 184/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.5924 - val_loss: 2.1718\n",
      "Epoch 185/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.5494 - val_loss: 2.5320\n",
      "Epoch 186/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.6782 - val_loss: 2.8122\n",
      "Epoch 187/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.7293 - val_loss: 2.4437\n",
      "Epoch 188/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.4873 - val_loss: 2.0462\n",
      "Epoch 189/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.6211 - val_loss: 1.9768\n",
      "Epoch 190/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.7005 - val_loss: 1.9740\n",
      "Epoch 191/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.5881 - val_loss: 2.1439\n",
      "Epoch 192/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.4868 - val_loss: 2.2667\n",
      "Epoch 193/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.4782 - val_loss: 2.2621\n",
      "Epoch 194/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.4974 - val_loss: 2.0468\n",
      "Epoch 195/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.4503 - val_loss: 2.3184\n",
      "Epoch 196/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 2.4202 - val_loss: 2.0625\n",
      "Epoch 197/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.4106 - val_loss: 2.0590\n",
      "Epoch 198/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.4114 - val_loss: 2.1110\n",
      "Epoch 199/200\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 2.4040 - val_loss: 2.0811\n",
      "Epoch 200/200\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 2.4040 - val_loss: 2.0241\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 10ms/step - loss: 596.9810\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 547.8382\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 497.3374\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 448.5186\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 406.8836\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 364.3003\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 322.3262\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 284.4173\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 246.0895\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 207.7864\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 169.6058\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 132.2676\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 98.8388\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 69.1303\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 44.4113\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 25.7068\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 12.8488\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 5.4838\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.3549\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.9914\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 5.2324\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 5.6216\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 5.2634\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 4.5511\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.9573\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.7397\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 3.4736\n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.4124\n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.4583\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.5473\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.5602\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.6249\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.6239\n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.5178\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.4096\n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.3481\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.4286\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.4676\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.4960\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.5493\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.5911\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.5946\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.4453\n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.3029\n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.4494\n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.5743\n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 3.6226\n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 3.5302\n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.4440\n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 3.3486\n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.3174\n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.3397\n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.3373\n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.3131\n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.3314\n",
      "Epoch 56/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.3184\n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.3197\n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.3082\n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.3725\n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.4689\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.4672\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.3941\n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.3536\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.3040\n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.3147\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.3238\n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.3254\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.3201\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.2931\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.2870\n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 3.3011\n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.2856\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.2882\n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.3276\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.4059\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.3878\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.3178\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.2883\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.2820\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.2429\n",
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.2435\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.3056\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.2668\n",
      "Epoch 84/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.2210\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.2246\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.2325\n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.2255\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.2103\n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.2101\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.2030\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.3087\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.4031\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.3862\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.2981\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.2062\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.1858\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.1751\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1640\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1818\n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1414\n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.1811\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1886\n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1615\n",
      "Epoch 104/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1622\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 3.1400\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1922\n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.2804\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.3202\n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.2175\n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.1323\n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1987\n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.2159\n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.1936\n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1542\n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1290\n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1078\n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1120\n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1112\n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.1060\n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1679\n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1664\n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.2896\n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.2764\n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.2521\n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.2519\n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.1709\n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 3.1100\n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1164\n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.1256\n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.0999\n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1132\n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 3.1836\n",
      "Epoch 133/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.1450\n",
      "Epoch 134/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.0745\n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.0601\n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.0558\n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.0474\n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.0469\n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.0539\n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 3.0635\n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.0431\n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.0812\n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.0370\n",
      "Epoch 144/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.0344\n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.1064\n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.1120\n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.0831\n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.0502\n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.1687\n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1891\n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1493\n",
      "Epoch 152/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.0921\n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.0957\n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.0155\n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.9832\n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.0582\n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1409\n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.1439\n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.0718\n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.0033\n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.9908\n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.9943\n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.9955\n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.9836\n",
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.9754\n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.0119\n",
      "Epoch 167/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.0007\n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.9957\n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.9867\n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.9889\n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.9606\n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 3.0227\n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.9834\n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.9670\n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.9522\n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.9504\n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.9474\n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.9615\n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.9486\n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.9320\n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.9689\n",
      "Epoch 182/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.9810\n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.9288\n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.9209\n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.9290\n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.9291\n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.9316\n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.0154\n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.9646\n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.9318\n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.9139\n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.9459\n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.9931\n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.9911\n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.9416\n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.8923\n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.9009\n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.8621\n",
      "Epoch 199/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.0817\n",
      "Epoch 200/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.2011\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Mean Squared Error on Entire Data (Stacking with Neural Network): 3.19\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load your dataset (replace 'your_data.csv' with your actual file)\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Assuming 'df' is your DataFrame with the provided data\n",
    "# Columns: State, Year, Avg Temp, Avg Humidity, Total Rainfall, Crop, Season, Area, Production, Annual_Rainfall, Fertilizer, Pesticide, Yield\n",
    "\n",
    "# Select relevant features for training\n",
    "features = ['AvgTemp', 'AvgHumidity', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df[features]\n",
    "y = df['Yield']\n",
    "\n",
    "# Normalize numerical features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X[features] = scaler.fit_transform(X[features])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.1, random_state=42)\n",
    "\n",
    "# Build and train the first neural network (base model 1)\n",
    "model_nn_base_1 = Sequential()\n",
    "model_nn_base_1.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_1.add(Dense(20, activation='relu'))\n",
    "model_nn_base_1.add(Dense(10, activation='relu'))\n",
    "model_nn_base_1.add(Dense(1, activation='linear'))\n",
    "model_nn_base_1.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "model_nn_base_1.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the first base neural network model\n",
    "y_pred_nn_base_1 = model_nn_base_1.predict(X)\n",
    "\n",
    "# Build and train the second neural network (base model 2)\n",
    "model_nn_base_2 = Sequential()\n",
    "model_nn_base_2.add(Dense(50, activation='relu', input_dim=len(features)))\n",
    "model_nn_base_2.add(Dense(25, activation='relu'))\n",
    "model_nn_base_2.add(Dense(10, activation='relu'))\n",
    "model_nn_base_2.add(Dense(5, activation='relu'))\n",
    "model_nn_base_2.add(Dense(1, activation='linear'))\n",
    "model_nn_base_2.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "model_nn_base_2.fit(X, y, epochs=200, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions from the second base neural network model\n",
    "y_pred_nn_base_2 = model_nn_base_2.predict(X)\n",
    "\n",
    "# Combine predictions from both base models\n",
    "X_meta_train = np.concatenate((y_pred_nn_base_1, y_pred_nn_base_2), axis=1)\n",
    "\n",
    "# Build and train the meta neural network\n",
    "model_nn_meta = Sequential()\n",
    "model_nn_meta.add(Dense(50, activation='relu', input_dim=X_meta_train.shape[1]))\n",
    "model_nn_meta.add(Dense(20, activation='relu'))\n",
    "model_nn_meta.add(Dense(10, activation='relu'))\n",
    "model_nn_meta.add(Dense(1, activation='linear'))\n",
    "model_nn_meta.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "model_nn_meta.fit(X_meta_train, y, epochs=200, batch_size=16)\n",
    "\n",
    "# Make predictions using the base models and meta-learner on the entire dataset\n",
    "X_meta_all = np.concatenate((model_nn_base_1.predict(X), model_nn_base_2.predict(X)), axis=1)\n",
    "y_pred_stacking_nn = model_nn_meta.predict(X_meta_all)\n",
    "\n",
    "# Evaluate the stacking model with Neural Network as meta-learner on the entire dataset\n",
    "mse_stacking_nn = mean_squared_error(y, y_pred_stacking_nn)\n",
    "print(f\"Mean Squared Error on Entire Data (Stacking with Neural Network): {mse_stacking_nn:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beadb945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
